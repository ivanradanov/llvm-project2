gpu-affine-opt: Before opt:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant 0 : i32
  %0 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %6 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield %c0_i32 : i32
        } else {
          %9 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] : memref<?xi8>, vector<4xi8>
          %10 = llvm.bitcast %9 : vector<4xi8> to i32
          affine.yield %10 : i32
        }
        %7 = llvm.bitcast %6 : i32 to vector<4xi8>
        affine.vector_store %7, %alloca[%arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %8 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg12 * 4 + 1052] : memref<1080xi8, 3>, vector<4xi8>
      }
      "affine.barrier"(%arg12, %arg13, %arg14) : (index, index, index) -> ()
      %4 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %6 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %7 = llvm.bitcast %6 : vector<4xi8> to i32
        %8 = arith.addi %7, %arg16 : i32
        affine.yield %8 : i32
      }
      %5 = llvm.bitcast %4 : i32 to vector<4xi8>
      affine.vector_store %5, %1[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Removed IVs:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant 0 : i32
  %0 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %6 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield %c0_i32 : i32
        } else {
          %9 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] : memref<?xi8>, vector<4xi8>
          %10 = llvm.bitcast %9 : vector<4xi8> to i32
          affine.yield %10 : i32
        }
        %7 = llvm.bitcast %6 : i32 to vector<4xi8>
        affine.vector_store %7, %alloca[%arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %8 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg12 * 4 + 1052] : memref<1080xi8, 3>, vector<4xi8>
      }
      "affine.barrier"(%arg12, %arg13, %arg14) : (index, index, index) -> ()
      %4 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %6 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %7 = llvm.bitcast %6 : vector<4xi8> to i32
        %8 = arith.addi %7, %arg16 : i32
        affine.yield %8 : i32
      }
      %5 = llvm.bitcast %4 : i32 to vector<4xi8>
      affine.vector_store %5, %1[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: To Affine:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant 0 : i32
  %0 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %6 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield %c0_i32 : i32
        } else {
          %9 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] : memref<?xi8>, vector<4xi8>
          %10 = llvm.bitcast %9 : vector<4xi8> to i32
          affine.yield %10 : i32
        }
        %7 = llvm.bitcast %6 : i32 to vector<4xi8>
        affine.vector_store %7, %alloca[%arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %8 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg12 * 4 + 1052] : memref<1080xi8, 3>, vector<4xi8>
      }
      "affine.barrier"(%arg12, %arg13, %arg14) : (index, index, index) -> ()
      %4 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %6 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %7 = llvm.bitcast %6 : vector<4xi8> to i32
        %8 = arith.addi %7, %arg16 : i32
        affine.yield %8 : i32
      }
      %5 = llvm.bitcast %4 : i32 to vector<4xi8>
      affine.vector_store %5, %1[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Distributed:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant 0 : i32
  %0 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %4 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield %c0_i32 : i32
        } else {
          %7 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] : memref<?xi8>, vector<4xi8>
          %8 = llvm.bitcast %7 : vector<4xi8> to i32
          affine.yield %8 : i32
        }
        %5 = llvm.bitcast %4 : i32 to vector<4xi8>
        affine.vector_store %5, %alloca[%arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %6 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] : memref<?xi8>, vector<4xi8>
        affine.vector_store %6, %alloca[%arg12 * 4 + 1052] : memref<1080xi8, 3>, vector<4xi8>
      }
    } {gpu.par.block}
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %5 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %6 = llvm.bitcast %5 : vector<4xi8> to i32
        %7 = arith.addi %6, %arg16 : i32
        affine.yield %7 : i32
      }
      %4 = llvm.bitcast %3 : i32 to vector<4xi8>
      affine.vector_store %4, %1[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Canonicalized:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant 0 : i32
  %0 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %4 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield %c0_i32 : i32
        } else {
          %7 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] : memref<?xi8>, vector<4xi8>
          %8 = llvm.bitcast %7 : vector<4xi8> to i32
          affine.yield %8 : i32
        }
        %5 = llvm.bitcast %4 : i32 to vector<4xi8>
        affine.vector_store %5, %alloca[%arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %6 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] : memref<?xi8>, vector<4xi8>
        affine.vector_store %6, %alloca[%arg12 * 4 + 1052] : memref<1080xi8, 3>, vector<4xi8>
      }
    } {gpu.par.block}
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %5 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] : memref<1080xi8, 3>, vector<4xi8>
        %6 = llvm.bitcast %5 : vector<4xi8> to i32
        %7 = arith.addi %6, %arg16 : i32
        affine.yield %7 : i32
      }
      %4 = llvm.bitcast %3 : i32 to vector<4xi8>
      affine.vector_store %4, %1[%arg9 * 1024 + %arg12 * 4] : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
Schedule:
domain: "[P0] -> { S23_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S2_memref_ataddr[]; S8_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S9_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S19_llvm_bitcast[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S1_memref_ataddr[]; S10_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S12_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S16_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S22_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S15_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S20_arith_addi[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S17_affine_store_var[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S7_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0; S5_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0; S24_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S6_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S0_arith_constant[]; S13_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S3_arith_index_cast[]; S21_affine_yield[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S18_affine_vector_load[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S14_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S11_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255 }"
child:
  sequence:
  - filter: "[P0] -> { S0_arith_constant[] }"
  - filter: "[P0] -> { S1_memref_ataddr[] }"
  - filter: "[P0] -> { S2_memref_ataddr[] }"
  - filter: "[P0] -> { S3_arith_index_cast[] }"
  - filter: "[P0] -> { S15_affine_yield[i0, i1, i2, i3, i4, i5]; S16_affine_yield[i0, i1, i2, i3, i4, i5]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5]; S24_affine_yield[i0, i1, i2, i3, i4, i5]; S7_affine_yield[i0, i1, i2, i3, i4, i5]; S25_affine_yield[i0, i1, i2]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S10_affine_yield[i0, i1, i2, i3, i4, i5]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5]; S4_memref_alloca[i0, i1, i2]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] }"
    child:
      schedule: "[P0] -> L9.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S25_affine_yield[i0, i1, i2] -> [(i0)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S4_memref_alloca[i0, i1, i2] -> [(i0)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i0)] }]"
      permutable: 1
      child:
        schedule: "[P0] -> L8.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S25_affine_yield[i0, i1, i2] -> [(i1)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S4_memref_alloca[i0, i1, i2] -> [(i1)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i1)] }]"
        permutable: 1
        child:
          schedule: "[P0] -> L7.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S25_affine_yield[i0, i1, i2] -> [(i2)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S4_memref_alloca[i0, i1, i2] -> [(i2)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i2)] }]"
          permutable: 1
          child:
            sequence:
            - filter: "[P0] -> { S4_memref_alloca[i0, i1, i2] }"
            - filter: "[P0] -> { S15_affine_yield[i0, i1, i2, i3, i4, i5]; S16_affine_yield[i0, i1, i2, i3, i4, i5]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5]; S7_affine_yield[i0, i1, i2, i3, i4, i5]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5]; S10_affine_yield[i0, i1, i2, i3, i4, i5]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0] -> L2.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                child:
                  schedule: "[P0] -> L1.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  child:
                    schedule: "[P0] -> L0.affine.parallel[{ S15_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S16_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S7_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)]; S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S10_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    child:
                      sequence:
                      - filter: "[P0] -> { S5_affine_vector_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S6_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S8_affine_vector_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S10_affine_yield[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S7_affine_yield[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S12_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S15_affine_yield[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S16_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0] -> { S22_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S17_affine_store_var[i0, i1, i2, i3, i4, i5]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S24_affine_yield[i0, i1, i2, i3, i4, i5]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
              child:
                schedule: "[P0] -> L6.affine.parallel[{ S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                child:
                  schedule: "[P0] -> L5.affine.parallel[{ S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  child:
                    schedule: "[P0] -> L4.affine.parallel[{ S24_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    child:
                      sequence:
                      - filter: "[P0] -> { S17_affine_store_var[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S20_arith_addi[i0, i1, i2, i3, i4, i5, i6]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                        child:
                          schedule: "[P0] -> L3.affine.for[{ S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> [(i6)] }]"
                          child:
                            sequence:
                            - filter: "[P0] -> { S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0] -> { S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0] -> { S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0] -> { S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                      - filter: "[P0] -> { S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S23_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0] -> { S24_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0] -> { S25_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0] -> { S23_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S2_memref_ataddr[]; S8_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S9_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S19_llvm_bitcast[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S1_memref_ataddr[]; S10_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S12_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S16_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S22_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S15_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S20_arith_addi[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S17_affine_store_var[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S7_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0; S5_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0; S24_affine_yield[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S6_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 0 <= i3 <= 255; S0_arith_constant[]; S13_affine_vector_load[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S3_arith_index_cast[]; S21_affine_yield[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S18_affine_vector_load[i0, 0, 0, i3, 0, 0, i6] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14; S14_affine_vector_store[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255; S11_llvm_bitcast[i0, 0, 0, i3, 0, 0] : 0 <= i0 < P0 and 8 <= i3 <= 255 }"
accesses:
  - S0_arith_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_arith_index_cast:
        - read "[P0] -> { S3_arith_index_cast[] -> A_llvm_func_arg_0_0[] }"
  - S4_memref_alloca:
  - S5_affine_vector_load:
        - read "[P0] -> { S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_memref_ataddr_res_1[1024i0 + 4i3] }"
        - must_write "[P0] -> { S5_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_2[] }"
  - S6_affine_vector_store:
        - must_write "[P0] -> { S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_3[28 + 4i3] }"
        - read "[P0] -> { S6_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_2[] }"
  - S7_affine_yield:
        - must_write "[P0] -> { S7_affine_yield[i0, i1, i2, i3, i4, i5] -> A_affine_if_res_4[] }"
  - S8_affine_vector_load:
        - read "[P0] -> { S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_memref_ataddr_res_1[-28 + 1024i0 + 4i3] }"
        - must_write "[P0] -> { S8_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_5[] }"
  - S9_llvm_bitcast:
        - must_write "[P0] -> { S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_6[] }"
        - read "[P0] -> { S9_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_5[] }"
  - S10_affine_yield:
        - must_write "[P0] -> { S10_affine_yield[i0, i1, i2, i3, i4, i5] -> A_affine_if_res_4[] }"
        - read "[P0] -> { S10_affine_yield[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_6[] }"
        - kill "[P0] -> { S10_affine_yield[i0, 0, 0, i3, 0, 0] -> A_affine_vector_load_res_5[] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255 }"
        - kill "[P0] -> { S10_affine_yield[i0, 0, 0, i3, 0, 0] -> A_llvm_bitcast_res_6[] : 0 <= i0 < P0 and i3 >= 8 and 8 - 256i0 <= i3 <= 255 }"
  - S11_llvm_bitcast:
        - must_write "[P0] -> { S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_7[] }"
        - read "[P0] -> { S11_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_if_res_4[] }"
  - S12_affine_vector_store:
        - must_write "[P0] -> { S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_3[4i3] }"
        - read "[P0] -> { S12_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_7[] }"
  - S13_affine_vector_load:
        - read "[P0] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_memref_ataddr_res_1[1024 + 1024i0 + 4i3] }"
        - must_write "[P0] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_8[] }"
  - S14_affine_vector_store:
        - must_write "[P0] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_3[1052 + 4i3] }"
        - read "[P0] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_affine_vector_load_res_8[] }"
  - S15_affine_yield:
        - kill "[P0] -> { S15_affine_yield[i0, 0, 0, i3, 0, 0] -> A_affine_if_res_4[] : 0 <= i0 < P0 and 8 <= i3 <= 255 }"
        - kill "[P0] -> { S15_affine_yield[i0, 0, 0, i3, 0, 0] -> A_llvm_bitcast_res_7[] : 0 <= i0 < P0 and 8 <= i3 <= 255 }"
        - kill "[P0] -> { S15_affine_yield[i0, 0, 0, i3, 0, 0] -> A_affine_vector_load_res_8[] : 0 <= i0 < P0 and 8 <= i3 <= 255 }"
  - S16_affine_yield:
        - kill "[P0] -> { S16_affine_yield[i0, 0, 0, i3, 0, 0] -> A_affine_vector_load_res_2[] : 0 <= i0 < P0 and 0 <= i3 <= 255 }"
  - S17_affine_store_var:
        - must_write "[P0] -> { S17_affine_store_var[i0, i1, i2, i3, i4, i5] -> A_affine_for_res_9[] }"
  - S18_affine_vector_load:
        - read "[P0] -> { S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_3[4i3 + 4i6] }"
        - must_write "[P0] -> { S18_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_10[] }"
  - S19_llvm_bitcast:
        - must_write "[P0] -> { S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> A_llvm_bitcast_res_11[] }"
        - read "[P0] -> { S19_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_10[] }"
  - S20_arith_addi:
        - must_write "[P0] -> { S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> A_arith_addi_res_12[] }"
        - read "[P0] -> { S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> A_llvm_bitcast_res_11[] }"
        - read "[P0] -> { S20_arith_addi[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_9[] }"
  - S21_affine_yield:
        - must_write "[P0] -> { S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_9[] }"
        - read "[P0] -> { S21_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> A_arith_addi_res_12[] }"
        - kill "[P0] -> { S21_affine_yield[i0, 0, 0, i3, 0, 0, i6] -> A_affine_vector_load_res_10[] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14 }"
        - kill "[P0] -> { S21_affine_yield[i0, 0, 0, i3, 0, 0, i6] -> A_llvm_bitcast_res_11[] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14 }"
        - kill "[P0] -> { S21_affine_yield[i0, 0, 0, i3, 0, 0, i6] -> A_arith_addi_res_12[] : 0 <= i0 < P0 and 0 <= i3 <= 255 and 0 <= i6 <= 14 }"
  - S22_llvm_bitcast:
        - must_write "[P0] -> { S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_13[] }"
        - read "[P0] -> { S22_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_for_res_9[] }"
  - S23_affine_vector_store:
        - must_write "[P0] -> { S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_memref_ataddr_res_14[1024i0 + 4i3] }"
        - read "[P0] -> { S23_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_13[] }"
  - S24_affine_yield:
        - kill "[P0] -> { S24_affine_yield[i0, 0, 0, i3, 0, 0] -> A_affine_for_res_9[] : 0 <= i0 < P0 and 0 <= i3 <= 255 }"
        - kill "[P0] -> { S24_affine_yield[i0, 0, 0, i3, 0, 0] -> A_llvm_bitcast_res_13[] : 0 <= i0 < P0 and 0 <= i3 <= 255 }"
  - S25_affine_yield:
        - kill "[P0] -> { S25_affine_yield[i0, 0, 0] -> A_memref_alloca_res_3[o0] : 0 <= i0 < P0 }"
  - S26_llvm_return:
Schedule:
domain: "[P0] -> { S1_memref_ataddr[]; RS0_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0; S0_arith_constant[]; S3_arith_index_cast[]; RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; S2_memref_ataddr[]; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0 }"
child:
  sequence:
  - filter: "[P0] -> { S0_arith_constant[] }"
  - filter: "[P0] -> { S1_memref_ataddr[] }"
  - filter: "[P0] -> { S2_memref_ataddr[] }"
  - filter: "[P0] -> { S3_arith_index_cast[] }"
  - filter: "[P0] -> { S4_memref_alloca[i0, i1, i2]; RS1_affine_parallel[i0, i1, i2]; S25_affine_yield[i0, i1, i2]; RS0_affine_parallel[i0, i1, i2] }"
    child:
      schedule: "[P0] -> L2.affine.parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i0)]; S25_affine_yield[i0, i1, i2] -> [(i0)]; RS1_affine_parallel[i0, i1, i2] -> [(i0)]; S4_memref_alloca[i0, i1, i2] -> [(i0)] }]"
      permutable: 1
      child:
        schedule: "[P0] -> L1.affine.parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i1)]; S25_affine_yield[i0, i1, i2] -> [(i1)]; RS1_affine_parallel[i0, i1, i2] -> [(i1)]; S4_memref_alloca[i0, i1, i2] -> [(i1)] }]"
        permutable: 1
        child:
          schedule: "[P0] -> L0.affine.parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i2)]; S25_affine_yield[i0, i1, i2] -> [(i2)]; RS1_affine_parallel[i0, i1, i2] -> [(i2)]; S4_memref_alloca[i0, i1, i2] -> [(i2)] }]"
          permutable: 1
          child:
            sequence:
            - filter: "[P0] -> { S4_memref_alloca[i0, i1, i2] }"
            - filter: "[P0] -> { RS0_affine_parallel[i0, i1, i2] }"
            - filter: "[P0] -> { RS1_affine_parallel[i0, i1, i2] }"
            - filter: "[P0] -> { S25_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0] -> { S1_memref_ataddr[]; RS0_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0; S0_arith_constant[]; S3_arith_index_cast[]; RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; S2_memref_ataddr[]; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0 }"
accesses:
  - S0_arith_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_arith_index_cast:
        - read "[P0] -> { S3_arith_index_cast[] -> A_llvm_func_arg_0_0[] }"
  - S4_memref_alloca:
  - S25_affine_yield:
        - kill "[P0] -> { S25_affine_yield[i0, 0, 0] -> A_memref_alloca_res_3[o0] : 0 <= i0 < P0 }"
  - S26_llvm_return:
  - RS0_affine_parallel:
        - read "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 1024i0 <= o0 <= 1020 + 1024i0 }"
        - must_write "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 28 <= o0 <= 1048 }"
        - read "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 4 + 1024i0 <= o0 <= 992 + 1024i0 }"
        - must_write "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 32 <= o0 <= 1020 }"
        - read "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 1056 + 1024i0 <= o0 <= 2044 + 1024i0 }"
        - must_write "[P0] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 1084 <= o0 <= 2072 }"
  - RS1_affine_parallel:
        - read "[P0] -> { RS1_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= o0 <= 1076 }"
        - must_write "[P0] -> { RS1_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 1024i0 <= o0 <= 1020 + 1024i0 }"
ReductionTagMap: [P0] -> {  }
TaggedStmtDomain: [P0] -> { [RS1_affine_parallel[i0, i1, i2] -> S23_affine_vector_store_Write0[]]; [RS0_affine_parallel[i0, i1, i2] -> S6_affine_vector_store_Write0[]]; [RS0_affine_parallel[i0, i1, i2] -> S12_affine_vector_store_Write0[]]; [RS0_affine_parallel[i0, i1, i2] -> S8_affine_vector_load_Read0[]]; [RS1_affine_parallel[i0, i1, i2] -> S18_affine_vector_load_Read0[]]; [RS0_affine_parallel[i0, i1, i2] -> S5_affine_vector_load_Read0[]]; [S3_arith_index_cast[] -> S3_arith_index_cast_Read0[]]; [RS0_affine_parallel[i0, i1, i2] -> S14_affine_vector_store_Write0[]]; [RS0_affine_parallel[i0, i1, i2] -> S13_affine_vector_load_Read0[]]; [S25_affine_yield[i0, i1, i2] -> S25_affine_yield0[]] }
dep_order for A_llvm_func_arg_0_0 [P0] -> {  }
dep_order for A_memref_ataddr_res_1 [P0] -> {  }
dep_order for A_affine_vector_load_res_2 [P0] -> {  }
dep_order for A_memref_alloca_res_3 [P0] -> { RS1_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : i1 = 0 and i2 = 0 and o1 = 0 and o2 = 0 and i0 >= 0 and o0 > i0 and o0 < P0 }
dep_order for A_affine_if_res_4 [P0] -> {  }
dep_order for A_affine_vector_load_res_5 [P0] -> {  }
dep_order for A_llvm_bitcast_res_6 [P0] -> {  }
dep_order for A_llvm_bitcast_res_7 [P0] -> {  }
dep_order for A_affine_vector_load_res_8 [P0] -> {  }
dep_order for A_affine_for_res_9 [P0] -> {  }
dep_order for A_affine_vector_load_res_10 [P0] -> {  }
dep_order for A_llvm_bitcast_res_11 [P0] -> {  }
dep_order for A_arith_addi_res_12 [P0] -> {  }
dep_order for A_llvm_bitcast_res_13 [P0] -> {  }
dep_order for A_memref_ataddr_res_14 [P0] -> {  }
ReductionTagMap: [P0] -> {  }
TaggedStmtDomain: [P0] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[]]; [S25_affine_yield[i0, i1, i2] -> A_memref_alloca_res_3[]]; [RS1_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[]]; [RS0_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_1[]]; [S3_arith_index_cast[] -> A_llvm_func_arg_0_0[]]; [RS1_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_14[]] }
dep_order for A_llvm_func_arg_0_0 [P0] -> {  }
dep_order for A_memref_ataddr_res_1 [P0] -> {  }
dep_order for A_affine_vector_load_res_2 [P0] -> {  }
dep_order for A_memref_alloca_res_3 [P0] -> { RS1_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : i1 = 0 and i2 = 0 and o1 = 0 and o2 = 0 and i0 >= 0 and o0 > i0 and o0 < P0 }
dep_order for A_affine_if_res_4 [P0] -> {  }
dep_order for A_affine_vector_load_res_5 [P0] -> {  }
dep_order for A_llvm_bitcast_res_6 [P0] -> {  }
dep_order for A_llvm_bitcast_res_7 [P0] -> {  }
dep_order for A_affine_vector_load_res_8 [P0] -> {  }
dep_order for A_affine_for_res_9 [P0] -> {  }
dep_order for A_affine_vector_load_res_10 [P0] -> {  }
dep_order for A_llvm_bitcast_res_11 [P0] -> {  }
dep_order for A_arith_addi_res_12 [P0] -> {  }
dep_order for A_llvm_bitcast_res_13 [P0] -> {  }
dep_order for A_memref_ataddr_res_14 [P0] -> {  }
tagged_reads [P0] -> { [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 0 <= o0 <= 1076; [RS0_affine_parallel[i0, 0, 0] -> S8_affine_vector_load_Read0[]] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 4 + 1024i0 <= o0 <= 992 + 1024i0; [RS0_affine_parallel[i0, 0, 0] -> S13_affine_vector_load_Read0[]] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1056 + 1024i0 <= o0 <= 2044 + 1024i0; [S3_arith_index_cast[] -> S3_arith_index_cast_Read0[]] -> A_llvm_func_arg_0_0[]; [RS0_affine_parallel[i0, 0, 0] -> S5_affine_vector_load_Read0[]] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0 }
atagged_reads [P0] -> { [RS1_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 0 <= o0 <= 1076; [S3_arith_index_cast[] -> A_llvm_func_arg_0_0[]] -> A_llvm_func_arg_0_0[]; [RS0_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_1[]] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 2044 + 1024i0 and (o0 >= 1056 + 1024i0 or o0 <= 1020 + 1024i0) }
reads [P0] -> { S3_arith_index_cast[] -> A_llvm_func_arg_0_0[]; RS0_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 2044 + 1024i0 and (o0 >= 1056 + 1024i0 or o0 <= 1020 + 1024i0); RS1_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 0 <= o0 <= 1076 }
async_reads [P0] -> { RS0_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 2044 + 1024i0 and (o0 >= 1056 + 1024i0 or o0 <= 1020 + 1024i0) }
tagged_may_writes [P0] -> { [RS0_affine_parallel[i0, 0, 0] -> S6_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 1048; [RS0_affine_parallel[i0, 0, 0] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1084 <= o0 <= 2072; [RS1_affine_parallel[i0, 0, 0] -> S23_affine_vector_store_Write0[]] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0; [RS0_affine_parallel[i0, 0, 0] -> S12_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 32 <= o0 <= 1020 }
atagged_may_writes [P0] -> { [RS1_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_14[]] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0; [RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 2072 and (o0 >= 1084 or o0 <= 1048) }
may_writes [P0] -> { RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 2072 and (o0 >= 1084 or o0 <= 1048); RS1_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0 }
tagged_must_writes [P0] -> { [RS0_affine_parallel[i0, 0, 0] -> S6_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 1048; [RS0_affine_parallel[i0, 0, 0] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1084 <= o0 <= 2072; [RS1_affine_parallel[i0, 0, 0] -> S23_affine_vector_store_Write0[]] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0; [RS0_affine_parallel[i0, 0, 0] -> S12_affine_vector_store_Write0[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 32 <= o0 <= 1020 }
atagged_must_writes [P0] -> { [RS1_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_14[]] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0; [RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 2072 and (o0 >= 1084 or o0 <= 1048) }
must_writes [P0] -> { RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 2072 and (o0 >= 1084 or o0 <= 1048); RS1_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0 }
async_must_writes [P0] -> { RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 28 <= o0 <= 2072 and (o0 >= 1084 or o0 <= 1048) }
tagged_must_kills [P0] -> { [S25_affine_yield[i0, 0, 0] -> S25_affine_yield0[]] -> A_memref_alloca_res_3[o0] : 0 <= i0 < P0 }
atagged_must_kills [P0] -> { [S25_affine_yield[i0, 0, 0] -> A_memref_alloca_res_3[]] -> A_memref_alloca_res_3[o0] : 0 <= i0 < P0 }
must_kills [P0] -> { S25_affine_yield[i0, 0, 0] -> A_memref_alloca_res_3[o0] : 0 <= i0 < P0 }
live_in [P0] -> { RS0_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_1[o0] : (o0) mod 4 = 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 2044 + 1024i0 and (o0 >= 1056 + 1024i0 or (4 + 1024i0 <= o0 <= 992 + 1024i0) or o0 <= 1020 + 1024i0); S3_arith_index_cast[] -> A_llvm_func_arg_0_0[]; RS1_affine_parallel[0, 0, 0] -> A_memref_alloca_res_3[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= o0 <= 1076 and (o0 <= 27 or o0 >= 1052) }
live_out [P0] -> { RS1_affine_parallel[i0, 0, 0] -> A_memref_ataddr_res_14[o0] : (o0) mod 4 = 0 and P0 > 0 and 0 <= i0 < P0 and 1024i0 <= o0 <= 1020 + 1024i0 }
independence [P0] -> { S4_memref_alloca[i0, i1, i2] -> S4_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S4_memref_alloca[i0, i1, i2] -> S4_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S4_memref_alloca[i0, i1, i2] -> S4_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; S25_affine_yield[i0, i1, i2] -> S25_affine_yield[o0, o1, o2] : o2 < i2 or o2 > i2; S25_affine_yield[i0, i1, i2] -> S25_affine_yield[o0, i1, i2] : o0 < i0 or o0 > i0; S25_affine_yield[i0, i1, i2] -> S25_affine_yield[o0, o1, i2] : o1 < i1 or o1 > i1; RS1_affine_parallel[i0, i1, i2] -> RS1_affine_parallel[o0, o1, o2] : o2 < i2 or o2 > i2; RS1_affine_parallel[i0, i1, i2] -> RS1_affine_parallel[o0, i1, i2] : o0 < i0 or o0 > i0; RS1_affine_parallel[i0, i1, i2] -> RS1_affine_parallel[o0, o1, i2] : o1 < i1 or o1 > i1; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : o2 > i2 or o2 < i2; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, i1, i2] : o0 > i0 or o0 < i0; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, i2] : o1 > i1 or o1 < i1 }
dep_flow [P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }
tagged_dep_flow [P0] -> { [RS0_affine_parallel[i0, 0, 0] -> S6_affine_vector_store_Write0[]] -> [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] : 0 <= i0 < P0; [RS0_affine_parallel[i0, 0, 0] -> S12_affine_vector_store_Write0[]] -> [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] : 0 <= i0 < P0 }
atagged_dep_flow [P0] -> { [RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] -> [RS1_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] : 0 <= i0 < P0 }
dep_false [P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : 0 <= i0 <= -2 + P0; RS1_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : 0 <= i0 <= -2 + P0 }
dep_forced [P0] -> {  }
dep_order [P0] -> { RS1_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[o0, 0, 0] : i0 >= 0 and i0 < o0 < P0; RS0_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[o0, 0, 0] : i0 >= 0 and i0 < o0 < P0 }
tagged_dep_order [P0] -> { [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S12_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0; [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S6_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0; [RS0_affine_parallel[i0, 0, 0] -> S14_affine_vector_store_Write0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0 }
dep_async [P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }
array_order [P0] -> { RS1_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[o0, 0, 0] : i0 >= 0 and i0 < o0 < P0 }
tagger [P0] -> { [RS1_affine_parallel[i0, i1, i2] -> S23_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> S14_affine_vector_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> S8_affine_vector_load_Read0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2] -> S18_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> S6_affine_vector_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [S25_affine_yield[i0, i1, i2] -> S25_affine_yield0[]] -> S25_affine_yield[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> S13_affine_vector_load_Read0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> S12_affine_vector_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [S3_arith_index_cast[] -> S3_arith_index_cast_Read0[]] -> S3_arith_index_cast[]; [RS0_affine_parallel[i0, i1, i2] -> S5_affine_vector_load_Read0[]] -> RS0_affine_parallel[(i0), (i1), (i2)] }
atagger [P0] -> { [RS1_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[]] -> RS1_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_3[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS0_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_1[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [S3_arith_index_cast[] -> A_llvm_func_arg_0_0[]] -> S3_arith_index_cast[]; [S25_affine_yield[i0, i1, i2] -> A_memref_alloca_res_3[]] -> S25_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_14[]] -> RS1_affine_parallel[(i0), (i1), (i2)] }
schedule
domain: "[P0] -> { RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; RS0_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
child:
  schedule: "[P0] -> L2.affine.parallel[{ RS1_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  child:
    schedule: "[P0] -> L1.affine.parallel[{ RS1_affine_parallel[i0, i1, i2] -> [(0)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
    permutable: 1
    child:
      schedule: "[P0] -> L0.affine.parallel[{ RS1_affine_parallel[i0, i1, i2] -> [(0)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
      permutable: 1
      child:
        sequence:
        - filter: "[P0] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0] -> { RS1_affine_parallel[i0, i1, i2] }"
Schedule constraints:
domain: "[P0] -> { RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; RS0_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
validity: "[P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
proximity: "[P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
anti_proximity: "[P0] -> { RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
live_range_span: "[P0] -> { [RS0_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] -> [RS1_affine_parallel[i0, 0, 0] -> A_memref_alloca_res_3[]] : 0 <= i0 < P0 }"
coincidence: "[P0] -> { RS1_affine_parallel[i0, 0, 0] -> RS0_affine_parallel[o0, 0, 0] : i0 >= 0 and i0 < o0 < P0; RS0_affine_parallel[i0, 0, 0] -> RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
condition: "[P0] -> { [RS0_affine_parallel[i0, 0, 0] -> S6_affine_vector_store_Write0[]] -> [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] : 0 <= i0 < P0; [RS0_affine_parallel[i0, 0, 0] -> S12_affine_vector_store_Write0[]] -> [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] : 0 <= i0 < P0 }"
conditional_validity: "[P0] -> { [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S12_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0; [RS1_affine_parallel[i0, 0, 0] -> S18_affine_vector_load_Read0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S6_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0; [RS0_affine_parallel[i0, 0, 0] -> S14_affine_vector_store_Write0[]] -> [RS0_affine_parallel[o0, 0, 0] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and i0 < o0 < P0 }"
setup lp:
{ [i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15, i16, i17] : i7 = 100000 - i0 and i9 = 1 + i8 and i6 = i1 - i5 and i16 = i3 - i12 and i15 = i4 - i10 - i11 - i14 and 3i9 <= 535 and i9 > 0 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 and i17 <= i12 + i13 - i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 + i14 - i15 and i16 >= i12 and i17 >= 2i12 + i13 + i14 - i15 - 2i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 and i17 >= i12 + i13 - i16 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 + i14 - i15 and i16 <= i12 and i17 <= 2i12 + i13 + i14 - i15 - 2i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 and i17 >= -i8 + i12 + i13 - i16 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 and i17 <= -i7 + i12 + i13 - i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 + i14 - i15 and i16 >= i12 and i17 >= i7 + 2i12 + i13 + i14 - i15 - 2i16 and i15 >= i14 and i15 >= i7 + i14 }
sol:
[]
setup lp:
{ [i0, i1, i2, i3, i4, i5, i6, i7, i8, i9, i10, i11, i12, i13, i14, i15, i16, i17] : i7 = 100000 - i0 and i9 = 1 + i8 and i6 = i1 - i5 and i16 = i3 - i12 and i15 = i4 - i10 - i11 - i14 and 3i9 <= 535 and i9 > 0 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 and i17 <= i12 + i13 - i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 + i14 - i15 and i16 >= i12 and i17 >= 2i12 + i13 + i14 - i15 - 2i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 and i17 >= i12 + i13 - i16 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 + i14 - i15 and i16 <= i12 and i17 <= 2i12 + i13 + i14 - i15 - 2i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 and i17 >= -i8 + i12 + i13 - i16 and i16 <= -i10 + i11 + i12 + i14 - i15 and i16 <= i12 and i17 <= -i7 + i12 + i13 - i16 and i16 >= -i10 + i11 + i12 + i14 - i15 and i16 >= i12 + i14 - i15 and i16 >= i12 and i17 >= i7 + 2i12 + i13 + i14 - i15 - 2i16 and i15 >= i14 and i15 >= i7 + i14 }
sol:
[]
New Schedule:
domain: "[P0] -> { RS1_affine_parallel[i0, 0, 0] : 0 <= i0 < P0; RS0_affine_parallel[i0, 0, 0] : 0 <= i0 < P0 }"
child:
  schedule: "[P0] -> [{ RS0_affine_parallel[i0, i1, i2] -> [(i0)]; RS1_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  child:
    sequence:
    - filter: "[P0] -> { RS0_affine_parallel[i0, i1, i2] }"
    - filter: "[P0] -> { RS1_affine_parallel[i0, i1, i2] }"
New AST:
iterator:
  id: c0
init:
  val: 0
cond:
  op: lt
  args:
  - id: c0
  - id: P0@0x31051cf0
inc:
  val: 1
body:
- user:
    op: call
    args:
    - id: RS0_affine_parallel@0x31070060
    - id: c0
    - val: 0
    - val: 0
- user:
    op: call
    args:
    - id: RS1_affine_parallel@0x3106d8a0
    - id: c0
    - val: 0
    - val: 0
/scr/ivan/src/transformer-llvm-project/polly/lib/External/isl/isl_ctx.c:295: isl_ctx not freed as some objects still reference it
gpu-affine-opt: After opt:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr)>, linkage = #llvm.linkage<private>, sym_name = "stencil", sym_visibility = "private", unnamed_addr = 0 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr):
  %0 = "arith.constant"() <{value = 0 : i32}> {polymer.stmt.name = "S0_arith_constant"} : () -> i32
  %1 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S3_arith_index_cast"} : (i64) -> index
  "affine.parallel"(%3) <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<()[s0] -> (s0, 1, 1)>}> ({
  ^bb0(%arg9: index, %arg10: index, %arg11: index):
    %4 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S4_memref_alloca"} : () -> memref<1080xi8, 3>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (256, 1, 1)>}> ({
    ^bb0(%arg17: index, %arg18: index, %arg19: index):
      %10 = "affine.vector_load"(%1, %arg9, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 1024 + d1 * 4)>}> {polymer.stmt.name = "S5_affine_vector_load"} : (memref<?xi8>, index, index) -> vector<4xi8>
      "affine.vector_store"(%10, %4, %arg17) <{map = affine_map<(d0) -> (d0 * 4 + 28)>}> {polymer.stmt.name = "S6_affine_vector_store"} : (vector<4xi8>, memref<1080xi8, 3>, index) -> ()
      "affine.if"(%arg17) ({
        %11 = "affine.if"(%arg9, %arg17) ({
          "affine.yield"(%0) {polymer.stmt.name = "S7_affine_yield"} : (i32) -> ()
        }, {
          %14 = "affine.vector_load"(%1, %arg9, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 1024 + d1 * 4 - 28)>}> {polymer.stmt.name = "S8_affine_vector_load"} : (memref<?xi8>, index, index) -> vector<4xi8>
          %15 = "llvm.bitcast"(%14) {polymer.stmt.name = "S9_llvm_bitcast"} : (vector<4xi8>) -> i32
          "affine.yield"(%15) {polymer.stmt.name = "S10_affine_yield"} : (i32) -> ()
        }) {condition = affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>} : (index, index) -> i32
        %12 = "llvm.bitcast"(%11) {polymer.stmt.name = "S11_llvm_bitcast"} : (i32) -> vector<4xi8>
        "affine.vector_store"(%12, %4, %arg17) <{map = affine_map<(d0) -> (d0 * 4)>}> {polymer.stmt.name = "S12_affine_vector_store"} : (vector<4xi8>, memref<1080xi8, 3>, index) -> ()
        %13 = "affine.vector_load"(%1, %arg9, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 1024 + d1 * 4 + 1024)>}> {polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index) -> vector<4xi8>
        "affine.vector_store"(%13, %4, %arg17) <{map = affine_map<(d0) -> (d0 * 4 + 1052)>}> {polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1080xi8, 3>, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S15_affine_yield"} : () -> ()
      }, {
      }) {condition = affine_set<(d0) : (d0 - 8 >= 0)>} : (index) -> ()
      "affine.yield"() {polymer.stmt.name = "S16_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (256, 1, 1)>}> ({
    ^bb0(%arg12: index, %arg13: index, %arg14: index):
      "affine.store_var"(%0, %5) <{type = "for.iv.init"}> {polymer.stmt.name = "S17_affine_store_var"} : (i32, i32) -> ()
      %5 = "affine.for"(%0) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (15)>}> ({
      ^bb0(%arg15: index, %arg16: i32):
        %7 = "affine.vector_load"(%4, %arg15, %arg12) <{map = affine_map<(d0, d1) -> (d0 * 4 + d1 * 4)>}> {polymer.stmt.name = "S18_affine_vector_load"} : (memref<1080xi8, 3>, index, index) -> vector<4xi8>
        %8 = "llvm.bitcast"(%7) {polymer.stmt.name = "S19_llvm_bitcast"} : (vector<4xi8>) -> i32
        %9 = "arith.addi"(%8, %arg16) <{overflowFlags = #arith.overflow<none>}> {polymer.stmt.name = "S20_arith_addi"} : (i32, i32) -> i32
        "affine.yield"(%9) {polymer.stmt.name = "S21_affine_yield"} : (i32) -> ()
      }) : (i32) -> i32
      %6 = "llvm.bitcast"(%5) {polymer.stmt.name = "S22_llvm_bitcast"} : (i32) -> vector<4xi8>
      "affine.vector_store"(%6, %2, %arg9, %arg12) <{map = affine_map<(d0, d1) -> (d0 * 1024 + d1 * 4)>}> {polymer.stmt.name = "S23_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S24_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
    "affine.yield"() {polymer.stmt.name = "S25_affine_yield"} : () -> ()
  }) {gpu.par.grid} : (index) -> ()
  "llvm.return"() {polymer.stmt.name = "S26_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After gpuify:
llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c0_i32 = arith.constant {polymer.stmt.name = "S0_arith_constant"} 0 : i32
  %0 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %1 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = arith.index_cast %arg0 {polymer.stmt.name = "S3_arith_index_cast"} : i64 to index
  affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
    %alloca = memref.alloca() {polymer.stmt.name = "S4_memref_alloca"} : memref<1080xi8, 3>
    affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
      %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] {polymer.stmt.name = "S5_affine_vector_load"} : memref<?xi8>, vector<4xi8>
      affine.vector_store %3, %alloca[%arg12 * 4 + 28] {polymer.stmt.name = "S6_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
      affine.if affine_set<(d0) : (d0 - 8 >= 0)>(%arg12) {
        %6 = affine.if affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>(%arg9, %arg12) -> i32 {
          affine.yield {polymer.stmt.name = "S7_affine_yield"} %c0_i32 : i32
        } else {
          %9 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] {polymer.stmt.name = "S8_affine_vector_load"} : memref<?xi8>, vector<4xi8>
          %10 = llvm.bitcast %9 {polymer.stmt.name = "S9_llvm_bitcast"} : vector<4xi8> to i32
          affine.yield {polymer.stmt.name = "S10_affine_yield"} %10 : i32
        }
        %7 = llvm.bitcast %6 {polymer.stmt.name = "S11_llvm_bitcast"} : i32 to vector<4xi8>
        affine.vector_store %7, %alloca[%arg12 * 4] {polymer.stmt.name = "S12_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
        %8 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] {polymer.stmt.name = "S13_affine_vector_load"} : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg12 * 4 + 1052] {polymer.stmt.name = "S14_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
      }
      "affine.barrier"(%arg12, %arg13, %arg14) : (index, index, index) -> ()
      %4 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
        %6 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] {polymer.stmt.name = "S18_affine_vector_load"} : memref<1080xi8, 3>, vector<4xi8>
        %7 = llvm.bitcast %6 {polymer.stmt.name = "S19_llvm_bitcast"} : vector<4xi8> to i32
        %8 = arith.addi %7, %arg16 {polymer.stmt.name = "S20_arith_addi"} : i32
        affine.yield {polymer.stmt.name = "S21_affine_yield"} %8 : i32
      }
      %5 = llvm.bitcast %4 {polymer.stmt.name = "S22_llvm_bitcast"} : i32 to vector<4xi8>
      affine.vector_store %5, %1[%arg9 * 1024 + %arg12 * 4] {polymer.stmt.name = "S23_affine_vector_store"} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS0_affine_parallel"}
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S26_llvm_return"}
}
#set = affine_set<(d0) : (d0 - 8 >= 0)>
#set1 = affine_set<(d0, d1) : (d0 * 256 + d1 - 8 >= 0)>
module {
  gpu.module @m {
    llvm.func private @stencil(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr) attributes {gpu.par.kernel, sym_visibility = "private"} {
      %c0_i32 = arith.constant {polymer.stmt.name = "S0_arith_constant"} 0 : i32
      %0 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
      %1 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
      %2 = arith.index_cast %arg0 {polymer.stmt.name = "S3_arith_index_cast"} : i64 to index
      affine.parallel (%arg9, %arg10, %arg11) = (0, 0, 0) to (symbol(%2), 1, 1) {
        %alloca = memref.alloca() {polymer.stmt.name = "S4_memref_alloca"} : memref<1080xi8, 3>
        affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (256, 1, 1) {
          %3 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4] {polymer.stmt.name = "S5_affine_vector_load"} : memref<?xi8>, vector<4xi8>
          affine.vector_store %3, %alloca[%arg12 * 4 + 28] {polymer.stmt.name = "S6_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
          affine.if #set(%arg12) {
            %6 = affine.if #set1(%arg9, %arg12) -> i32 {
              affine.yield {polymer.stmt.name = "S7_affine_yield"} %c0_i32 : i32
            } else {
              %9 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 - 28] {polymer.stmt.name = "S8_affine_vector_load"} : memref<?xi8>, vector<4xi8>
              %10 = llvm.bitcast %9 {polymer.stmt.name = "S9_llvm_bitcast"} : vector<4xi8> to i32
              affine.yield {polymer.stmt.name = "S10_affine_yield"} %10 : i32
            }
            %7 = llvm.bitcast %6 {polymer.stmt.name = "S11_llvm_bitcast"} : i32 to vector<4xi8>
            affine.vector_store %7, %alloca[%arg12 * 4] {polymer.stmt.name = "S12_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
            %8 = affine.vector_load %0[%arg9 * 1024 + %arg12 * 4 + 1024] {polymer.stmt.name = "S13_affine_vector_load"} : memref<?xi8>, vector<4xi8>
            affine.vector_store %8, %alloca[%arg12 * 4 + 1052] {polymer.stmt.name = "S14_affine_vector_store"} : memref<1080xi8, 3>, vector<4xi8>
          }
          "affine.barrier"(%arg12, %arg13, %arg14) : (index, index, index) -> ()
          %4 = affine.for %arg15 = 0 to 15 iter_args(%arg16 = %c0_i32) -> (i32) {
            %6 = affine.vector_load %alloca[%arg15 * 4 + %arg12 * 4] {polymer.stmt.name = "S18_affine_vector_load"} : memref<1080xi8, 3>, vector<4xi8>
            %7 = llvm.bitcast %6 {polymer.stmt.name = "S19_llvm_bitcast"} : vector<4xi8> to i32
            %8 = arith.addi %7, %arg16 {polymer.stmt.name = "S20_arith_addi"} : i32
            affine.yield {polymer.stmt.name = "S21_affine_yield"} %8 : i32
          }
          %5 = llvm.bitcast %4 {polymer.stmt.name = "S22_llvm_bitcast"} : i32 to vector<4xi8>
          affine.vector_store %5, %1[%arg9 * 1024 + %arg12 * 4] {polymer.stmt.name = "S23_affine_vector_store"} : memref<?xi8>, vector<4xi8>
        } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS0_affine_parallel"}
      } {gpu.par.grid}
      llvm.return {polymer.stmt.name = "S26_llvm_return"}
    }
  }
}

Eliminated dead instances: [P0] -> { S2_memref_ataddr[]; S0_arith_constant[]; S3_arith_index_cast[]; S1_memref_ataddr[]; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0 }
Eliminated dead instances: [P0] -> { S2_memref_ataddr[]; S0_arith_constant[]; S3_arith_index_cast[]; S1_memref_ataddr[]; S4_memref_alloca[i0, 0, 0] : 0 <= i0 < P0; S25_affine_yield[i0, 0, 0] : 0 <= i0 < P0 }
