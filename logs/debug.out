gpu-affine-opt: Before opt:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.addi %12, %arg10 : i32
      %14 = arith.shli %7, %c4_i32 : i32
      %15 = arith.shli %arg11, %c4_i32 : i32
      %16 = arith.muli %10, %arg10 : i32
      %17 = arith.addi %16, %9 : i32
      %18 = arith.extui %10 : i32 to i64
      %19 = arith.extui %9 : i32 to i64
      %20 = llvm.getelementptr inbounds %5[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %21 = arith.muli %10, %arg11 : i32
      %22 = arith.addi %21, %9 : i32
      %23 = llvm.getelementptr inbounds %6[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %24:2 = scf.for %arg18 = %12 to %13 step %c16_i32 iter_args(%arg19 = %14, %arg20 = %0) -> (i32, f32)  : i32 {
        %31 = arith.addi %17, %arg18 : i32
        %32 = arith.extsi %31 : i32 to i64
        %33 = llvm.getelementptr inbounds %arg8[%32] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %34 = llvm.load %33 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %34, %20 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %35 = arith.addi %22, %arg19 : i32
        %36 = arith.extsi %35 : i32 to i64
        %37 = llvm.getelementptr inbounds %arg9[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %38 = llvm.load %37 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %38, %23 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %39 = scf.for %arg21 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg22 = %arg20) -> (f32)  : i32 {
          %41 = arith.extui %arg21 : i32 to i64
          %42 = llvm.getelementptr inbounds %5[0, %18, %41] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %43 = llvm.load %42 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %44 = llvm.getelementptr inbounds %6[0, %41, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %45 = llvm.load %44 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %46 = llvm.fmul %43, %45  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %47 = llvm.fadd %arg22, %46  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %47 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %40 = arith.addi %arg19, %15 : i32
        scf.yield %40, %39 : i32, f32
      }
      %25 = arith.muli %15, %8 : i32
      %26 = arith.addi %14, %9 : i32
      %27 = arith.addi %26, %21 : i32
      %28 = arith.addi %27, %25 : i32
      %29 = arith.extsi %28 : i32 to i64
      %30 = llvm.getelementptr inbounds %arg7[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %24#1, %30 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Removed IVs:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.shli %7, %c4_i32 : i32
      %14 = arith.shli %arg11, %c4_i32 : i32
      %15 = arith.muli %10, %arg10 : i32
      %16 = arith.addi %15, %9 : i32
      %17 = arith.extui %10 : i32 to i64
      %18 = arith.extui %9 : i32 to i64
      %19 = llvm.getelementptr inbounds %5[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %20 = arith.muli %10, %arg11 : i32
      %21 = arith.addi %20, %9 : i32
      %22 = llvm.getelementptr inbounds %6[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %23 = arith.subi %arg10, %c1_i32 : i32
      %24 = arith.divui %23, %c16_i32 : i32
      %25 = arith.addi %24, %c1_i32 : i32
      %26 = scf.for %arg18 = %c0_i32 to %25 step %c1_i32 iter_args(%arg19 = %0) -> (f32)  : i32 {
        %33 = arith.muli %arg18, %14 : i32
        %34 = arith.addi %33, %13 : i32
        %35 = arith.muli %arg18, %c16_i32 : i32
        %36 = arith.addi %12, %35 : i32
        %37 = arith.addi %16, %36 : i32
        %38 = arith.extsi %37 : i32 to i64
        %39 = llvm.getelementptr inbounds %arg8[%38] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %40 = llvm.load %39 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %40, %19 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %41 = arith.addi %21, %34 : i32
        %42 = arith.extsi %41 : i32 to i64
        %43 = llvm.getelementptr inbounds %arg9[%42] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %44 = llvm.load %43 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %44, %22 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %45 = scf.for %arg20 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg21 = %arg19) -> (f32)  : i32 {
          %46 = arith.extui %arg20 : i32 to i64
          %47 = llvm.getelementptr inbounds %5[0, %17, %46] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %48 = llvm.load %47 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %49 = llvm.getelementptr inbounds %6[0, %46, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %50 = llvm.load %49 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %51 = llvm.fmul %48, %50  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %52 = llvm.fadd %arg21, %51  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %52 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        scf.yield %45 : f32
      }
      %27 = arith.muli %14, %8 : i32
      %28 = arith.addi %13, %9 : i32
      %29 = arith.addi %28, %20 : i32
      %30 = arith.addi %29, %27 : i32
      %31 = arith.extsi %30 : i32 to i64
      %32 = llvm.getelementptr inbounds %arg7[%31] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %26, %32 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: To Affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.for %arg18 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] iter_args(%arg19 = %0) -> (f32) {
        %15 = affine.vector_load %2[(%arg16 * symbol(%7)) * 4 + %arg15 * 4 + %arg18 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %15, %alloca[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %16 = affine.vector_load %1[(%arg16 * symbol(%5)) * 4 + %arg15 * 4 + %arg12 * 64 + (%arg18 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %16, %alloca_0[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %17 = affine.for %arg20 = 0 to 16 iter_args(%arg21 = %arg19) -> (f32) {
          %18 = affine.vector_load %alloca[%arg16 * 64 + %arg20 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %19 = llvm.bitcast %18 : vector<4xi8> to f32
          %20 = affine.vector_load %alloca_0[%arg20 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %21 = llvm.bitcast %20 : vector<4xi8> to f32
          %22 = llvm.fmul %19, %21  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %23 = llvm.fadd %arg21, %22  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %23 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        affine.yield %17 : f32
      }
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Distributed:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c16 = arith.constant 16 : index
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca(%c16, %c16, %c1) : memref<?x?x?xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.vector_load %2[(%arg17 * symbol(%7)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %13, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %14 = affine.vector_load %1[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %14, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
        %14 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %13) -> (f32) {
          %15 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %16 = llvm.bitcast %15 : vector<4xi8> to f32
          %17 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %18 = llvm.bitcast %17 : vector<4xi8> to f32
          %19 = llvm.fmul %16, %18  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %20 = llvm.fadd %arg20, %19  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %20 : f32
        }
        affine.store %14, %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg10 : i32 to index
  %6 = arith.index_cast %arg1 : i64 to index
  %7 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%7), symbol(%6), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca() : memref<16x16x1xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%5] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.vector_load %2[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%5) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %9 = affine.vector_load %1[(%arg17 * symbol(%4)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %9, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
        %9 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %8) -> (f32) {
          %10 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %11 = llvm.bitcast %10 : vector<4xi8> to f32
          %12 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %13 = llvm.bitcast %12 : vector<4xi8> to f32
          %14 = llvm.fmul %11, %13  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %15 = llvm.fadd %arg20, %14  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %15 : f32
        }
        affine.store %9, %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %8 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
      %9 = llvm.bitcast %8 : f32 to vector<4xi8>
      affine.vector_store %9, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%4)) * 4 + (%arg13 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
Schedule:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S10_memref_alloca[i0, i1, i2]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S30_affine_load[i0, i1, i2, i3, i4, i5]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S34_affine_yield[i0, i1, i2]; S8_memref_alloca[i0, i1, i2]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S11_affine_store[i0, i1, i2, i3, i4, i5]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S33_affine_yield[i0, i1, i2, i3, i4, i5]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S12_affine_yield[i0, i1, i2, i3, i4, i5]; S9_memref_alloca[i0, i1, i2]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L16_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L15_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L14_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5]; S11_affine_store[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L0_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S29_affine_yield[i0, i1, i2, i3]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L10_affine_for[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L5_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L4_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L9_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L8_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L7_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                              child:
                                schedule: "[P0, P1, P2, P3] -> L6_affine_for[{ S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)] }]"
                                array_expansion: [ none ]
                                child:
                                  sequence:
                                  - filter: "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
                            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S30_affine_load[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L13_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L12_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L11_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S11_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
  - S12_affine_yield:
  - S13_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S14_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_7[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S15_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S16_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_10[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S17_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_6[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_9[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S18_affine_load:
        - read "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - must_write "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
  - S19_affine_store_var:
        - read "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
        - must_write "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S20_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_7[64i5 + 4i7] }"
        - must_write "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S21_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S22_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_10[4i4 + 64i7] }"
        - must_write "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S23_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
        - read "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S24_llvm_fmul:
        - must_write "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
  - S25_llvm_fadd:
        - must_write "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
  - S26_affine_yield:
        - must_write "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_13[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_14[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_15[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_16[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fmul_res_17[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fadd_res_18[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
  - S27_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - read "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S28_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_load_res_11[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_for_res_12[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S29_affine_yield:
  - S30_affine_load:
        - read "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
        - must_write "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S31_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
        - read "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S32_affine_vector_store:
        - may_write "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, 0, i3, i4, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - read "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
  - S33_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_affine_load_res_19[] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_llvm_bitcast_res_20[] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
  - S35_llvm_return:
Schedule:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; S8_memref_alloca[i0, i1, i2]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2]; S29_affine_yield[i0, i1, i2, i3]; S10_memref_alloca[i0, i1, i2]; S9_memref_alloca[i0, i1, i2]; RS0_affine_parallel[i0, i1, i2] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i2)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; RS3_affine_parallel[i0, i1, i2] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S29_affine_yield:
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
  - S35_llvm_return:
  - RS0_affine_parallel:
        - must_write "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS1_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
  - RS2_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - must_write "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS3_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - may_write "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] }
dep_order for A_llvm_func_arg_11_0 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_10_1 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_1_2 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_0_3 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_4 [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 > i3 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P0 and o3 > i3 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0) }
dep_order for A_memref_ataddr_res_5 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_6 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_7 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_memref_ataddr_res_8 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_9 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_10 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_affine_load_res_11 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_for_res_12 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_13 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_14 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_15 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_16 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fmul_res_17 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fadd_res_18 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_load_res_19 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_20 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_ataddr_res_21 [P0, P1, P2, P3] -> {  }
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] }
dep_order for A_llvm_func_arg_11_0 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_10_1 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_1_2 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_0_3 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_4 [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 > i3 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P0 and o3 > i3 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0) }
dep_order for A_memref_ataddr_res_5 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_6 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_7 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_memref_ataddr_res_8 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_9 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_10 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_affine_load_res_11 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_for_res_12 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_13 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_14 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_15 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_16 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fmul_res_17 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fadd_res_18 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_load_res_19 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_20 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_ataddr_res_21 [P0, P1, P2, P3] -> {  }
tagged_reads [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> A_llvm_func_arg_10_1[]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> A_llvm_func_arg_11_0[]; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> A_llvm_func_arg_0_3[]; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> A_llvm_func_arg_1_2[]; [RS1_affine_parallel[i0, i1, 0, i3] -> S15_affine_vector_load_Read0[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S13_affine_vector_load_Read0[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
atagged_reads [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> A_llvm_func_arg_1_2[]; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> A_llvm_func_arg_10_1[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> A_llvm_func_arg_0_3[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> A_llvm_func_arg_11_0[] }
reads [P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }
async_reads [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
tagged_may_writes [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
atagged_may_writes [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
may_writes [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
tagged_must_writes [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
atagged_must_writes [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
must_writes [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
async_must_writes [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
tagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> S34_affine_yield1[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield0[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield2[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }
atagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
must_kills [P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
live_in [P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
live_out [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
independence [P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : o2 < i2 or o2 > i2; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, i1, i2] : o0 < i0 or o0 > i0; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, i2] : o1 < i1 or o1 > i1; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, o2] : o2 > i2 or o2 < i2; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, i1, i2] : o0 > i0 or o0 < i0; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, i2] : o1 > i1 or o1 < i1; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, o2] : o2 < i2 or o2 > i2; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, i1, i2] : o0 < i0 or o0 > i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, i2] : o1 < i1 or o1 > i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, o2, i3] : o2 > i2 or o2 < i2; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[i0, i1, i2, o3] : o3 > i3 or o3 < i3; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, i2, i3] : o1 > i1 or o1 < i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, i1, i2, i3] : o0 > i0 or o0 < i0 }
dep_flow [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }
tagged_dep_flow [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }
atagged_dep_flow [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
dep_false [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, -1 + P0, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P0 > 0 and P2 > 0 and 0 <= i0 <= -2 + P1 and -16 + P2 <= 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and -16 + P2 <= 16i3 < P2; RS1_affine_parallel[i0, -1 + P0, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P0 > 0 and P2 > 0 and 0 <= i0 <= -2 + P1 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, -1 + P0, 0, i3] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and 0 <= i0 <= -2 + P1 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P1 and 0 <= i1 <= -2 + P0; RS3_affine_parallel[i0, -1 + P0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and 0 <= i0 <= -2 + P1; RS0_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0; RS0_affine_parallel[i0, -1 + P0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and P2 <= 0 and 0 <= i0 <= -2 + P1 }
dep_forced [P0, P1, P2, P3] -> {  }
dep_order [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 }
tagged_dep_order [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2 }
dep_async [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
array_order [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and ((16i3 < P2 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P2)); RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and ((16i3 < P2 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P2)); RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 }
tagger [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> S6_arith_index_cast[]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> S4_arith_index_cast[]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> S7_arith_index_cast[]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> S5_arith_index_cast[] }
atagger [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> S4_arith_index_cast[]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> S7_arith_index_cast[]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> S6_arith_index_cast[]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> S5_arith_index_cast[]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]] -> RS3_affine_parallel[(i0), (i1), (i2)] }
schedule
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  array_expansion: [ none ]
  child:
    schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }]"
    permutable: 1
    array_expansion: [ none ]
    child:
      schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS3_affine_parallel[i0, i1, i2] -> [(0)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
          child:
            schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
            array_expansion: [ none ]
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
        - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Schedule constraints:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
validity: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }"
coincidence: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }"
condition: "[P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }"
conditional_validity: "[P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2 }"
proximity: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }"
anti_proximity: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
live_range_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
live_range_maximal_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and (P2 >= 17 or P2 <= 0 or (0 < P2 <= 16)); [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
array_sizes: "[P0, P1, P2, P3] -> { A_memref_alloca_res_10[] -> [1024]; A_memref_alloca_res_4[] -> [1024]; A_memref_alloca_res_7[] -> [1024] }"
New Schedule:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  coincident: [ 1, 1 ]
  array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
  child:
    sequence:
    - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
    - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
      child:
        schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
        permutable: 1
        coincident: [ 1 ]
        array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
        child:
          set:
          - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
          - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Prefix schedule [P0, P1, P2, P3] -> {  }
[P0, P1, P2, P3] -> {  }
# YOU ARE HERE
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_4@0x2f6c8e70
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> []; RS3_affine_parallel[i0, i1, i2] -> []; RS2_affine_parallel[i0, i1, i2, i3] -> []; RS0_affine_parallel[i0, i1, i2] -> [] }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  # YOU ARE HERE
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Deltas [P0, P1, P2, P3] -> { [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Array A_memref_alloca_res_4@0x2f6c8e70
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    # YOU ARE HERE
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : (o0 = i0 and o1 = i1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 <= 0) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 >= 17) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0) }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Array A_memref_alloca_res_4@0x2f6c8e70
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      # YOU ARE HERE
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : (i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 1 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : (i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 >= 17) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 > 0 and P1 > 0 and P2 <= 0) }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : (i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 1 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Array A_memref_alloca_res_4@0x2f6c8e70
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        child:
          # YOU ARE HERE
          leaf
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 16i3; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          # YOU ARE HERE
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 16i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : exists (e0 = floor((i2)/16): o0 = i0 and o1 = i1 and o2 = 21 + i2 and 16e0 = i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and i2 < P2) }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : exists (e0 = floor((i2)/16): i3 = i0 and i4 = i1 and i5 = 21 + i2 and o0 = 0 and o1 = 0 and o2 = 21 and 16e0 = i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and i2 < P2) }
Array A_memref_alloca_res_10@0x2f689dc0
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : exists (e0 = floor((i2)/16): o0 = i0 and o1 = i1 and o2 = 21 + i2 and 16e0 = i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and i2 < P2) }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : exists (e0 = floor((i2)/16): i3 = i0 and i4 = i1 and i5 = 21 + i2 and o0 = 0 and o1 = 0 and o2 = 21 and 16e0 = i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and i2 < P2) }
Array A_memref_alloca_res_7@0x2f6cd8d0
 coincidence: 0
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 20 + P2 and o3 = 0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and o3 = 0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 16i3 and o3 = 1 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            # YOU ARE HERE
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 20 + P2 and o3 = 1; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and o3 = 0 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                # YOU ARE HERE
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - # YOU ARE HERE
                  filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + 16i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  child:
                    # YOU ARE HERE
                    leaf
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - # YOU ARE HERE
                  filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 20 + P2 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
                  child:
                    # YOU ARE HERE
                    leaf
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 16i3 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 20 + P2 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 16i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 16i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                # YOU ARE HERE
                leaf
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> {  }
New Schedule Prepared for GPU:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      mark: "allocate_array"
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
          child:
            mark: "allocate_array"
            child:
              schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }]"
              permutable: 1
              coincident: [ 1 ]
              array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
              child:
                set:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2] }"
                  child:
                    sequence:
                    - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                      child:
                        mark: "async_wait_group"
                    - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and 16i3 >= -16 + P2 and 16i3 < P2 and i3 >= 0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 <= 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 1 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and 16i3 >= -16 + P2 and 16i3 < P2 and i3 >= 0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 <= 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 1 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 20 + P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 21 + 16i3 }
RS2_affine_parallel@0x2f6a6a10
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : exists (e0 = floor((i0)/16): o0 = 21 + i0 and 16e0 = i0 and P0 > 0 and P1 > 0 and i0 >= 0 and i0 < P2) }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : exists (e0 = floor((i0)/16): o0 = 21 + i0 and 16e0 = i0 and P0 > 0 and P1 > 0 and i0 >= 0 and i0 < P2) }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS3_affine_parallel@0x2f6a7cc0
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 20 + P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 21 + 16i3 }
RS2_affine_parallel@0x2f6a6a10
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : exists (e0 = floor((i0)/16): o0 = 21 + i0 and 16e0 = i0 and P0 > 0 and P1 > 0 and i0 >= 0 and i0 < P2) }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : exists (e0 = floor((i0)/16): o0 = 21 + i0 and 16e0 = i0 and P0 > 0 and P1 > 0 and i0 >= 0 and i0 < P2) }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS3_affine_parallel@0x2f6a7cc0
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 16i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 21 + 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 20 + P2 }
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
RS3_affine_parallel@0x2f6a7cc0
New AST:
mark: grid_parallel@0x2
node:
  iterator:
    id: c0
  init:
    val: 0
  cond:
    op: lt
    args:
    - id: c0
    - id: P0@0x2f690eb0
  inc:
    val: 1
  body:
    iterator:
      id: c1
    init:
      val: 0
    cond:
      op: lt
      args:
      - id: c1
      - id: P1@0x2f690fa0
    inc:
      val: 1
    body:
      mark: allocate_array@0x2f898550
      node:
      - user:
          op: call
          args:
          - id: RS0_affine_parallel@0x2f6c4400
          - id: c1
          - id: c0
          - val: 0
          - op: call
            args:
            - id: A_memref_alloca_res_4@0x2f6c8e70
            - val: 0
            - val: 0
      - mark: allocate_array@0x2f7854c0
        node:
        - guard:
            op: ge
            args:
            - id: P2@0x2f6d2cd0
            - val: 1
          then:
            user:
              op: call
              args:
              - id: RS1_affine_parallel@0x2f68d170
              - id: c1
              - id: c0
              - val: 0
              - val: 0
              - op: call
                args:
                - id: A_memref_alloca_res_10@0x2f689dc0
                - val: 0
                - val: 0
                - val: 21
              - op: call
                args:
                - id: A_memref_alloca_res_7@0x2f6cd8d0
                - val: 0
                - val: 0
                - val: 21
        - 
          - iterator:
              id: c2
            init:
              val: 16
            cond:
              op: le
              args:
              - id: c2
              - op: add
                args:
                - id: P2@0x2f6d2cd0
                - val: 15
            inc:
              val: 16
            body:
            - guard:
                op: ge
                args:
                - id: P2@0x2f6d2cd0
                - op: add
                  args:
                  - id: c2
                  - val: 1
              then:
                user:
                  op: call
                  args:
                  - id: RS1_affine_parallel@0x2f68d170
                  - id: c1
                  - id: c0
                  - val: 0
                  - op: div
                    args:
                    - id: c2
                    - val: 16
                  - op: call
                    args:
                    - id: A_memref_alloca_res_10@0x2f689dc0
                    - val: 0
                    - val: 0
                    - op: pdiv_r
                      args:
                      - op: sub
                        args:
                        - id: c2
                        - val: 1
                      - val: 22
                  - op: call
                    args:
                    - id: A_memref_alloca_res_7@0x2f6cd8d0
                    - val: 0
                    - val: 0
                    - op: pdiv_r
                      args:
                      - op: sub
                        args:
                        - id: c2
                        - val: 1
                      - val: 22
            - mark: async_wait_group@0x2f89ca40
              node:
                user:
                  op: call
                  args:
                  - id: RS2_affine_parallel@0x2f6a6a10
                  - id: c1
                  - id: c0
                  - val: 0
                  - op: sub
                    args:
                    - op: div
                      args:
                      - id: c2
                      - val: 16
                    - val: 1
                  - op: call
                    args:
                    - id: A_memref_alloca_res_10@0x2f689dc0
                    - val: 0
                    - val: 0
                    - op: pdiv_r
                      args:
                      - op: add
                        args:
                        - id: c2
                        - val: 4
                      - val: 22
                  - op: call
                    args:
                    - id: A_memref_alloca_res_4@0x2f6c8e70
                    - val: 0
                    - val: 0
                    - val: 0
                  - op: call
                    args:
                    - id: A_memref_alloca_res_7@0x2f6cd8d0
                    - val: 0
                    - val: 0
                    - op: pdiv_r
                      args:
                      - op: add
                        args:
                        - id: c2
                        - val: 4
                      - val: 22
          - user:
              op: call
              args:
              - id: RS3_affine_parallel@0x2f6a7cc0
              - id: c1
              - id: c0
              - val: 0
              - op: call
                args:
                - id: A_memref_alloca_res_4@0x2f6c8e70
                - val: 0
                - val: 0
                - val: 0
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x2f6a7cc0
RS0_affine_parallel@0x2f6c4400
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + 16i3)]; RS3_affine_parallel[i0, i1, i2] -> [(20 + P2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(16i3)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 20 + P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 21 + 16i3 }
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
RS3_affine_parallel@0x2f6a7cc0
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 20 + P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 21 + 16i3 }
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
RS3_affine_parallel@0x2f6a7cc0
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 16i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 21 + 16i3; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 20 + P2 }
RS2_affine_parallel@0x2f6a6a10
RS1_affine_parallel@0x2f68d170
RS3_affine_parallel@0x2f6a7cc0
New func:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg35: index, %arg36: index, %arg37: index):
      "affine.store"(%0, %20, %arg35, %arg36, %arg37) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.minsi"(%28, %25) : (i64, i64) -> i64
    %30 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %32 = "arith.addi"(%29, %31) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %32, %30) ({
    ^bb0(%arg31: i64):
      %156 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %157 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %158 = "arith.divsi"(%arg31, %157) : (i64, i64) -> i64
      %159 = "arith.index_cast"(%arg31) : (i64) -> index
      %160 = "memref.subview"(%23, %159) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %161 = "arith.index_cast"(%arg31) : (i64) -> index
      %162 = "memref.subview"(%22, %161) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %163 = "arith.index_cast"(%158) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg32: index, %arg33: index, %arg34: index):
        %164 = "affine.vector_load"(%2, %arg33, %arg32, %arg12, %163, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%164, %160, %arg33, %arg32) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %165 = "affine.vector_load"(%1, %arg33, %arg32, %163, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%165, %162, %arg33, %arg32) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %33 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %34 = "arith.constant"() <{value = 19 : i64}> : () -> i64
    %35 = "arith.index_cast"(%5) : (index) -> i64
    %36 = "arith.addi"(%35, %34) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.addi"(%36, %38) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%33, %39, %37) ({
    ^bb0(%arg22: i64):
      %106 = "arith.constant"() <{value = 5 : i64}> : () -> i64
      %107 = "arith.subi"(%arg22, %106) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %108 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %109 = "arith.remsi"(%107, %108) : (i64, i64) -> i64
      %110 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %111 = "arith.cmpi"(%109, %110) <{predicate = 0 : i64}> : (i64, i64) -> i1
      "scf.if"(%111) ({
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %135 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %136 = "arith.constant"() <{value = 21 : i64}> : () -> i64
        %137 = "arith.subi"(%arg22, %136) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %138 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %139 = "arith.divsi"(%137, %138) : (i64, i64) -> i64
        %140 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %141 = "arith.remui"(%arg22, %140) : (i64, i64) -> i64
        %142 = "arith.index_cast"(%141) : (i64) -> index
        %143 = "memref.subview"(%23, %142) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %144 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %145 = "arith.remui"(%arg22, %144) : (i64, i64) -> i64
        %146 = "arith.index_cast"(%145) : (i64) -> index
        %147 = "memref.subview"(%22, %146) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg26: index, %arg27: index, %arg28: index):
          %148 = "affine.load"(%20, %arg26, %arg27, %arg28) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          "affine.store_var"(%148, %149) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
          %149 = "affine.for"(%148) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg29: index, %arg30: f32):
            %150 = "affine.vector_load"(%143, %arg27, %arg29) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %151 = "llvm.bitcast"(%150) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %152 = "affine.vector_load"(%147, %arg29, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %153 = "llvm.bitcast"(%152) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %154 = "llvm.fmul"(%151, %153) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %155 = "llvm.fadd"(%arg30, %154) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%155) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%149, %20, %arg26, %arg27, %arg28) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
        %112 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %113 = "arith.addi"(%arg22, %112) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %114 = "arith.index_cast"(%5) : (index) -> i64
        %115 = "arith.cmpi"(%114, %113) <{predicate = 5 : i64}> : (i64, i64) -> i1
        %116 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %117 = "arith.remsi"(%arg22, %116) : (i64, i64) -> i64
        %118 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %119 = "arith.cmpi"(%117, %118) <{predicate = 0 : i64}> : (i64, i64) -> i1
        %120 = "arith.andi"(%115, %119) : (i1, i1) -> i1
        "scf.if"(%120) ({
          %121 = "arith.constant"() <{value = 0 : i64}> : () -> i64
          %122 = "arith.constant"() <{value = 16 : i64}> : () -> i64
          %123 = "arith.divsi"(%arg22, %122) : (i64, i64) -> i64
          %124 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %125 = "arith.remui"(%arg22, %124) : (i64, i64) -> i64
          %126 = "arith.index_cast"(%125) : (i64) -> index
          %127 = "memref.subview"(%23, %126) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %128 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %129 = "arith.remui"(%arg22, %128) : (i64, i64) -> i64
          %130 = "arith.index_cast"(%129) : (i64) -> index
          %131 = "memref.subview"(%22, %130) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %132 = "arith.index_cast"(%123) : (i64) -> index
          "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
          ^bb0(%arg23: index, %arg24: index, %arg25: index):
            %133 = "affine.vector_load"(%2, %arg24, %arg23, %arg12, %132, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
            "affine.vector_store"(%133, %127, %arg24, %arg23) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
            %134 = "affine.vector_load"(%1, %arg24, %arg23, %132, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
            "affine.vector_store"(%134, %131, %arg24, %arg23) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
            "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
          }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %40 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %41 = "arith.index_cast"(%5) : (index) -> i64
    %42 = "arith.cmpi"(%41, %40) <{predicate = 5 : i64}> : (i64, i64) -> i1
    %43 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %44 = "arith.index_cast"(%5) : (index) -> i64
    %45 = "arith.subi"(%44, %43) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %46 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %47 = "arith.remsi"(%45, %46) : (i64, i64) -> i64
    %48 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %49 = "arith.cmpi"(%47, %48) <{predicate = 0 : i64}> : (i64, i64) -> i1
    %50 = "arith.andi"(%42, %49) : (i1, i1) -> i1
    "scf.if"(%50) ({
      "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
      %54 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %56 = "arith.index_cast"(%5) : (index) -> i64
      %57 = "arith.subi"(%56, %55) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %58 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %59 = "arith.divsi"(%57, %58) : (i64, i64) -> i64
      %60 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %61 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %62 = "arith.index_cast"(%5) : (index) -> i64
      %63 = "arith.addi"(%62, %61) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %66 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %67 = "arith.subi"(%63, %64) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %68 = "arith.addi"(%67, %65) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.cmpi"(%63, %66) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %70 = "arith.select"(%69, %68, %63) : (i1, i64, i64) -> i64
      %71 = "arith.divsi"(%70, %64) : (i64, i64) -> i64
      %72 = "arith.muli"(%60, %71) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %73 = "arith.index_cast"(%5) : (index) -> i64
      %74 = "arith.subi"(%73, %72) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %76 = "arith.addi"(%74, %75) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%23, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %80 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %81 = "arith.index_cast"(%5) : (index) -> i64
      %82 = "arith.addi"(%81, %80) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %84 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %85 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %86 = "arith.subi"(%82, %83) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %87 = "arith.addi"(%86, %84) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %88 = "arith.cmpi"(%82, %85) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %89 = "arith.select"(%88, %87, %82) : (i1, i64, i64) -> i64
      %90 = "arith.divsi"(%89, %83) : (i64, i64) -> i64
      %91 = "arith.muli"(%79, %90) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %92 = "arith.index_cast"(%5) : (index) -> i64
      %93 = "arith.subi"(%92, %91) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %94 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %95 = "arith.addi"(%93, %94) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %96 = "arith.index_cast"(%95) : (i64) -> index
      %97 = "memref.subview"(%22, %96) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg17: index, %arg18: index, %arg19: index):
        %98 = "affine.load"(%20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        "affine.store_var"(%98, %99) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
        %99 = "affine.for"(%98) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg20: index, %arg21: f32):
          %100 = "affine.vector_load"(%78, %arg18, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %101 = "llvm.bitcast"(%100) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %102 = "affine.vector_load"(%97, %arg20, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %103 = "llvm.bitcast"(%102) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %104 = "llvm.fmul"(%101, %103) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %105 = "llvm.fadd"(%arg21, %104) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%105) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%99, %20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    %51 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %52 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %53 = "llvm.bitcast"(%52) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%53, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
Found copy
Found copy
Found copy
Found copy
Converted to async:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg41: index, %arg42: index, %arg43: index):
      "affine.store"(%0, %20, %arg41, %arg42, %arg43) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.minsi"(%28, %25) : (i64, i64) -> i64
    %30 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %32 = "arith.addi"(%29, %31) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %32, %30) ({
    ^bb0(%arg34: i64):
      %160 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %161 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %162 = "arith.divsi"(%arg34, %161) : (i64, i64) -> i64
      %163 = "arith.index_cast"(%arg34) : (i64) -> index
      %164 = "memref.subview"(%23, %163) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %165 = "arith.index_cast"(%arg34) : (i64) -> index
      %166 = "memref.subview"(%22, %165) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %167 = "arith.index_cast"(%162) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg35: index, %arg36: index, %arg37: index):
        "affine.if"(%arg35, %arg36, %arg37) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg38: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg39: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg40: index):
                %168 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %169 = "affine.apply"(%arg39, %arg38, %arg12, %167, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %170 = "nvgpu.device_async_copy"(%164, %168, %2, %169) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %171 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %172 = "affine.apply"(%arg39, %arg38, %167, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %173 = "nvgpu.device_async_copy"(%166, %171, %1, %172) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %33 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %34 = "arith.constant"() <{value = 19 : i64}> : () -> i64
    %35 = "arith.index_cast"(%5) : (index) -> i64
    %36 = "arith.addi"(%35, %34) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.addi"(%36, %38) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%33, %39, %37) ({
    ^bb0(%arg22: i64):
      %106 = "arith.constant"() <{value = 5 : i64}> : () -> i64
      %107 = "arith.subi"(%arg22, %106) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %108 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %109 = "arith.remsi"(%107, %108) : (i64, i64) -> i64
      %110 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %111 = "arith.cmpi"(%109, %110) <{predicate = 0 : i64}> : (i64, i64) -> i1
      "scf.if"(%111) ({
        %139 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %140 = "arith.constant"() <{value = 21 : i64}> : () -> i64
        %141 = "arith.subi"(%arg22, %140) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %142 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %143 = "arith.divsi"(%141, %142) : (i64, i64) -> i64
        %144 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %145 = "arith.remui"(%arg22, %144) : (i64, i64) -> i64
        %146 = "arith.index_cast"(%145) : (i64) -> index
        %147 = "memref.subview"(%23, %146) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %148 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %149 = "arith.remui"(%arg22, %148) : (i64, i64) -> i64
        %150 = "arith.index_cast"(%149) : (i64) -> index
        %151 = "memref.subview"(%22, %150) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg29: index, %arg30: index, %arg31: index):
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %152 = "affine.load"(%20, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          "affine.store_var"(%152, %153) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
          %153 = "affine.for"(%152) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg32: index, %arg33: f32):
            %154 = "affine.vector_load"(%147, %arg30, %arg32) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %155 = "llvm.bitcast"(%154) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %156 = "affine.vector_load"(%151, %arg32, %arg29) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %157 = "llvm.bitcast"(%156) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %158 = "llvm.fmul"(%155, %157) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %159 = "llvm.fadd"(%arg33, %158) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%159) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%153, %20, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
        %112 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %113 = "arith.addi"(%arg22, %112) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %114 = "arith.index_cast"(%5) : (index) -> i64
        %115 = "arith.cmpi"(%114, %113) <{predicate = 5 : i64}> : (i64, i64) -> i1
        %116 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %117 = "arith.remsi"(%arg22, %116) : (i64, i64) -> i64
        %118 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %119 = "arith.cmpi"(%117, %118) <{predicate = 0 : i64}> : (i64, i64) -> i1
        %120 = "arith.andi"(%115, %119) : (i1, i1) -> i1
        "scf.if"(%120) ({
          %121 = "arith.constant"() <{value = 0 : i64}> : () -> i64
          %122 = "arith.constant"() <{value = 16 : i64}> : () -> i64
          %123 = "arith.divsi"(%arg22, %122) : (i64, i64) -> i64
          %124 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %125 = "arith.remui"(%arg22, %124) : (i64, i64) -> i64
          %126 = "arith.index_cast"(%125) : (i64) -> index
          %127 = "memref.subview"(%23, %126) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %128 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %129 = "arith.remui"(%arg22, %128) : (i64, i64) -> i64
          %130 = "arith.index_cast"(%129) : (i64) -> index
          %131 = "memref.subview"(%22, %130) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %132 = "arith.index_cast"(%123) : (i64) -> index
          "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
          ^bb0(%arg23: index, %arg24: index, %arg25: index):
            "affine.if"(%arg23, %arg24, %arg25) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg26: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg27: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg28: index):
                    %133 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %134 = "affine.apply"(%arg27, %arg26, %arg12, %132, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %135 = "nvgpu.device_async_copy"(%127, %133, %2, %134) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %136 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %137 = "affine.apply"(%arg27, %arg26, %132, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %138 = "nvgpu.device_async_copy"(%131, %136, %1, %137) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "affine.yield"() : () -> ()
          }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %40 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %41 = "arith.index_cast"(%5) : (index) -> i64
    %42 = "arith.cmpi"(%41, %40) <{predicate = 5 : i64}> : (i64, i64) -> i1
    %43 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %44 = "arith.index_cast"(%5) : (index) -> i64
    %45 = "arith.subi"(%44, %43) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %46 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %47 = "arith.remsi"(%45, %46) : (i64, i64) -> i64
    %48 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %49 = "arith.cmpi"(%47, %48) <{predicate = 0 : i64}> : (i64, i64) -> i1
    %50 = "arith.andi"(%42, %49) : (i1, i1) -> i1
    "scf.if"(%50) ({
      %54 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %56 = "arith.index_cast"(%5) : (index) -> i64
      %57 = "arith.subi"(%56, %55) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %58 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %59 = "arith.divsi"(%57, %58) : (i64, i64) -> i64
      %60 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %61 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %62 = "arith.index_cast"(%5) : (index) -> i64
      %63 = "arith.addi"(%62, %61) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %66 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %67 = "arith.subi"(%63, %64) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %68 = "arith.addi"(%67, %65) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.cmpi"(%63, %66) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %70 = "arith.select"(%69, %68, %63) : (i1, i64, i64) -> i64
      %71 = "arith.divsi"(%70, %64) : (i64, i64) -> i64
      %72 = "arith.muli"(%60, %71) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %73 = "arith.index_cast"(%5) : (index) -> i64
      %74 = "arith.subi"(%73, %72) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %76 = "arith.addi"(%74, %75) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%23, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %80 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %81 = "arith.index_cast"(%5) : (index) -> i64
      %82 = "arith.addi"(%81, %80) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %84 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %85 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %86 = "arith.subi"(%82, %83) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %87 = "arith.addi"(%86, %84) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %88 = "arith.cmpi"(%82, %85) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %89 = "arith.select"(%88, %87, %82) : (i1, i64, i64) -> i64
      %90 = "arith.divsi"(%89, %83) : (i64, i64) -> i64
      %91 = "arith.muli"(%79, %90) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %92 = "arith.index_cast"(%5) : (index) -> i64
      %93 = "arith.subi"(%92, %91) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %94 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %95 = "arith.addi"(%93, %94) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %96 = "arith.index_cast"(%95) : (i64) -> index
      %97 = "memref.subview"(%22, %96) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg17: index, %arg18: index, %arg19: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %98 = "affine.load"(%20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        "affine.store_var"(%98, %99) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
        %99 = "affine.for"(%98) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg20: index, %arg21: f32):
          %100 = "affine.vector_load"(%78, %arg18, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %101 = "llvm.bitcast"(%100) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %102 = "affine.vector_load"(%97, %arg20, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %103 = "llvm.bitcast"(%102) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %104 = "llvm.fmul"(%101, %103) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %105 = "llvm.fadd"(%arg21, %104) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%105) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%99, %20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    %51 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %52 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %53 = "llvm.bitcast"(%52) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%53, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
/scr/ivan/src/transformer-llvm-project/polly/lib/External/isl/isl_ctx.c:307: isl_ctx not freed as some objects still reference it
gpu-affine-opt: After opt:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg41: index, %arg42: index, %arg43: index):
      "affine.store"(%0, %20, %arg41, %arg42, %arg43) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.minsi"(%28, %25) : (i64, i64) -> i64
    %30 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %32 = "arith.addi"(%29, %31) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %32, %30) ({
    ^bb0(%arg34: i64):
      %160 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %161 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %162 = "arith.divsi"(%arg34, %161) : (i64, i64) -> i64
      %163 = "arith.index_cast"(%arg34) : (i64) -> index
      %164 = "memref.subview"(%23, %163) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %165 = "arith.index_cast"(%arg34) : (i64) -> index
      %166 = "memref.subview"(%22, %165) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %167 = "arith.index_cast"(%162) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg35: index, %arg36: index, %arg37: index):
        "affine.if"(%arg35, %arg36, %arg37) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg38: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg39: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg40: index):
                %168 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %169 = "affine.apply"(%arg39, %arg38, %arg12, %167, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %170 = "nvgpu.device_async_copy"(%164, %168, %2, %169) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %171 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %172 = "affine.apply"(%arg39, %arg38, %167, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %173 = "nvgpu.device_async_copy"(%166, %171, %1, %172) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %33 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %34 = "arith.constant"() <{value = 19 : i64}> : () -> i64
    %35 = "arith.index_cast"(%5) : (index) -> i64
    %36 = "arith.addi"(%35, %34) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.addi"(%36, %38) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%33, %39, %37) ({
    ^bb0(%arg22: i64):
      %106 = "arith.constant"() <{value = 5 : i64}> : () -> i64
      %107 = "arith.subi"(%arg22, %106) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %108 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %109 = "arith.remsi"(%107, %108) : (i64, i64) -> i64
      %110 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %111 = "arith.cmpi"(%109, %110) <{predicate = 0 : i64}> : (i64, i64) -> i1
      "scf.if"(%111) ({
        %139 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %140 = "arith.constant"() <{value = 21 : i64}> : () -> i64
        %141 = "arith.subi"(%arg22, %140) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %142 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %143 = "arith.divsi"(%141, %142) : (i64, i64) -> i64
        %144 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %145 = "arith.remui"(%arg22, %144) : (i64, i64) -> i64
        %146 = "arith.index_cast"(%145) : (i64) -> index
        %147 = "memref.subview"(%23, %146) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %148 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %149 = "arith.remui"(%arg22, %148) : (i64, i64) -> i64
        %150 = "arith.index_cast"(%149) : (i64) -> index
        %151 = "memref.subview"(%22, %150) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg29: index, %arg30: index, %arg31: index):
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %152 = "affine.load"(%20, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          %153 = "affine.for"(%152) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg32: index, %arg33: f32):
            %154 = "affine.vector_load"(%147, %arg30, %arg32) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %155 = "llvm.bitcast"(%154) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %156 = "affine.vector_load"(%151, %arg32, %arg29) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %157 = "llvm.bitcast"(%156) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %158 = "llvm.fmul"(%155, %157) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %159 = "llvm.fadd"(%arg33, %158) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%159) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%153, %20, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
        %112 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %113 = "arith.addi"(%arg22, %112) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %114 = "arith.index_cast"(%5) : (index) -> i64
        %115 = "arith.cmpi"(%114, %113) <{predicate = 5 : i64}> : (i64, i64) -> i1
        %116 = "arith.constant"() <{value = 16 : i64}> : () -> i64
        %117 = "arith.remsi"(%arg22, %116) : (i64, i64) -> i64
        %118 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %119 = "arith.cmpi"(%117, %118) <{predicate = 0 : i64}> : (i64, i64) -> i1
        %120 = "arith.andi"(%115, %119) : (i1, i1) -> i1
        "scf.if"(%120) ({
          %121 = "arith.constant"() <{value = 0 : i64}> : () -> i64
          %122 = "arith.constant"() <{value = 16 : i64}> : () -> i64
          %123 = "arith.divsi"(%arg22, %122) : (i64, i64) -> i64
          %124 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %125 = "arith.remui"(%arg22, %124) : (i64, i64) -> i64
          %126 = "arith.index_cast"(%125) : (i64) -> index
          %127 = "memref.subview"(%23, %126) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %128 = "arith.constant"() <{value = 22 : i64}> : () -> i64
          %129 = "arith.remui"(%arg22, %128) : (i64, i64) -> i64
          %130 = "arith.index_cast"(%129) : (i64) -> index
          %131 = "memref.subview"(%22, %130) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %132 = "arith.index_cast"(%123) : (i64) -> index
          "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
          ^bb0(%arg23: index, %arg24: index, %arg25: index):
            "affine.if"(%arg23, %arg24, %arg25) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg26: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg27: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg28: index):
                    %133 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %134 = "affine.apply"(%arg27, %arg26, %arg12, %132, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %135 = "nvgpu.device_async_copy"(%127, %133, %2, %134) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %136 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %137 = "affine.apply"(%arg27, %arg26, %132, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %138 = "nvgpu.device_async_copy"(%131, %136, %1, %137) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "affine.yield"() : () -> ()
          }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %40 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %41 = "arith.index_cast"(%5) : (index) -> i64
    %42 = "arith.cmpi"(%41, %40) <{predicate = 5 : i64}> : (i64, i64) -> i1
    %43 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %44 = "arith.index_cast"(%5) : (index) -> i64
    %45 = "arith.subi"(%44, %43) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %46 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %47 = "arith.remsi"(%45, %46) : (i64, i64) -> i64
    %48 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %49 = "arith.cmpi"(%47, %48) <{predicate = 0 : i64}> : (i64, i64) -> i1
    %50 = "arith.andi"(%42, %49) : (i1, i1) -> i1
    "scf.if"(%50) ({
      %54 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %56 = "arith.index_cast"(%5) : (index) -> i64
      %57 = "arith.subi"(%56, %55) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %58 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %59 = "arith.divsi"(%57, %58) : (i64, i64) -> i64
      %60 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %61 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %62 = "arith.index_cast"(%5) : (index) -> i64
      %63 = "arith.addi"(%62, %61) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %66 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %67 = "arith.subi"(%63, %64) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %68 = "arith.addi"(%67, %65) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.cmpi"(%63, %66) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %70 = "arith.select"(%69, %68, %63) : (i1, i64, i64) -> i64
      %71 = "arith.divsi"(%70, %64) : (i64, i64) -> i64
      %72 = "arith.muli"(%60, %71) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %73 = "arith.index_cast"(%5) : (index) -> i64
      %74 = "arith.subi"(%73, %72) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %76 = "arith.addi"(%74, %75) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%23, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %80 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %81 = "arith.index_cast"(%5) : (index) -> i64
      %82 = "arith.addi"(%81, %80) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %84 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %85 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %86 = "arith.subi"(%82, %83) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %87 = "arith.addi"(%86, %84) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %88 = "arith.cmpi"(%82, %85) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %89 = "arith.select"(%88, %87, %82) : (i1, i64, i64) -> i64
      %90 = "arith.divsi"(%89, %83) : (i64, i64) -> i64
      %91 = "arith.muli"(%79, %90) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %92 = "arith.index_cast"(%5) : (index) -> i64
      %93 = "arith.subi"(%92, %91) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %94 = "arith.constant"() <{value = 20 : i64}> : () -> i64
      %95 = "arith.addi"(%93, %94) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %96 = "arith.index_cast"(%95) : (i64) -> index
      %97 = "memref.subview"(%22, %96) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg17: index, %arg18: index, %arg19: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %98 = "affine.load"(%20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %99 = "affine.for"(%98) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg20: index, %arg21: f32):
          %100 = "affine.vector_load"(%78, %arg18, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %101 = "llvm.bitcast"(%100) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %102 = "affine.vector_load"(%97, %arg20, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %103 = "llvm.bitcast"(%102) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %104 = "llvm.fmul"(%101, %103) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %105 = "llvm.fadd"(%arg21, %104) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%105) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%99, %20, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    %51 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %52 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %53 = "llvm.bitcast"(%52) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%53, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After shmem to alloca:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 5 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 19 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %10 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %11 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %14 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %16 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %18 = "arith.index_cast"(%16) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  %20 = "arith.index_cast"(%17) : (index) -> i64
  %21 = "arith.index_cast"(%20) : (i64) -> index
  "scf.parallel"(%1, %1, %19, %21, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg41: index, %arg42: index, %arg43: index):
      "affine.store"(%10, %22, %arg41, %arg42, %arg43) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %23 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
    %24 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
    %25 = "arith.index_cast"(%15) : (index) -> i64
    %26 = "arith.subi"(%25, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %27 = "arith.minsi"(%26, %7) : (i64, i64) -> i64
    %28 = "arith.addi"(%27, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%9, %28, %6) ({
    ^bb0(%arg34: i64):
      %112 = "arith.divsi"(%arg34, %6) : (i64, i64) -> i64
      %113 = "arith.index_cast"(%arg34) : (i64) -> index
      %114 = "memref.subview"(%24, %113) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %115 = "arith.index_cast"(%arg34) : (i64) -> index
      %116 = "memref.subview"(%23, %115) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %117 = "arith.index_cast"(%112) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg35: index, %arg36: index, %arg37: index):
        "affine.if"(%arg35, %arg36, %arg37) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg38: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg39: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg40: index):
                %118 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %119 = "affine.apply"(%arg39, %arg38, %arg12, %117, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %120 = "nvgpu.device_async_copy"(%114, %118, %12, %119) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %121 = "affine.apply"(%arg39, %arg38) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %122 = "affine.apply"(%arg39, %arg38, %117, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %123 = "nvgpu.device_async_copy"(%116, %121, %11, %122) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %29 = "arith.index_cast"(%15) : (index) -> i64
    %30 = "arith.addi"(%29, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %31 = "arith.addi"(%30, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%5, %31, %8) ({
    ^bb0(%arg22: i64):
      %75 = "arith.subi"(%arg22, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %76 = "arith.remsi"(%75, %6) : (i64, i64) -> i64
      %77 = "arith.cmpi"(%76, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
      "scf.if"(%77) ({
        %98 = "arith.remui"(%arg22, %2) : (i64, i64) -> i64
        %99 = "arith.index_cast"(%98) : (i64) -> index
        %100 = "memref.subview"(%24, %99) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %101 = "arith.remui"(%arg22, %2) : (i64, i64) -> i64
        %102 = "arith.index_cast"(%101) : (i64) -> index
        %103 = "memref.subview"(%23, %102) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg29: index, %arg30: index, %arg31: index):
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %104 = "affine.load"(%22, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          %105 = "affine.for"(%104) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg32: index, %arg33: f32):
            %106 = "affine.vector_load"(%100, %arg30, %arg32) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %107 = "llvm.bitcast"(%106) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %108 = "affine.vector_load"(%103, %arg32, %arg29) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %109 = "llvm.bitcast"(%108) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %110 = "llvm.fmul"(%107, %109) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %111 = "llvm.fadd"(%arg33, %110) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%111) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%105, %22, %arg29, %arg30, %arg31) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
        %78 = "arith.addi"(%arg22, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %79 = "arith.index_cast"(%15) : (index) -> i64
        %80 = "arith.cmpi"(%79, %78) <{predicate = 5 : i64}> : (i64, i64) -> i1
        %81 = "arith.remsi"(%arg22, %6) : (i64, i64) -> i64
        %82 = "arith.cmpi"(%81, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
        %83 = "arith.andi"(%80, %82) : (i1, i1) -> i1
        "scf.if"(%83) ({
          %84 = "arith.divsi"(%arg22, %6) : (i64, i64) -> i64
          %85 = "arith.remui"(%arg22, %2) : (i64, i64) -> i64
          %86 = "arith.index_cast"(%85) : (i64) -> index
          %87 = "memref.subview"(%24, %86) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %88 = "arith.remui"(%arg22, %2) : (i64, i64) -> i64
          %89 = "arith.index_cast"(%88) : (i64) -> index
          %90 = "memref.subview"(%23, %89) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %91 = "arith.index_cast"(%84) : (i64) -> index
          "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
          ^bb0(%arg23: index, %arg24: index, %arg25: index):
            "affine.if"(%arg23, %arg24, %arg25) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg26: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg27: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg28: index):
                    %92 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %93 = "affine.apply"(%arg27, %arg26, %arg12, %91, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %94 = "nvgpu.device_async_copy"(%87, %92, %12, %93) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %95 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %96 = "affine.apply"(%arg27, %arg26, %91, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %97 = "nvgpu.device_async_copy"(%90, %95, %11, %96) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "affine.yield"() : () -> ()
          }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %32 = "arith.index_cast"(%15) : (index) -> i64
    %33 = "arith.cmpi"(%32, %8) <{predicate = 5 : i64}> : (i64, i64) -> i1
    %34 = "arith.index_cast"(%15) : (index) -> i64
    %35 = "arith.subi"(%34, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %36 = "arith.remsi"(%35, %6) : (i64, i64) -> i64
    %37 = "arith.cmpi"(%36, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
    %38 = "arith.andi"(%33, %37) : (i1, i1) -> i1
    "scf.if"(%38) ({
      %41 = "arith.index_cast"(%15) : (index) -> i64
      %42 = "arith.addi"(%41, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %43 = "arith.subi"(%42, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %44 = "arith.addi"(%43, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %45 = "arith.cmpi"(%42, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %46 = "arith.select"(%45, %44, %42) : (i1, i64, i64) -> i64
      %47 = "arith.divsi"(%46, %2) : (i64, i64) -> i64
      %48 = "arith.muli"(%47, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %49 = "arith.index_cast"(%15) : (index) -> i64
      %50 = "arith.subi"(%49, %48) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %51 = "arith.addi"(%50, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %52 = "arith.index_cast"(%51) : (i64) -> index
      %53 = "memref.subview"(%24, %52) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %54 = "arith.index_cast"(%15) : (index) -> i64
      %55 = "arith.addi"(%54, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %56 = "arith.subi"(%55, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %57 = "arith.addi"(%56, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %58 = "arith.cmpi"(%55, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %59 = "arith.select"(%58, %57, %55) : (i1, i64, i64) -> i64
      %60 = "arith.divsi"(%59, %2) : (i64, i64) -> i64
      %61 = "arith.muli"(%60, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %62 = "arith.index_cast"(%15) : (index) -> i64
      %63 = "arith.subi"(%62, %61) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.addi"(%63, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %65 = "arith.index_cast"(%64) : (i64) -> index
      %66 = "memref.subview"(%23, %65) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg17: index, %arg18: index, %arg19: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %67 = "affine.load"(%22, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %68 = "affine.for"(%67) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg20: index, %arg21: f32):
          %69 = "affine.vector_load"(%53, %arg18, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %70 = "llvm.bitcast"(%69) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %71 = "affine.vector_load"(%66, %arg20, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %72 = "llvm.bitcast"(%71) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %73 = "llvm.fmul"(%70, %72) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %74 = "llvm.fadd"(%arg21, %73) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%74) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%68, %22, %arg17, %arg18, %arg19) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %39 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %40 = "llvm.bitcast"(%39) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%40, %13, %arg13, %arg14, %arg15, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After gpuify:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 5 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 19 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %10 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %11 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %14 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %16 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %18 = "arith.index_cast"(%16) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  %20 = "arith.index_cast"(%17) : (index) -> i64
  %21 = "arith.index_cast"(%20) : (i64) -> index
  "scf.parallel"(%1, %1, %19, %21, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      "affine.store"(%10, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %23 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %24 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %25 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %26 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %27 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %28 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %29 = "arith.index_cast"(%15) : (index) -> i64
      %30 = "arith.subi"(%29, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %31 = "arith.minsi"(%30, %7) : (i64, i64) -> i64
      %32 = "arith.addi"(%31, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%9, %32, %6) ({
      ^bb0(%arg25: i64):
        %144 = "arith.divsi"(%arg25, %6) : (i64, i64) -> i64
        %145 = "arith.index_cast"(%arg25) : (i64) -> index
        %146 = "memref.subview"(%28, %145) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %147 = "arith.index_cast"(%arg25) : (i64) -> index
        %148 = "memref.subview"(%27, %147) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %149 = "arith.index_cast"(%144) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg26: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg27: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg28: index):
                %150 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %151 = "affine.apply"(%arg27, %arg26, %arg12, %149, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %152 = "nvgpu.device_async_copy"(%146, %150, %12, %151) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %153 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %154 = "affine.apply"(%arg27, %arg26, %149, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %155 = "nvgpu.device_async_copy"(%148, %153, %11, %154) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %33 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %34 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %35 = "arith.index_cast"(%15) : (index) -> i64
      %36 = "arith.addi"(%35, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %37 = "arith.addi"(%36, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%5, %37, %8) ({
      ^bb0(%arg19: i64):
        %107 = "arith.subi"(%arg19, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %108 = "arith.remsi"(%107, %6) : (i64, i64) -> i64
        %109 = "arith.cmpi"(%108, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
        "scf.if"(%109) ({
          %130 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %131 = "arith.index_cast"(%130) : (i64) -> index
          %132 = "memref.subview"(%34, %131) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %133 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %134 = "arith.index_cast"(%133) : (i64) -> index
          %135 = "memref.subview"(%33, %134) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %136 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          %137 = "affine.for"(%136) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg23: index, %arg24: f32):
            %138 = "affine.vector_load"(%132, %arg15, %arg23) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %139 = "llvm.bitcast"(%138) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %140 = "affine.vector_load"(%135, %arg23, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %141 = "llvm.bitcast"(%140) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %142 = "llvm.fmul"(%139, %141) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %143 = "llvm.fadd"(%arg24, %142) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%143) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%137, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
          %110 = "arith.addi"(%arg19, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %111 = "arith.index_cast"(%15) : (index) -> i64
          %112 = "arith.cmpi"(%111, %110) <{predicate = 5 : i64}> : (i64, i64) -> i1
          %113 = "arith.remsi"(%arg19, %6) : (i64, i64) -> i64
          %114 = "arith.cmpi"(%113, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
          %115 = "arith.andi"(%112, %114) : (i1, i1) -> i1
          "scf.if"(%115) ({
            %116 = "arith.divsi"(%arg19, %6) : (i64, i64) -> i64
            %117 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %118 = "arith.index_cast"(%117) : (i64) -> index
            %119 = "memref.subview"(%34, %118) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %120 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %121 = "arith.index_cast"(%120) : (i64) -> index
            %122 = "memref.subview"(%33, %121) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %123 = "arith.index_cast"(%116) : (i64) -> index
            "affine.if"(%arg14, %arg15, %arg16) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg20: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg21: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg22: index):
                    %124 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %125 = "affine.apply"(%arg21, %arg20, %arg12, %123, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %126 = "nvgpu.device_async_copy"(%119, %124, %12, %125) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %127 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %128 = "affine.apply"(%arg21, %arg20, %123, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %129 = "nvgpu.device_async_copy"(%122, %127, %11, %128) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %38 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %39 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %40 = "arith.index_cast"(%15) : (index) -> i64
      %41 = "arith.addi"(%40, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %42 = "arith.addi"(%41, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %43 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %44 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %45 = "arith.index_cast"(%15) : (index) -> i64
      %46 = "arith.addi"(%45, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %47 = "arith.addi"(%46, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %48 = "arith.index_cast"(%15) : (index) -> i64
      %49 = "arith.cmpi"(%48, %8) <{predicate = 5 : i64}> : (i64, i64) -> i1
      %50 = "arith.index_cast"(%15) : (index) -> i64
      %51 = "arith.subi"(%50, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %52 = "arith.remsi"(%51, %6) : (i64, i64) -> i64
      %53 = "arith.cmpi"(%52, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
      %54 = "arith.andi"(%49, %53) : (i1, i1) -> i1
      "scf.if"(%54) ({
        %73 = "arith.index_cast"(%15) : (index) -> i64
        %74 = "arith.addi"(%73, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %75 = "arith.subi"(%74, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %76 = "arith.addi"(%75, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %77 = "arith.cmpi"(%74, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %78 = "arith.select"(%77, %76, %74) : (i1, i64, i64) -> i64
        %79 = "arith.divsi"(%78, %2) : (i64, i64) -> i64
        %80 = "arith.muli"(%79, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %81 = "arith.index_cast"(%15) : (index) -> i64
        %82 = "arith.subi"(%81, %80) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %83 = "arith.addi"(%82, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %84 = "arith.index_cast"(%83) : (i64) -> index
        %85 = "memref.subview"(%44, %84) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %86 = "arith.index_cast"(%15) : (index) -> i64
        %87 = "arith.addi"(%86, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.subi"(%87, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %89 = "arith.addi"(%88, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %90 = "arith.cmpi"(%87, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %91 = "arith.select"(%90, %89, %87) : (i1, i64, i64) -> i64
        %92 = "arith.divsi"(%91, %2) : (i64, i64) -> i64
        %93 = "arith.muli"(%92, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %94 = "arith.index_cast"(%15) : (index) -> i64
        %95 = "arith.subi"(%94, %93) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %96 = "arith.addi"(%95, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %97 = "arith.index_cast"(%96) : (i64) -> index
        %98 = "memref.subview"(%43, %97) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %99 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %100 = "affine.for"(%99) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg17: index, %arg18: f32):
          %101 = "affine.vector_load"(%85, %arg15, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %102 = "llvm.bitcast"(%101) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %103 = "affine.vector_load"(%98, %arg17, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %104 = "llvm.bitcast"(%103) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %105 = "llvm.fmul"(%102, %104) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %106 = "llvm.fadd"(%arg18, %105) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%106) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%100, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      %55 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %56 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %57 = "arith.index_cast"(%15) : (index) -> i64
      %58 = "arith.subi"(%57, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %59 = "arith.minsi"(%58, %7) : (i64, i64) -> i64
      %60 = "arith.addi"(%59, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %61 = "arith.index_cast"(%15) : (index) -> i64
      %62 = "arith.addi"(%61, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %63 = "arith.addi"(%62, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.index_cast"(%15) : (index) -> i64
      %65 = "arith.cmpi"(%64, %8) <{predicate = 5 : i64}> : (i64, i64) -> i1
      %66 = "arith.index_cast"(%15) : (index) -> i64
      %67 = "arith.subi"(%66, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %68 = "arith.remsi"(%67, %6) : (i64, i64) -> i64
      %69 = "arith.cmpi"(%68, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
      %70 = "arith.andi"(%65, %69) : (i1, i1) -> i1
      %71 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %72 = "llvm.bitcast"(%71) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%72, %13, %arg13, %arg14, %arg15, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After expand subview:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 5 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 19 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %10 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %11 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %14 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %16 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %18 = "arith.index_cast"(%16) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  %20 = "arith.index_cast"(%17) : (index) -> i64
  %21 = "arith.index_cast"(%20) : (i64) -> index
  "scf.parallel"(%1, %1, %19, %21, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      "affine.store"(%10, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %23 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %24 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %25 = "arith.index_cast"(%15) : (index) -> i64
      %26 = "arith.subi"(%25, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %27 = "arith.minsi"(%26, %7) : (i64, i64) -> i64
      %28 = "arith.addi"(%27, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%9, %28, %6) ({
      ^bb0(%arg25: i64):
        %122 = "arith.divsi"(%arg25, %6) : (i64, i64) -> i64
        %123 = "arith.index_cast"(%arg25) : (i64) -> index
        %124 = "affine.apply"(%123) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %125 = "memref.reinterpret_cast"(%24, %124) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %126 = "arith.index_cast"(%arg25) : (i64) -> index
        %127 = "affine.apply"(%126) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %128 = "memref.reinterpret_cast"(%23, %127) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %129 = "arith.index_cast"(%122) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg26: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg27: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg28: index):
                %130 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %131 = "affine.apply"(%arg27, %arg26, %arg12, %129, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %132 = "nvgpu.device_async_copy"(%125, %130, %12, %131) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %133 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %134 = "affine.apply"(%arg27, %arg26, %129, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %135 = "nvgpu.device_async_copy"(%128, %133, %11, %134) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %29 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %30 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %31 = "arith.index_cast"(%15) : (index) -> i64
      %32 = "arith.addi"(%31, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %33 = "arith.addi"(%32, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%5, %33, %8) ({
      ^bb0(%arg19: i64):
        %81 = "arith.subi"(%arg19, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %82 = "arith.remsi"(%81, %6) : (i64, i64) -> i64
        %83 = "arith.cmpi"(%82, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
        "scf.if"(%83) ({
          %106 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %107 = "arith.index_cast"(%106) : (i64) -> index
          %108 = "affine.apply"(%107) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %109 = "memref.reinterpret_cast"(%30, %108) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %110 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %111 = "arith.index_cast"(%110) : (i64) -> index
          %112 = "affine.apply"(%111) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %113 = "memref.reinterpret_cast"(%29, %112) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %114 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
          %115 = "affine.for"(%114) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg23: index, %arg24: f32):
            %116 = "affine.vector_load"(%109, %arg15, %arg23) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %117 = "llvm.bitcast"(%116) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %118 = "affine.vector_load"(%113, %arg23, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %119 = "llvm.bitcast"(%118) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %120 = "llvm.fmul"(%117, %119) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %121 = "llvm.fadd"(%arg24, %120) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%121) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "affine.store"(%115, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
          %84 = "arith.addi"(%arg19, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %85 = "arith.index_cast"(%15) : (index) -> i64
          %86 = "arith.cmpi"(%85, %84) <{predicate = 5 : i64}> : (i64, i64) -> i1
          %87 = "arith.remsi"(%arg19, %6) : (i64, i64) -> i64
          %88 = "arith.cmpi"(%87, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
          %89 = "arith.andi"(%86, %88) : (i1, i1) -> i1
          "scf.if"(%89) ({
            %90 = "arith.divsi"(%arg19, %6) : (i64, i64) -> i64
            %91 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %92 = "arith.index_cast"(%91) : (i64) -> index
            %93 = "affine.apply"(%92) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
            %94 = "memref.reinterpret_cast"(%30, %93) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %95 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %96 = "arith.index_cast"(%95) : (i64) -> index
            %97 = "affine.apply"(%96) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
            %98 = "memref.reinterpret_cast"(%29, %97) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %99 = "arith.index_cast"(%90) : (i64) -> index
            "affine.if"(%arg14, %arg15, %arg16) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg20: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg21: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg22: index):
                    %100 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %101 = "affine.apply"(%arg21, %arg20, %arg12, %99, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %102 = "nvgpu.device_async_copy"(%94, %100, %12, %101) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %103 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %104 = "affine.apply"(%arg21, %arg20, %99, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %105 = "nvgpu.device_async_copy"(%98, %103, %11, %104) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %34 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %35 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %36 = "arith.index_cast"(%15) : (index) -> i64
      %37 = "arith.cmpi"(%36, %8) <{predicate = 5 : i64}> : (i64, i64) -> i1
      %38 = "arith.index_cast"(%15) : (index) -> i64
      %39 = "arith.subi"(%38, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %40 = "arith.remsi"(%39, %6) : (i64, i64) -> i64
      %41 = "arith.cmpi"(%40, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
      %42 = "arith.andi"(%37, %41) : (i1, i1) -> i1
      "scf.if"(%42) ({
        %45 = "arith.index_cast"(%15) : (index) -> i64
        %46 = "arith.addi"(%45, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %47 = "arith.subi"(%46, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %48 = "arith.addi"(%47, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %49 = "arith.cmpi"(%46, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %50 = "arith.select"(%49, %48, %46) : (i1, i64, i64) -> i64
        %51 = "arith.divsi"(%50, %2) : (i64, i64) -> i64
        %52 = "arith.muli"(%51, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %53 = "arith.index_cast"(%15) : (index) -> i64
        %54 = "arith.subi"(%53, %52) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %55 = "arith.addi"(%54, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %56 = "arith.index_cast"(%55) : (i64) -> index
        %57 = "affine.apply"(%56) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %58 = "memref.reinterpret_cast"(%35, %57) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %59 = "arith.index_cast"(%15) : (index) -> i64
        %60 = "arith.addi"(%59, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %61 = "arith.subi"(%60, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %62 = "arith.addi"(%61, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %63 = "arith.cmpi"(%60, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %64 = "arith.select"(%63, %62, %60) : (i1, i64, i64) -> i64
        %65 = "arith.divsi"(%64, %2) : (i64, i64) -> i64
        %66 = "arith.muli"(%65, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %67 = "arith.index_cast"(%15) : (index) -> i64
        %68 = "arith.subi"(%67, %66) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %69 = "arith.addi"(%68, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %70 = "arith.index_cast"(%69) : (i64) -> index
        %71 = "affine.apply"(%70) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %72 = "memref.reinterpret_cast"(%34, %71) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %73 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %74 = "affine.for"(%73) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg17: index, %arg18: f32):
          %75 = "affine.vector_load"(%58, %arg15, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %76 = "llvm.bitcast"(%75) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %77 = "affine.vector_load"(%72, %arg17, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %78 = "llvm.bitcast"(%77) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %79 = "llvm.fmul"(%76, %78) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %80 = "llvm.fadd"(%arg18, %79) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%80) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%74, %22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      %43 = "affine.load"(%22, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %44 = "llvm.bitcast"(%43) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%44, %13, %arg13, %arg14, %arg15, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After rar:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 5 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 19 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %10 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %11 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %14 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %16 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %18 = "arith.index_cast"(%16) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  %20 = "arith.index_cast"(%17) : (index) -> i64
  %21 = "arith.index_cast"(%20) : (i64) -> index
  "scf.parallel"(%1, %1, %19, %21, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<f32>
      "memref.store"(%10, %22) <{nontemporal = false}> : (f32, memref<f32>) -> ()
      "nvvm.barrier0"() : () -> ()
      %23 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %24 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %25 = "arith.index_cast"(%15) : (index) -> i64
      %26 = "arith.subi"(%25, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %27 = "arith.minsi"(%26, %7) : (i64, i64) -> i64
      %28 = "arith.addi"(%27, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%9, %28, %6) ({
      ^bb0(%arg25: i64):
        %122 = "arith.divsi"(%arg25, %6) : (i64, i64) -> i64
        %123 = "arith.index_cast"(%arg25) : (i64) -> index
        %124 = "affine.apply"(%123) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %125 = "memref.reinterpret_cast"(%24, %124) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %126 = "arith.index_cast"(%arg25) : (i64) -> index
        %127 = "affine.apply"(%126) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %128 = "memref.reinterpret_cast"(%23, %127) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %129 = "arith.index_cast"(%122) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg26: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg27: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg28: index):
                %130 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %131 = "affine.apply"(%arg27, %arg26, %arg12, %129, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %132 = "nvgpu.device_async_copy"(%125, %130, %12, %131) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %133 = "affine.apply"(%arg27, %arg26) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %134 = "affine.apply"(%arg27, %arg26, %129, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %135 = "nvgpu.device_async_copy"(%128, %133, %11, %134) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %29 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %30 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %31 = "arith.index_cast"(%15) : (index) -> i64
      %32 = "arith.addi"(%31, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %33 = "arith.addi"(%32, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%5, %33, %8) ({
      ^bb0(%arg19: i64):
        %81 = "arith.subi"(%arg19, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %82 = "arith.remsi"(%81, %6) : (i64, i64) -> i64
        %83 = "arith.cmpi"(%82, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
        "scf.if"(%83) ({
          %106 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %107 = "arith.index_cast"(%106) : (i64) -> index
          %108 = "affine.apply"(%107) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %109 = "memref.reinterpret_cast"(%30, %108) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %110 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
          %111 = "arith.index_cast"(%110) : (i64) -> index
          %112 = "affine.apply"(%111) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %113 = "memref.reinterpret_cast"(%29, %112) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
          %114 = "memref.load"(%22) <{nontemporal = false}> : (memref<f32>) -> f32
          %115 = "affine.for"(%114) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg23: index, %arg24: f32):
            %116 = "affine.vector_load"(%109, %arg15, %arg23) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %117 = "llvm.bitcast"(%116) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
            %118 = "affine.vector_load"(%113, %arg23, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
            %119 = "llvm.bitcast"(%118) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
            %120 = "llvm.fmul"(%117, %119) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
            %121 = "llvm.fadd"(%arg24, %120) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
            "affine.yield"(%121) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
          }) : (f32) -> f32
          "memref.store"(%115, %22) <{nontemporal = false}> : (f32, memref<f32>) -> ()
          "scf.yield"() : () -> ()
        }, {
          %84 = "arith.addi"(%arg19, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %85 = "arith.index_cast"(%15) : (index) -> i64
          %86 = "arith.cmpi"(%85, %84) <{predicate = 5 : i64}> : (i64, i64) -> i1
          %87 = "arith.remsi"(%arg19, %6) : (i64, i64) -> i64
          %88 = "arith.cmpi"(%87, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
          %89 = "arith.andi"(%86, %88) : (i1, i1) -> i1
          "scf.if"(%89) ({
            %90 = "arith.divsi"(%arg19, %6) : (i64, i64) -> i64
            %91 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %92 = "arith.index_cast"(%91) : (i64) -> index
            %93 = "affine.apply"(%92) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
            %94 = "memref.reinterpret_cast"(%30, %93) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %95 = "arith.remui"(%arg19, %2) : (i64, i64) -> i64
            %96 = "arith.index_cast"(%95) : (i64) -> index
            %97 = "affine.apply"(%96) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
            %98 = "memref.reinterpret_cast"(%29, %97) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
            %99 = "arith.index_cast"(%90) : (i64) -> index
            "affine.if"(%arg14, %arg15, %arg16) ({
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg20: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
                ^bb0(%arg21: index):
                  "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                  ^bb0(%arg22: index):
                    %100 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %101 = "affine.apply"(%arg21, %arg20, %arg12, %99, %15) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %102 = "nvgpu.device_async_copy"(%94, %100, %12, %101) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    %103 = "affine.apply"(%arg21, %arg20) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                    %104 = "affine.apply"(%arg21, %arg20, %99, %arg13, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                    %105 = "nvgpu.device_async_copy"(%98, %103, %11, %104) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                    "affine.yield"() : () -> ()
                  }) : () -> ()
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "nvvm.cp.async.commit.group"() : () -> ()
              "affine.yield"() : () -> ()
            }, {
            }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
            "scf.yield"() : () -> ()
          }, {
          }) : (i1) -> ()
          "scf.yield"() : () -> ()
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %34 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %35 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %36 = "arith.index_cast"(%15) : (index) -> i64
      %37 = "arith.cmpi"(%36, %8) <{predicate = 5 : i64}> : (i64, i64) -> i1
      %38 = "arith.index_cast"(%15) : (index) -> i64
      %39 = "arith.subi"(%38, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %40 = "arith.remsi"(%39, %6) : (i64, i64) -> i64
      %41 = "arith.cmpi"(%40, %9) <{predicate = 0 : i64}> : (i64, i64) -> i1
      %42 = "arith.andi"(%37, %41) : (i1, i1) -> i1
      "scf.if"(%42) ({
        %45 = "arith.index_cast"(%15) : (index) -> i64
        %46 = "arith.addi"(%45, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %47 = "arith.subi"(%46, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %48 = "arith.addi"(%47, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %49 = "arith.cmpi"(%46, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %50 = "arith.select"(%49, %48, %46) : (i1, i64, i64) -> i64
        %51 = "arith.divsi"(%50, %2) : (i64, i64) -> i64
        %52 = "arith.muli"(%51, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %53 = "arith.index_cast"(%15) : (index) -> i64
        %54 = "arith.subi"(%53, %52) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %55 = "arith.addi"(%54, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %56 = "arith.index_cast"(%55) : (i64) -> index
        %57 = "affine.apply"(%56) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %58 = "memref.reinterpret_cast"(%35, %57) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %59 = "arith.index_cast"(%15) : (index) -> i64
        %60 = "arith.addi"(%59, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %61 = "arith.subi"(%60, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %62 = "arith.addi"(%61, %8) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %63 = "arith.cmpi"(%60, %9) <{predicate = 2 : i64}> : (i64, i64) -> i1
        %64 = "arith.select"(%63, %62, %60) : (i1, i64, i64) -> i64
        %65 = "arith.divsi"(%64, %2) : (i64, i64) -> i64
        %66 = "arith.muli"(%65, %2) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %67 = "arith.index_cast"(%15) : (index) -> i64
        %68 = "arith.subi"(%67, %66) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %69 = "arith.addi"(%68, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %70 = "arith.index_cast"(%69) : (i64) -> index
        %71 = "affine.apply"(%70) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %72 = "memref.reinterpret_cast"(%34, %71) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %73 = "memref.load"(%22) <{nontemporal = false}> : (memref<f32>) -> f32
        %74 = "affine.for"(%73) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg17: index, %arg18: f32):
          %75 = "affine.vector_load"(%58, %arg15, %arg17) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %76 = "llvm.bitcast"(%75) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %77 = "affine.vector_load"(%72, %arg17, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %78 = "llvm.bitcast"(%77) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %79 = "llvm.fmul"(%76, %78) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %80 = "llvm.fadd"(%arg18, %79) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%80) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "memref.store"(%74, %22) <{nontemporal = false}> : (f32, memref<f32>) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      %43 = "memref.load"(%22) <{nontemporal = false}> : (memref<f32>) -> f32
      %44 = "llvm.bitcast"(%43) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%44, %13, %arg13, %arg14, %arg15, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After lower affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c22_i64 = arith.constant 22 : i64
  %c5_i64 = arith.constant 5 : i64
  %c19_i64 = arith.constant 19 : i64
  %c21_i64 = arith.constant 21 : i64
  %c16_i64 = arith.constant 16 : i64
  %c20_i64 = arith.constant 20 : i64
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %0 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 {polymer.stmt.name = "S4_arith_index_cast"} : i32 to index
  %5 = arith.index_cast %arg10 {polymer.stmt.name = "S5_arith_index_cast"} : i32 to index
  %6 = arith.index_cast %arg1 {polymer.stmt.name = "S6_arith_index_cast"} : i64 to index
  %7 = arith.index_cast %arg0 {polymer.stmt.name = "S7_arith_index_cast"} : i64 to index
  %8 = arith.index_cast %6 : index to i64
  %9 = arith.index_cast %8 : i64 to index
  %10 = arith.index_cast %7 : index to i64
  %11 = arith.index_cast %10 : i64 to index
  scf.parallel (%arg12, %arg13) = (%c0, %c0) to (%9, %11) step (%c1, %c1) {
    %c0_0 = arith.constant 0 : index
    %c16 = arith.constant 16 : index
    %c0_1 = arith.constant 0 : index
    %c16_2 = arith.constant 16 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %c1_5 = arith.constant 1 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    scf.parallel (%arg14, %arg15, %arg16) = (%c0_0, %c0_1, %c0_3) to (%c16, %c16_2, %c1_4) step (%c1_5, %c1_6, %c1_7) {
      %alloca = memref.alloca() : memref<f32>
      memref.store %0, %alloca[] : memref<f32>
      nvvm.barrier0
      %12 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %13 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %14 = arith.index_cast %5 : index to i64
      %15 = arith.subi %14, %c1_i64 : i64
      %16 = arith.minsi %15, %c20_i64 : i64
      %17 = arith.addi %16, %c1_i64 : i64
      scf.for %arg17 = %c0_i64 to %17 step %c16_i64  : i64 {
        %44 = arith.divsi %arg17, %c16_i64 : i64
        %45 = arith.index_cast %arg17 : i64 to index
        %c1024 = arith.constant 1024 : index
        %46 = arith.muli %45, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %13 to offset: [%46], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %47 = arith.index_cast %arg17 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %48 = arith.muli %47, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %12 to offset: [%48], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %49 = arith.index_cast %44 : i64 to index
        %c0_13 = arith.constant 0 : index
        %50 = arith.cmpi eq, %arg14, %c0_13 : index
        %51 = arith.cmpi eq, %arg15, %c0_13 : index
        %52 = arith.andi %50, %51 : i1
        %53 = arith.cmpi eq, %arg16, %c0_13 : index
        %54 = arith.andi %52, %53 : i1
        scf.if %54 {
          %c0_14 = arith.constant 0 : index
          %c16_15 = arith.constant 16 : index
          %c1_16 = arith.constant 1 : index
          scf.for %arg18 = %c0_14 to %c16_15 step %c1_16 {
            %c0_17 = arith.constant 0 : index
            %c16_18 = arith.constant 16 : index
            %c1_19 = arith.constant 1 : index
            scf.for %arg19 = %c0_17 to %c16_18 step %c1_19 {
              %c0_20 = arith.constant 0 : index
              %c1_21 = arith.constant 1 : index
              %c1_22 = arith.constant 1 : index
              scf.for %arg20 = %c0_20 to %c1_21 step %c1_22 {
                %c64_23 = arith.constant 64 : index
                %55 = arith.muli %arg19, %c64_23 : index
                %c4_24 = arith.constant 4 : index
                %56 = arith.muli %arg18, %c4_24 : index
                %57 = arith.addi %55, %56 : index
                %58 = arith.muli %arg19, %5 : index
                %c4_25 = arith.constant 4 : index
                %59 = arith.muli %58, %c4_25 : index
                %c4_26 = arith.constant 4 : index
                %60 = arith.muli %arg18, %c4_26 : index
                %61 = arith.addi %59, %60 : index
                %c64_27 = arith.constant 64 : index
                %62 = arith.muli %49, %c64_27 : index
                %63 = arith.addi %61, %62 : index
                %c16_28 = arith.constant 16 : index
                %64 = arith.muli %5, %c16_28 : index
                %65 = arith.muli %arg12, %64 : index
                %c4_29 = arith.constant 4 : index
                %66 = arith.muli %65, %c4_29 : index
                %67 = arith.addi %63, %66 : index
                %68 = nvgpu.device_async_copy %2[%67], %reinterpret_cast[%57], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                %c64_30 = arith.constant 64 : index
                %69 = arith.muli %arg19, %c64_30 : index
                %c4_31 = arith.constant 4 : index
                %70 = arith.muli %arg18, %c4_31 : index
                %71 = arith.addi %69, %70 : index
                %72 = arith.muli %arg19, %4 : index
                %c4_32 = arith.constant 4 : index
                %73 = arith.muli %72, %c4_32 : index
                %c4_33 = arith.constant 4 : index
                %74 = arith.muli %arg18, %c4_33 : index
                %75 = arith.addi %73, %74 : index
                %c64_34 = arith.constant 64 : index
                %76 = arith.muli %arg13, %c64_34 : index
                %77 = arith.addi %75, %76 : index
                %c16_35 = arith.constant 16 : index
                %78 = arith.muli %4, %c16_35 : index
                %79 = arith.muli %49, %78 : index
                %c4_36 = arith.constant 4 : index
                %80 = arith.muli %79, %c4_36 : index
                %81 = arith.addi %77, %80 : index
                %82 = nvgpu.device_async_copy %1[%81], %reinterpret_cast_12[%71], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %18 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %19 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %20 = arith.index_cast %5 : index to i64
      %21 = arith.addi %20, %c19_i64 : i64
      %22 = arith.addi %21, %c1_i64 : i64
      scf.for %arg17 = %c21_i64 to %22 step %c1_i64  : i64 {
        %44 = arith.subi %arg17, %c5_i64 : i64
        %45 = arith.remsi %44, %c16_i64 : i64
        %46 = arith.cmpi eq, %45, %c0_i64 : i64
        scf.if %46 {
          %47 = arith.remui %arg17, %c22_i64 : i64
          %48 = arith.index_cast %47 : i64 to index
          %c1024 = arith.constant 1024 : index
          %49 = arith.muli %48, %c1024 : index
          %reinterpret_cast = memref.reinterpret_cast %19 to offset: [%49], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          %50 = arith.remui %arg17, %c22_i64 : i64
          %51 = arith.index_cast %50 : i64 to index
          %c1024_11 = arith.constant 1024 : index
          %52 = arith.muli %51, %c1024_11 : index
          %reinterpret_cast_12 = memref.reinterpret_cast %18 to offset: [%52], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          nvvm.cp.async.wait.group 20
          %53 = memref.load %alloca[] : memref<f32>
          %c0_13 = arith.constant 0 : index
          %c16_14 = arith.constant 16 : index
          %c1_15 = arith.constant 1 : index
          %54 = scf.for %arg18 = %c0_13 to %c16_14 step %c1_15 iter_args(%arg19 = %53) -> (f32) {
            %c64_16 = arith.constant 64 : index
            %55 = arith.muli %arg15, %c64_16 : index
            %c4_17 = arith.constant 4 : index
            %56 = arith.muli %arg18, %c4_17 : index
            %57 = arith.addi %55, %56 : index
            %58 = vector.load %reinterpret_cast[%57] {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
            %59 = llvm.bitcast %58 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
            %c64_18 = arith.constant 64 : index
            %60 = arith.muli %arg18, %c64_18 : index
            %c4_19 = arith.constant 4 : index
            %61 = arith.muli %arg14, %c4_19 : index
            %62 = arith.addi %60, %61 : index
            %63 = vector.load %reinterpret_cast_12[%62] {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
            %64 = llvm.bitcast %63 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
            %65 = llvm.fmul %59, %64  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
            %66 = llvm.fadd %arg19, %65  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
            scf.yield %66 : f32
          }
          memref.store %54, %alloca[] : memref<f32>
        } else {
          %47 = arith.addi %arg17, %c1_i64 : i64
          %48 = arith.index_cast %5 : index to i64
          %49 = arith.cmpi sge, %48, %47 : i64
          %50 = arith.remsi %arg17, %c16_i64 : i64
          %51 = arith.cmpi eq, %50, %c0_i64 : i64
          %52 = arith.andi %49, %51 : i1
          scf.if %52 {
            %53 = arith.divsi %arg17, %c16_i64 : i64
            %54 = arith.remui %arg17, %c22_i64 : i64
            %55 = arith.index_cast %54 : i64 to index
            %c1024 = arith.constant 1024 : index
            %56 = arith.muli %55, %c1024 : index
            %reinterpret_cast = memref.reinterpret_cast %19 to offset: [%56], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
            %57 = arith.remui %arg17, %c22_i64 : i64
            %58 = arith.index_cast %57 : i64 to index
            %c1024_11 = arith.constant 1024 : index
            %59 = arith.muli %58, %c1024_11 : index
            %reinterpret_cast_12 = memref.reinterpret_cast %18 to offset: [%59], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
            %60 = arith.index_cast %53 : i64 to index
            %c0_13 = arith.constant 0 : index
            %61 = arith.cmpi eq, %arg14, %c0_13 : index
            %62 = arith.cmpi eq, %arg15, %c0_13 : index
            %63 = arith.andi %61, %62 : i1
            %64 = arith.cmpi eq, %arg16, %c0_13 : index
            %65 = arith.andi %63, %64 : i1
            scf.if %65 {
              %c0_14 = arith.constant 0 : index
              %c16_15 = arith.constant 16 : index
              %c1_16 = arith.constant 1 : index
              scf.for %arg18 = %c0_14 to %c16_15 step %c1_16 {
                %c0_17 = arith.constant 0 : index
                %c16_18 = arith.constant 16 : index
                %c1_19 = arith.constant 1 : index
                scf.for %arg19 = %c0_17 to %c16_18 step %c1_19 {
                  %c0_20 = arith.constant 0 : index
                  %c1_21 = arith.constant 1 : index
                  %c1_22 = arith.constant 1 : index
                  scf.for %arg20 = %c0_20 to %c1_21 step %c1_22 {
                    %c64_23 = arith.constant 64 : index
                    %66 = arith.muli %arg19, %c64_23 : index
                    %c4_24 = arith.constant 4 : index
                    %67 = arith.muli %arg18, %c4_24 : index
                    %68 = arith.addi %66, %67 : index
                    %69 = arith.muli %arg19, %5 : index
                    %c4_25 = arith.constant 4 : index
                    %70 = arith.muli %69, %c4_25 : index
                    %c4_26 = arith.constant 4 : index
                    %71 = arith.muli %arg18, %c4_26 : index
                    %72 = arith.addi %70, %71 : index
                    %c64_27 = arith.constant 64 : index
                    %73 = arith.muli %60, %c64_27 : index
                    %74 = arith.addi %72, %73 : index
                    %c16_28 = arith.constant 16 : index
                    %75 = arith.muli %5, %c16_28 : index
                    %76 = arith.muli %arg12, %75 : index
                    %c4_29 = arith.constant 4 : index
                    %77 = arith.muli %76, %c4_29 : index
                    %78 = arith.addi %74, %77 : index
                    %79 = nvgpu.device_async_copy %2[%78], %reinterpret_cast[%68], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                    %c64_30 = arith.constant 64 : index
                    %80 = arith.muli %arg19, %c64_30 : index
                    %c4_31 = arith.constant 4 : index
                    %81 = arith.muli %arg18, %c4_31 : index
                    %82 = arith.addi %80, %81 : index
                    %83 = arith.muli %arg19, %4 : index
                    %c4_32 = arith.constant 4 : index
                    %84 = arith.muli %83, %c4_32 : index
                    %c4_33 = arith.constant 4 : index
                    %85 = arith.muli %arg18, %c4_33 : index
                    %86 = arith.addi %84, %85 : index
                    %c64_34 = arith.constant 64 : index
                    %87 = arith.muli %arg13, %c64_34 : index
                    %88 = arith.addi %86, %87 : index
                    %c16_35 = arith.constant 16 : index
                    %89 = arith.muli %4, %c16_35 : index
                    %90 = arith.muli %60, %89 : index
                    %c4_36 = arith.constant 4 : index
                    %91 = arith.muli %90, %c4_36 : index
                    %92 = arith.addi %88, %91 : index
                    %93 = nvgpu.device_async_copy %1[%92], %reinterpret_cast_12[%82], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %23 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %24 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %25 = arith.index_cast %5 : index to i64
      %26 = arith.cmpi sge, %25, %c1_i64 : i64
      %27 = arith.index_cast %5 : index to i64
      %28 = arith.subi %27, %c1_i64 : i64
      %29 = arith.remsi %28, %c16_i64 : i64
      %30 = arith.cmpi eq, %29, %c0_i64 : i64
      %31 = arith.andi %26, %30 : i1
      scf.if %31 {
        %44 = arith.index_cast %5 : index to i64
        %45 = arith.addi %44, %c20_i64 : i64
        %46 = arith.subi %45, %c22_i64 : i64
        %47 = arith.addi %46, %c1_i64 : i64
        %48 = arith.cmpi slt, %45, %c0_i64 : i64
        %49 = arith.select %48, %47, %45 : i64
        %50 = arith.divsi %49, %c22_i64 : i64
        %51 = arith.muli %50, %c22_i64 : i64
        %52 = arith.index_cast %5 : index to i64
        %53 = arith.subi %52, %51 : i64
        %54 = arith.addi %53, %c20_i64 : i64
        %55 = arith.index_cast %54 : i64 to index
        %c1024 = arith.constant 1024 : index
        %56 = arith.muli %55, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %24 to offset: [%56], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %57 = arith.index_cast %5 : index to i64
        %58 = arith.addi %57, %c20_i64 : i64
        %59 = arith.subi %58, %c22_i64 : i64
        %60 = arith.addi %59, %c1_i64 : i64
        %61 = arith.cmpi slt, %58, %c0_i64 : i64
        %62 = arith.select %61, %60, %58 : i64
        %63 = arith.divsi %62, %c22_i64 : i64
        %64 = arith.muli %63, %c22_i64 : i64
        %65 = arith.index_cast %5 : index to i64
        %66 = arith.subi %65, %64 : i64
        %67 = arith.addi %66, %c20_i64 : i64
        %68 = arith.index_cast %67 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %69 = arith.muli %68, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %23 to offset: [%69], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        nvvm.cp.async.wait.group 20
        %70 = memref.load %alloca[] : memref<f32>
        %c0_13 = arith.constant 0 : index
        %c16_14 = arith.constant 16 : index
        %c1_15 = arith.constant 1 : index
        %71 = scf.for %arg17 = %c0_13 to %c16_14 step %c1_15 iter_args(%arg18 = %70) -> (f32) {
          %c64_16 = arith.constant 64 : index
          %72 = arith.muli %arg15, %c64_16 : index
          %c4_17 = arith.constant 4 : index
          %73 = arith.muli %arg17, %c4_17 : index
          %74 = arith.addi %72, %73 : index
          %75 = vector.load %reinterpret_cast[%74] {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %76 = llvm.bitcast %75 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %c64_18 = arith.constant 64 : index
          %77 = arith.muli %arg17, %c64_18 : index
          %c4_19 = arith.constant 4 : index
          %78 = arith.muli %arg14, %c4_19 : index
          %79 = arith.addi %77, %78 : index
          %80 = vector.load %reinterpret_cast_12[%79] {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %81 = llvm.bitcast %80 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %82 = llvm.fmul %76, %81  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %83 = llvm.fadd %arg18, %82  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %83 : f32
        }
        memref.store %71, %alloca[] : memref<f32>
      }
      nvvm.barrier0
      %32 = memref.load %alloca[] : memref<f32>
      %33 = llvm.bitcast %32 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %c64 = arith.constant 64 : index
      %34 = arith.muli %arg13, %c64 : index
      %c4 = arith.constant 4 : index
      %35 = arith.muli %arg14, %c4 : index
      %36 = arith.addi %34, %35 : index
      %37 = arith.muli %arg15, %4 : index
      %c4_8 = arith.constant 4 : index
      %38 = arith.muli %37, %c4_8 : index
      %39 = arith.addi %36, %38 : index
      %c16_9 = arith.constant 16 : index
      %40 = arith.muli %4, %c16_9 : index
      %41 = arith.muli %arg12, %40 : index
      %c4_10 = arith.constant 4 : index
      %42 = arith.muli %41, %c4_10 : index
      %43 = arith.addi %39, %42 : index
      vector.store %33, %3[%43] {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : memref<?xi8>, vector<4xi8>
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: After lower accesses:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  %1 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = builtin.unrealized_conversion_cast %2 : i64 to index
  %4 = llvm.mlir.constant(0 : i64) : i64
  %5 = builtin.unrealized_conversion_cast %4 : i64 to index
  %6 = llvm.mlir.constant(22 : i64) : i64
  %7 = llvm.mlir.constant(5 : i64) : i64
  %8 = llvm.mlir.constant(19 : i64) : i64
  %9 = llvm.mlir.constant(21 : i64) : i64
  %10 = llvm.mlir.constant(16 : i64) : i64
  %11 = llvm.mlir.constant(20 : i64) : i64
  %12 = llvm.mlir.constant(1 : i64) : i64
  %13 = llvm.mlir.constant(0 : i64) : i64
  %14 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %15 = llvm.sext %arg11 : i32 to i64
  %16 = llvm.sext %arg10 : i32 to i64
  scf.parallel (%arg12, %arg13) = (%5, %5) to (%1, %0) step (%3, %3) {
    %17 = builtin.unrealized_conversion_cast %arg13 : index to i64
    %18 = builtin.unrealized_conversion_cast %arg12 : index to i64
    %19 = llvm.mlir.constant(0 : i64) : i64
    %20 = builtin.unrealized_conversion_cast %19 : i64 to index
    %21 = llvm.mlir.constant(16 : i64) : i64
    %22 = builtin.unrealized_conversion_cast %21 : i64 to index
    %23 = llvm.mlir.constant(0 : i64) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(16 : i64) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(0 : i64) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(1 : i64) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(1 : i64) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(1 : i64) : i64
    %34 = builtin.unrealized_conversion_cast %33 : i64 to index
    %35 = llvm.mlir.constant(1 : i64) : i64
    %36 = builtin.unrealized_conversion_cast %35 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%20, %24, %28) to (%22, %26, %30) step (%32, %34, %36) {
      %37 = builtin.unrealized_conversion_cast %arg16 : index to i64
      %38 = builtin.unrealized_conversion_cast %arg15 : index to i64
      %39 = builtin.unrealized_conversion_cast %arg14 : index to i64
      %40 = llvm.mlir.constant(1 : i64) : i64
      %41 = llvm.alloca %40 x f32 : (i64) -> !llvm.ptr
      llvm.store %14, %41 : f32, !llvm.ptr
      nvvm.barrier0
      %42 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %43 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %44 = llvm.sub %16, %12 : i64
      %45 = llvm.intr.smin(%44, %11)  : (i64, i64) -> i64
      %46 = llvm.add %45, %12 : i64
      scf.for %arg17 = %13 to %46 step %10  : i64 {
        %77 = llvm.sdiv %arg17, %10  : i64
        %78 = llvm.mlir.constant(1024 : i64) : i64
        %79 = llvm.mul %arg17, %78 : i64
        %80 = llvm.getelementptr %43[%79] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %81 = llvm.mlir.constant(1024 : i64) : i64
        %82 = llvm.mul %arg17, %81 : i64
        %83 = llvm.getelementptr %42[%82] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %84 = llvm.mlir.constant(0 : i64) : i64
        %85 = llvm.icmp "eq" %39, %84 : i64
        %86 = llvm.icmp "eq" %38, %84 : i64
        %87 = llvm.and %85, %86  : i1
        %88 = llvm.icmp "eq" %37, %84 : i64
        %89 = llvm.and %87, %88  : i1
        scf.if %89 {
          %90 = llvm.mlir.constant(0 : i64) : i64
          %91 = builtin.unrealized_conversion_cast %90 : i64 to index
          %92 = llvm.mlir.constant(16 : i64) : i64
          %93 = builtin.unrealized_conversion_cast %92 : i64 to index
          %94 = llvm.mlir.constant(1 : i64) : i64
          %95 = builtin.unrealized_conversion_cast %94 : i64 to index
          scf.for %arg18 = %91 to %93 step %95 {
            %96 = builtin.unrealized_conversion_cast %arg18 : index to i64
            %97 = llvm.mlir.constant(0 : i64) : i64
            %98 = builtin.unrealized_conversion_cast %97 : i64 to index
            %99 = llvm.mlir.constant(16 : i64) : i64
            %100 = builtin.unrealized_conversion_cast %99 : i64 to index
            %101 = llvm.mlir.constant(1 : i64) : i64
            %102 = builtin.unrealized_conversion_cast %101 : i64 to index
            scf.for %arg19 = %98 to %100 step %102 {
              %103 = builtin.unrealized_conversion_cast %arg19 : index to i64
              %104 = llvm.mlir.constant(0 : i64) : i64
              %105 = builtin.unrealized_conversion_cast %104 : i64 to index
              %106 = llvm.mlir.constant(1 : i64) : i64
              %107 = builtin.unrealized_conversion_cast %106 : i64 to index
              %108 = llvm.mlir.constant(1 : i64) : i64
              %109 = builtin.unrealized_conversion_cast %108 : i64 to index
              scf.for %arg20 = %105 to %107 step %109 {
                %110 = llvm.mlir.constant(64 : i64) : i64
                %111 = llvm.mul %103, %110 : i64
                %112 = llvm.mlir.constant(4 : i64) : i64
                %113 = llvm.mul %96, %112 : i64
                %114 = llvm.add %111, %113 : i64
                %115 = llvm.mul %103, %16 : i64
                %116 = llvm.mlir.constant(4 : i64) : i64
                %117 = llvm.mul %115, %116 : i64
                %118 = llvm.mlir.constant(4 : i64) : i64
                %119 = llvm.mul %96, %118 : i64
                %120 = llvm.add %117, %119 : i64
                %121 = llvm.mlir.constant(64 : i64) : i64
                %122 = llvm.mul %77, %121 : i64
                %123 = llvm.add %120, %122 : i64
                %124 = llvm.mlir.constant(16 : i64) : i64
                %125 = llvm.mul %16, %124 : i64
                %126 = llvm.mul %18, %125 : i64
                %127 = llvm.mlir.constant(4 : i64) : i64
                %128 = llvm.mul %126, %127 : i64
                %129 = llvm.add %123, %128 : i64
                %130 = llvm.getelementptr %80[%114] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %131 = llvm.getelementptr %arg8[%129] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %132 = llvm.addrspacecast %131 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %130, %132, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %133 = llvm.mlir.constant(0 : i32) : i32
                %134 = llvm.mlir.constant(64 : i64) : i64
                %135 = llvm.mul %103, %134 : i64
                %136 = llvm.mlir.constant(4 : i64) : i64
                %137 = llvm.mul %96, %136 : i64
                %138 = llvm.add %135, %137 : i64
                %139 = llvm.mul %103, %15 : i64
                %140 = llvm.mlir.constant(4 : i64) : i64
                %141 = llvm.mul %139, %140 : i64
                %142 = llvm.mlir.constant(4 : i64) : i64
                %143 = llvm.mul %96, %142 : i64
                %144 = llvm.add %141, %143 : i64
                %145 = llvm.mlir.constant(64 : i64) : i64
                %146 = llvm.mul %17, %145 : i64
                %147 = llvm.add %144, %146 : i64
                %148 = llvm.mlir.constant(16 : i64) : i64
                %149 = llvm.mul %15, %148 : i64
                %150 = llvm.mul %77, %149 : i64
                %151 = llvm.mlir.constant(4 : i64) : i64
                %152 = llvm.mul %150, %151 : i64
                %153 = llvm.add %147, %152 : i64
                %154 = llvm.getelementptr %83[%138] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %155 = llvm.getelementptr %arg9[%153] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %156 = llvm.addrspacecast %155 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %154, %156, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %157 = llvm.mlir.constant(0 : i32) : i32
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %47 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %48 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %49 = llvm.add %16, %8 : i64
      %50 = llvm.add %49, %12 : i64
      scf.for %arg17 = %9 to %50 step %12  : i64 {
        %77 = llvm.sub %arg17, %7 : i64
        %78 = llvm.srem %77, %10  : i64
        %79 = llvm.icmp "eq" %78, %13 : i64
        scf.if %79 {
          %80 = llvm.urem %arg17, %6  : i64
          %81 = llvm.mlir.constant(1024 : i64) : i64
          %82 = llvm.mul %80, %81 : i64
          %83 = llvm.getelementptr %48[%82] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %84 = llvm.urem %arg17, %6  : i64
          %85 = llvm.mlir.constant(1024 : i64) : i64
          %86 = llvm.mul %84, %85 : i64
          %87 = llvm.getelementptr %47[%86] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          nvvm.cp.async.wait.group 20
          %88 = llvm.load %41 : !llvm.ptr -> f32
          %89 = llvm.mlir.constant(0 : i64) : i64
          %90 = builtin.unrealized_conversion_cast %89 : i64 to index
          %91 = llvm.mlir.constant(16 : i64) : i64
          %92 = builtin.unrealized_conversion_cast %91 : i64 to index
          %93 = llvm.mlir.constant(1 : i64) : i64
          %94 = builtin.unrealized_conversion_cast %93 : i64 to index
          %95 = scf.for %arg18 = %90 to %92 step %94 iter_args(%arg19 = %88) -> (f32) {
            %96 = builtin.unrealized_conversion_cast %arg18 : index to i64
            %97 = llvm.mlir.constant(64 : i64) : i64
            %98 = llvm.mul %38, %97 : i64
            %99 = llvm.mlir.constant(4 : i64) : i64
            %100 = llvm.mul %96, %99 : i64
            %101 = llvm.add %98, %100 : i64
            %102 = llvm.getelementptr %83[%101] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %103 = llvm.load %102 : !llvm.ptr<3> -> f32
            %104 = llvm.bitcast %103 : f32 to vector<4xi8>
            %105 = llvm.bitcast %104 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
            %106 = llvm.mlir.constant(64 : i64) : i64
            %107 = llvm.mul %96, %106 : i64
            %108 = llvm.mlir.constant(4 : i64) : i64
            %109 = llvm.mul %39, %108 : i64
            %110 = llvm.add %107, %109 : i64
            %111 = llvm.getelementptr %87[%110] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %112 = llvm.load %111 : !llvm.ptr<3> -> f32
            %113 = llvm.bitcast %112 : f32 to vector<4xi8>
            %114 = llvm.bitcast %113 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
            %115 = llvm.fmul %105, %114  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
            %116 = llvm.fadd %arg19, %115  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
            scf.yield %116 : f32
          }
          llvm.store %95, %41 : f32, !llvm.ptr
        } else {
          %80 = llvm.add %arg17, %12 : i64
          %81 = llvm.icmp "sge" %16, %80 : i64
          %82 = llvm.srem %arg17, %10  : i64
          %83 = llvm.icmp "eq" %82, %13 : i64
          %84 = llvm.and %81, %83  : i1
          scf.if %84 {
            %85 = llvm.sdiv %arg17, %10  : i64
            %86 = llvm.urem %arg17, %6  : i64
            %87 = llvm.mlir.constant(1024 : i64) : i64
            %88 = llvm.mul %86, %87 : i64
            %89 = llvm.getelementptr %48[%88] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %90 = llvm.urem %arg17, %6  : i64
            %91 = llvm.mlir.constant(1024 : i64) : i64
            %92 = llvm.mul %90, %91 : i64
            %93 = llvm.getelementptr %47[%92] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %94 = llvm.mlir.constant(0 : i64) : i64
            %95 = llvm.icmp "eq" %39, %94 : i64
            %96 = llvm.icmp "eq" %38, %94 : i64
            %97 = llvm.and %95, %96  : i1
            %98 = llvm.icmp "eq" %37, %94 : i64
            %99 = llvm.and %97, %98  : i1
            scf.if %99 {
              %100 = llvm.mlir.constant(0 : i64) : i64
              %101 = builtin.unrealized_conversion_cast %100 : i64 to index
              %102 = llvm.mlir.constant(16 : i64) : i64
              %103 = builtin.unrealized_conversion_cast %102 : i64 to index
              %104 = llvm.mlir.constant(1 : i64) : i64
              %105 = builtin.unrealized_conversion_cast %104 : i64 to index
              scf.for %arg18 = %101 to %103 step %105 {
                %106 = builtin.unrealized_conversion_cast %arg18 : index to i64
                %107 = llvm.mlir.constant(0 : i64) : i64
                %108 = builtin.unrealized_conversion_cast %107 : i64 to index
                %109 = llvm.mlir.constant(16 : i64) : i64
                %110 = builtin.unrealized_conversion_cast %109 : i64 to index
                %111 = llvm.mlir.constant(1 : i64) : i64
                %112 = builtin.unrealized_conversion_cast %111 : i64 to index
                scf.for %arg19 = %108 to %110 step %112 {
                  %113 = builtin.unrealized_conversion_cast %arg19 : index to i64
                  %114 = llvm.mlir.constant(0 : i64) : i64
                  %115 = builtin.unrealized_conversion_cast %114 : i64 to index
                  %116 = llvm.mlir.constant(1 : i64) : i64
                  %117 = builtin.unrealized_conversion_cast %116 : i64 to index
                  %118 = llvm.mlir.constant(1 : i64) : i64
                  %119 = builtin.unrealized_conversion_cast %118 : i64 to index
                  scf.for %arg20 = %115 to %117 step %119 {
                    %120 = llvm.mlir.constant(64 : i64) : i64
                    %121 = llvm.mul %113, %120 : i64
                    %122 = llvm.mlir.constant(4 : i64) : i64
                    %123 = llvm.mul %106, %122 : i64
                    %124 = llvm.add %121, %123 : i64
                    %125 = llvm.mul %113, %16 : i64
                    %126 = llvm.mlir.constant(4 : i64) : i64
                    %127 = llvm.mul %125, %126 : i64
                    %128 = llvm.mlir.constant(4 : i64) : i64
                    %129 = llvm.mul %106, %128 : i64
                    %130 = llvm.add %127, %129 : i64
                    %131 = llvm.mlir.constant(64 : i64) : i64
                    %132 = llvm.mul %85, %131 : i64
                    %133 = llvm.add %130, %132 : i64
                    %134 = llvm.mlir.constant(16 : i64) : i64
                    %135 = llvm.mul %16, %134 : i64
                    %136 = llvm.mul %18, %135 : i64
                    %137 = llvm.mlir.constant(4 : i64) : i64
                    %138 = llvm.mul %136, %137 : i64
                    %139 = llvm.add %133, %138 : i64
                    %140 = llvm.getelementptr %89[%124] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %141 = llvm.getelementptr %arg8[%139] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %142 = llvm.addrspacecast %141 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %140, %142, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %143 = llvm.mlir.constant(0 : i32) : i32
                    %144 = llvm.mlir.constant(64 : i64) : i64
                    %145 = llvm.mul %113, %144 : i64
                    %146 = llvm.mlir.constant(4 : i64) : i64
                    %147 = llvm.mul %106, %146 : i64
                    %148 = llvm.add %145, %147 : i64
                    %149 = llvm.mul %113, %15 : i64
                    %150 = llvm.mlir.constant(4 : i64) : i64
                    %151 = llvm.mul %149, %150 : i64
                    %152 = llvm.mlir.constant(4 : i64) : i64
                    %153 = llvm.mul %106, %152 : i64
                    %154 = llvm.add %151, %153 : i64
                    %155 = llvm.mlir.constant(64 : i64) : i64
                    %156 = llvm.mul %17, %155 : i64
                    %157 = llvm.add %154, %156 : i64
                    %158 = llvm.mlir.constant(16 : i64) : i64
                    %159 = llvm.mul %15, %158 : i64
                    %160 = llvm.mul %85, %159 : i64
                    %161 = llvm.mlir.constant(4 : i64) : i64
                    %162 = llvm.mul %160, %161 : i64
                    %163 = llvm.add %157, %162 : i64
                    %164 = llvm.getelementptr %93[%148] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %165 = llvm.getelementptr %arg9[%163] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %166 = llvm.addrspacecast %165 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %164, %166, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %167 = llvm.mlir.constant(0 : i32) : i32
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %51 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %52 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %53 = llvm.icmp "sge" %16, %12 : i64
      %54 = llvm.sub %16, %12 : i64
      %55 = llvm.srem %54, %10  : i64
      %56 = llvm.icmp "eq" %55, %13 : i64
      %57 = llvm.and %53, %56  : i1
      scf.if %57 {
        %77 = llvm.add %16, %11 : i64
        %78 = llvm.sub %77, %6 : i64
        %79 = llvm.add %78, %12 : i64
        %80 = llvm.icmp "slt" %77, %13 : i64
        %81 = llvm.select %80, %79, %77 : i1, i64
        %82 = llvm.sdiv %81, %6  : i64
        %83 = llvm.mul %82, %6 : i64
        %84 = llvm.sub %16, %83 : i64
        %85 = llvm.add %84, %11 : i64
        %86 = llvm.mlir.constant(1024 : i64) : i64
        %87 = llvm.mul %85, %86 : i64
        %88 = llvm.getelementptr %52[%87] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %89 = llvm.add %16, %11 : i64
        %90 = llvm.sub %89, %6 : i64
        %91 = llvm.add %90, %12 : i64
        %92 = llvm.icmp "slt" %89, %13 : i64
        %93 = llvm.select %92, %91, %89 : i1, i64
        %94 = llvm.sdiv %93, %6  : i64
        %95 = llvm.mul %94, %6 : i64
        %96 = llvm.sub %16, %95 : i64
        %97 = llvm.add %96, %11 : i64
        %98 = llvm.mlir.constant(1024 : i64) : i64
        %99 = llvm.mul %97, %98 : i64
        %100 = llvm.getelementptr %51[%99] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        nvvm.cp.async.wait.group 20
        %101 = llvm.load %41 : !llvm.ptr -> f32
        %102 = llvm.mlir.constant(0 : i64) : i64
        %103 = builtin.unrealized_conversion_cast %102 : i64 to index
        %104 = llvm.mlir.constant(16 : i64) : i64
        %105 = builtin.unrealized_conversion_cast %104 : i64 to index
        %106 = llvm.mlir.constant(1 : i64) : i64
        %107 = builtin.unrealized_conversion_cast %106 : i64 to index
        %108 = scf.for %arg17 = %103 to %105 step %107 iter_args(%arg18 = %101) -> (f32) {
          %109 = builtin.unrealized_conversion_cast %arg17 : index to i64
          %110 = llvm.mlir.constant(64 : i64) : i64
          %111 = llvm.mul %38, %110 : i64
          %112 = llvm.mlir.constant(4 : i64) : i64
          %113 = llvm.mul %109, %112 : i64
          %114 = llvm.add %111, %113 : i64
          %115 = llvm.getelementptr %88[%114] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %116 = llvm.load %115 : !llvm.ptr<3> -> f32
          %117 = llvm.bitcast %116 : f32 to vector<4xi8>
          %118 = llvm.bitcast %117 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %119 = llvm.mlir.constant(64 : i64) : i64
          %120 = llvm.mul %109, %119 : i64
          %121 = llvm.mlir.constant(4 : i64) : i64
          %122 = llvm.mul %39, %121 : i64
          %123 = llvm.add %120, %122 : i64
          %124 = llvm.getelementptr %100[%123] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %125 = llvm.load %124 : !llvm.ptr<3> -> f32
          %126 = llvm.bitcast %125 : f32 to vector<4xi8>
          %127 = llvm.bitcast %126 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %128 = llvm.fmul %118, %127  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %129 = llvm.fadd %arg18, %128  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %129 : f32
        }
        llvm.store %108, %41 : f32, !llvm.ptr
      }
      nvvm.barrier0
      %58 = llvm.load %41 : !llvm.ptr -> f32
      %59 = llvm.bitcast %58 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %60 = llvm.mlir.constant(64 : i64) : i64
      %61 = llvm.mul %17, %60 : i64
      %62 = llvm.mlir.constant(4 : i64) : i64
      %63 = llvm.mul %39, %62 : i64
      %64 = llvm.add %61, %63 : i64
      %65 = llvm.mul %38, %15 : i64
      %66 = llvm.mlir.constant(4 : i64) : i64
      %67 = llvm.mul %65, %66 : i64
      %68 = llvm.add %64, %67 : i64
      %69 = llvm.mlir.constant(16 : i64) : i64
      %70 = llvm.mul %15, %69 : i64
      %71 = llvm.mul %18, %70 : i64
      %72 = llvm.mlir.constant(4 : i64) : i64
      %73 = llvm.mul %71, %72 : i64
      %74 = llvm.add %68, %73 : i64
      %75 = llvm.getelementptr %arg7[%74] : (!llvm.ptr, i64) -> !llvm.ptr, i8
      %76 = llvm.bitcast %59 : vector<4xi8> to f32
      llvm.store %76, %75 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(4 : i64) : i64
  %1 = llvm.mlir.constant(64 : i64) : i64
  %2 = llvm.mlir.constant(1024 : i64) : i64
  %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
  %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
  %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %6 = llvm.mlir.constant(20 : i64) : i64
  %7 = llvm.mlir.constant(16 : i64) : i64
  %8 = llvm.mlir.constant(21 : i64) : i64
  %9 = llvm.mlir.constant(19 : i64) : i64
  %10 = llvm.mlir.constant(5 : i64) : i64
  %11 = llvm.mlir.constant(22 : i64) : i64
  %12 = llvm.mlir.constant(0 : i64) : i64
  %13 = llvm.mlir.constant(1 : i64) : i64
  %14 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  %15 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %16 = builtin.unrealized_conversion_cast %13 : i64 to index
  %17 = builtin.unrealized_conversion_cast %12 : i64 to index
  %18 = llvm.sext %arg11 : i32 to i64
  %19 = llvm.sext %arg10 : i32 to i64
  scf.parallel (%arg12, %arg13) = (%17, %17) to (%15, %14) step (%16, %16) {
    %20 = builtin.unrealized_conversion_cast %arg13 : index to i64
    %21 = builtin.unrealized_conversion_cast %arg12 : index to i64
    %22 = builtin.unrealized_conversion_cast %7 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%17, %17, %17) to (%22, %22, %16) step (%16, %16, %16) {
      %23 = builtin.unrealized_conversion_cast %arg16 : index to i64
      %24 = builtin.unrealized_conversion_cast %arg15 : index to i64
      %25 = builtin.unrealized_conversion_cast %arg14 : index to i64
      %26 = llvm.alloca %13 x f32 : (i64) -> !llvm.ptr
      llvm.store %5, %26 : f32, !llvm.ptr
      nvvm.barrier0
      %27 = llvm.sub %19, %13 : i64
      %28 = llvm.intr.smin(%27, %6)  : (i64, i64) -> i64
      %29 = llvm.add %28, %13 : i64
      scf.for %arg17 = %12 to %29 step %7  : i64 {
        %48 = llvm.sdiv %arg17, %7  : i64
        %49 = llvm.mul %arg17, %2 : i64
        %50 = llvm.getelementptr %3[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %51 = llvm.getelementptr %4[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %52 = llvm.icmp "eq" %25, %12 : i64
        %53 = llvm.icmp "eq" %24, %12 : i64
        %54 = llvm.and %52, %53  : i1
        %55 = llvm.icmp "eq" %23, %12 : i64
        %56 = llvm.and %54, %55  : i1
        scf.if %56 {
          scf.for %arg18 = %17 to %22 step %16 {
            %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
            scf.for %arg19 = %17 to %22 step %16 {
              %58 = builtin.unrealized_conversion_cast %arg19 : index to i64
              scf.for %arg20 = %17 to %16 step %16 {
                %59 = llvm.mul %58, %1 : i64
                %60 = llvm.mul %57, %0 : i64
                %61 = llvm.add %59, %60 : i64
                %62 = llvm.mul %58, %19 : i64
                %63 = llvm.mul %62, %0 : i64
                %64 = llvm.add %63, %60 : i64
                %65 = llvm.mul %48, %1 : i64
                %66 = llvm.add %64, %65 : i64
                %67 = llvm.mul %19, %7 : i64
                %68 = llvm.mul %21, %67 : i64
                %69 = llvm.mul %68, %0 : i64
                %70 = llvm.add %66, %69 : i64
                %71 = llvm.getelementptr %50[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %72 = llvm.getelementptr %arg8[%70] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %73 = llvm.addrspacecast %72 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %71, %73, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %74 = llvm.mul %58, %18 : i64
                %75 = llvm.mul %74, %0 : i64
                %76 = llvm.add %75, %60 : i64
                %77 = llvm.mul %20, %1 : i64
                %78 = llvm.add %76, %77 : i64
                %79 = llvm.mul %18, %7 : i64
                %80 = llvm.mul %48, %79 : i64
                %81 = llvm.mul %80, %0 : i64
                %82 = llvm.add %78, %81 : i64
                %83 = llvm.getelementptr %51[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %84 = llvm.getelementptr %arg9[%82] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %85 = llvm.addrspacecast %84 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %83, %85, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %30 = llvm.add %19, %9 : i64
      %31 = llvm.add %30, %13 : i64
      scf.for %arg17 = %8 to %31 step %13  : i64 {
        %48 = llvm.sub %arg17, %10 : i64
        %49 = llvm.srem %48, %7  : i64
        %50 = llvm.icmp "eq" %49, %12 : i64
        scf.if %50 {
          %51 = llvm.urem %arg17, %11  : i64
          %52 = llvm.mul %51, %2 : i64
          %53 = llvm.getelementptr %3[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %54 = llvm.getelementptr %4[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          nvvm.cp.async.wait.group 20
          %55 = llvm.load %26 : !llvm.ptr -> f32
          %56 = scf.for %arg18 = %17 to %22 step %16 iter_args(%arg19 = %55) -> (f32) {
            %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
            %58 = llvm.mul %24, %1 : i64
            %59 = llvm.mul %57, %0 : i64
            %60 = llvm.add %58, %59 : i64
            %61 = llvm.getelementptr %53[%60] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %62 = llvm.load %61 : !llvm.ptr<3> -> f32
            %63 = llvm.mul %57, %1 : i64
            %64 = llvm.mul %25, %0 : i64
            %65 = llvm.add %63, %64 : i64
            %66 = llvm.getelementptr %54[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %67 = llvm.load %66 : !llvm.ptr<3> -> f32
            %68 = llvm.fmul %62, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
            %69 = llvm.fadd %arg19, %68  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
            scf.yield %69 : f32
          }
          llvm.store %56, %26 : f32, !llvm.ptr
        } else {
          %51 = llvm.add %arg17, %13 : i64
          %52 = llvm.icmp "sge" %19, %51 : i64
          %53 = llvm.srem %arg17, %7  : i64
          %54 = llvm.icmp "eq" %53, %12 : i64
          %55 = llvm.and %52, %54  : i1
          scf.if %55 {
            %56 = llvm.sdiv %arg17, %7  : i64
            %57 = llvm.urem %arg17, %11  : i64
            %58 = llvm.mul %57, %2 : i64
            %59 = llvm.getelementptr %3[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %60 = llvm.getelementptr %4[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %61 = llvm.icmp "eq" %25, %12 : i64
            %62 = llvm.icmp "eq" %24, %12 : i64
            %63 = llvm.and %61, %62  : i1
            %64 = llvm.icmp "eq" %23, %12 : i64
            %65 = llvm.and %63, %64  : i1
            scf.if %65 {
              scf.for %arg18 = %17 to %22 step %16 {
                %66 = builtin.unrealized_conversion_cast %arg18 : index to i64
                scf.for %arg19 = %17 to %22 step %16 {
                  %67 = builtin.unrealized_conversion_cast %arg19 : index to i64
                  scf.for %arg20 = %17 to %16 step %16 {
                    %68 = llvm.mul %67, %1 : i64
                    %69 = llvm.mul %66, %0 : i64
                    %70 = llvm.add %68, %69 : i64
                    %71 = llvm.mul %67, %19 : i64
                    %72 = llvm.mul %71, %0 : i64
                    %73 = llvm.add %72, %69 : i64
                    %74 = llvm.mul %56, %1 : i64
                    %75 = llvm.add %73, %74 : i64
                    %76 = llvm.mul %19, %7 : i64
                    %77 = llvm.mul %21, %76 : i64
                    %78 = llvm.mul %77, %0 : i64
                    %79 = llvm.add %75, %78 : i64
                    %80 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %81 = llvm.getelementptr %arg8[%79] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %82 = llvm.addrspacecast %81 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %80, %82, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %83 = llvm.mul %67, %18 : i64
                    %84 = llvm.mul %83, %0 : i64
                    %85 = llvm.add %84, %69 : i64
                    %86 = llvm.mul %20, %1 : i64
                    %87 = llvm.add %85, %86 : i64
                    %88 = llvm.mul %18, %7 : i64
                    %89 = llvm.mul %56, %88 : i64
                    %90 = llvm.mul %89, %0 : i64
                    %91 = llvm.add %87, %90 : i64
                    %92 = llvm.getelementptr %60[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %93 = llvm.getelementptr %arg9[%91] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %94 = llvm.addrspacecast %93 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %92, %94, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %32 = llvm.icmp "sge" %19, %13 : i64
      %33 = llvm.srem %27, %7  : i64
      %34 = llvm.icmp "eq" %33, %12 : i64
      %35 = llvm.and %32, %34  : i1
      scf.if %35 {
        %48 = llvm.add %19, %6 : i64
        %49 = llvm.sub %48, %11 : i64
        %50 = llvm.add %49, %13 : i64
        %51 = llvm.icmp "slt" %48, %12 : i64
        %52 = llvm.select %51, %50, %48 : i1, i64
        %53 = llvm.sdiv %52, %11  : i64
        %54 = llvm.mul %53, %11 : i64
        %55 = llvm.sub %19, %54 : i64
        %56 = llvm.add %55, %6 : i64
        %57 = llvm.mul %56, %2 : i64
        %58 = llvm.getelementptr %3[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %59 = llvm.getelementptr %4[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        nvvm.cp.async.wait.group 20
        %60 = llvm.load %26 : !llvm.ptr -> f32
        %61 = scf.for %arg17 = %17 to %22 step %16 iter_args(%arg18 = %60) -> (f32) {
          %62 = builtin.unrealized_conversion_cast %arg17 : index to i64
          %63 = llvm.mul %24, %1 : i64
          %64 = llvm.mul %62, %0 : i64
          %65 = llvm.add %63, %64 : i64
          %66 = llvm.getelementptr %58[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %67 = llvm.load %66 : !llvm.ptr<3> -> f32
          %68 = llvm.mul %62, %1 : i64
          %69 = llvm.mul %25, %0 : i64
          %70 = llvm.add %68, %69 : i64
          %71 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %72 = llvm.load %71 : !llvm.ptr<3> -> f32
          %73 = llvm.fmul %67, %72  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %74 = llvm.fadd %arg18, %73  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %74 : f32
        }
        llvm.store %61, %26 : f32, !llvm.ptr
      }
      nvvm.barrier0
      %36 = llvm.load %26 : !llvm.ptr -> f32
      %37 = llvm.mul %20, %1 : i64
      %38 = llvm.mul %25, %0 : i64
      %39 = llvm.add %37, %38 : i64
      %40 = llvm.mul %24, %18 : i64
      %41 = llvm.mul %40, %0 : i64
      %42 = llvm.add %39, %41 : i64
      %43 = llvm.mul %18, %7 : i64
      %44 = llvm.mul %21, %43 : i64
      %45 = llvm.mul %44, %0 : i64
      %46 = llvm.add %42, %45 : i64
      %47 = llvm.getelementptr %arg7[%46] : (!llvm.ptr, i64) -> !llvm.ptr, i8
      llvm.store %36, %47 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: After gpu module lower accesses:gpu.module @__mlir_gpu_module [#nvvm.target<chip = "sm_80">] {
  llvm.mlir.global external @shared_mem_1() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
    %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
    llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
  }
  llvm.mlir.global external @shared_mem_0() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
    %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
    llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
  }
  llvm.comdat @__llvm_global_comdat {
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2As any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2Bs any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2As any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2Bs any
    llvm.comdat_selector @_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii any
    llvm.comdat_selector @_Z13MatrixMulCUDAILi32EEvPfS0_S0_ii any
  }
  llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
    %0 = llvm.mlir.constant(4 : i64) : i64
    %1 = llvm.mlir.constant(64 : i64) : i64
    %2 = llvm.mlir.constant(1024 : i64) : i64
    %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
    %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
    %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
    %6 = llvm.mlir.constant(20 : i64) : i64
    %7 = llvm.mlir.constant(16 : i64) : i64
    %8 = llvm.mlir.constant(21 : i64) : i64
    %9 = llvm.mlir.constant(19 : i64) : i64
    %10 = llvm.mlir.constant(5 : i64) : i64
    %11 = llvm.mlir.constant(22 : i64) : i64
    %12 = llvm.mlir.constant(0 : i64) : i64
    %13 = llvm.mlir.constant(1 : i64) : i64
    %14 = builtin.unrealized_conversion_cast %arg0 : i64 to index
    %15 = builtin.unrealized_conversion_cast %arg1 : i64 to index
    %16 = builtin.unrealized_conversion_cast %13 : i64 to index
    %17 = builtin.unrealized_conversion_cast %12 : i64 to index
    %18 = llvm.sext %arg11 : i32 to i64
    %19 = llvm.sext %arg10 : i32 to i64
    scf.parallel (%arg12, %arg13) = (%17, %17) to (%15, %14) step (%16, %16) {
      %20 = builtin.unrealized_conversion_cast %arg13 : index to i64
      %21 = builtin.unrealized_conversion_cast %arg12 : index to i64
      %22 = builtin.unrealized_conversion_cast %7 : i64 to index
      scf.parallel (%arg14, %arg15, %arg16) = (%17, %17, %17) to (%22, %22, %16) step (%16, %16, %16) {
        %23 = builtin.unrealized_conversion_cast %arg16 : index to i64
        %24 = builtin.unrealized_conversion_cast %arg15 : index to i64
        %25 = builtin.unrealized_conversion_cast %arg14 : index to i64
        %26 = llvm.alloca %13 x f32 : (i64) -> !llvm.ptr
        llvm.store %5, %26 : f32, !llvm.ptr
        nvvm.barrier0
        %27 = llvm.sub %19, %13 : i64
        %28 = llvm.intr.smin(%27, %6)  : (i64, i64) -> i64
        %29 = llvm.add %28, %13 : i64
        scf.for %arg17 = %12 to %29 step %7  : i64 {
          %48 = llvm.sdiv %arg17, %7  : i64
          %49 = llvm.mul %arg17, %2 : i64
          %50 = llvm.getelementptr %3[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %51 = llvm.getelementptr %4[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %52 = llvm.icmp "eq" %25, %12 : i64
          %53 = llvm.icmp "eq" %24, %12 : i64
          %54 = llvm.and %52, %53  : i1
          %55 = llvm.icmp "eq" %23, %12 : i64
          %56 = llvm.and %54, %55  : i1
          scf.if %56 {
            scf.for %arg18 = %17 to %22 step %16 {
              %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
              scf.for %arg19 = %17 to %22 step %16 {
                %58 = builtin.unrealized_conversion_cast %arg19 : index to i64
                scf.for %arg20 = %17 to %16 step %16 {
                  %59 = llvm.mul %58, %1 : i64
                  %60 = llvm.mul %57, %0 : i64
                  %61 = llvm.add %59, %60 : i64
                  %62 = llvm.mul %58, %19 : i64
                  %63 = llvm.mul %62, %0 : i64
                  %64 = llvm.add %63, %60 : i64
                  %65 = llvm.mul %48, %1 : i64
                  %66 = llvm.add %64, %65 : i64
                  %67 = llvm.mul %19, %7 : i64
                  %68 = llvm.mul %21, %67 : i64
                  %69 = llvm.mul %68, %0 : i64
                  %70 = llvm.add %66, %69 : i64
                  %71 = llvm.getelementptr %50[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %72 = llvm.getelementptr %arg8[%70] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %73 = llvm.addrspacecast %72 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %71, %73, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  %74 = llvm.mul %58, %18 : i64
                  %75 = llvm.mul %74, %0 : i64
                  %76 = llvm.add %75, %60 : i64
                  %77 = llvm.mul %20, %1 : i64
                  %78 = llvm.add %76, %77 : i64
                  %79 = llvm.mul %18, %7 : i64
                  %80 = llvm.mul %48, %79 : i64
                  %81 = llvm.mul %80, %0 : i64
                  %82 = llvm.add %78, %81 : i64
                  %83 = llvm.getelementptr %51[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %84 = llvm.getelementptr %arg9[%82] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %85 = llvm.addrspacecast %84 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %83, %85, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                }
              }
            }
            nvvm.cp.async.commit.group
          }
          nvvm.barrier0
        }
        nvvm.barrier0
        %30 = llvm.add %19, %9 : i64
        %31 = llvm.add %30, %13 : i64
        scf.for %arg17 = %8 to %31 step %13  : i64 {
          %48 = llvm.sub %arg17, %10 : i64
          %49 = llvm.srem %48, %7  : i64
          %50 = llvm.icmp "eq" %49, %12 : i64
          scf.if %50 {
            %51 = llvm.urem %arg17, %11  : i64
            %52 = llvm.mul %51, %2 : i64
            %53 = llvm.getelementptr %3[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %54 = llvm.getelementptr %4[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            nvvm.cp.async.wait.group 20
            %55 = llvm.load %26 : !llvm.ptr -> f32
            %56 = scf.for %arg18 = %17 to %22 step %16 iter_args(%arg19 = %55) -> (f32) {
              %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
              %58 = llvm.mul %24, %1 : i64
              %59 = llvm.mul %57, %0 : i64
              %60 = llvm.add %58, %59 : i64
              %61 = llvm.getelementptr %53[%60] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %62 = llvm.load %61 : !llvm.ptr<3> -> f32
              %63 = llvm.mul %57, %1 : i64
              %64 = llvm.mul %25, %0 : i64
              %65 = llvm.add %63, %64 : i64
              %66 = llvm.getelementptr %54[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %67 = llvm.load %66 : !llvm.ptr<3> -> f32
              %68 = llvm.fmul %62, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
              %69 = llvm.fadd %arg19, %68  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
              scf.yield %69 : f32
            }
            llvm.store %56, %26 : f32, !llvm.ptr
          } else {
            %51 = llvm.add %arg17, %13 : i64
            %52 = llvm.icmp "sge" %19, %51 : i64
            %53 = llvm.srem %arg17, %7  : i64
            %54 = llvm.icmp "eq" %53, %12 : i64
            %55 = llvm.and %52, %54  : i1
            scf.if %55 {
              %56 = llvm.sdiv %arg17, %7  : i64
              %57 = llvm.urem %arg17, %11  : i64
              %58 = llvm.mul %57, %2 : i64
              %59 = llvm.getelementptr %3[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %60 = llvm.getelementptr %4[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %61 = llvm.icmp "eq" %25, %12 : i64
              %62 = llvm.icmp "eq" %24, %12 : i64
              %63 = llvm.and %61, %62  : i1
              %64 = llvm.icmp "eq" %23, %12 : i64
              %65 = llvm.and %63, %64  : i1
              scf.if %65 {
                scf.for %arg18 = %17 to %22 step %16 {
                  %66 = builtin.unrealized_conversion_cast %arg18 : index to i64
                  scf.for %arg19 = %17 to %22 step %16 {
                    %67 = builtin.unrealized_conversion_cast %arg19 : index to i64
                    scf.for %arg20 = %17 to %16 step %16 {
                      %68 = llvm.mul %67, %1 : i64
                      %69 = llvm.mul %66, %0 : i64
                      %70 = llvm.add %68, %69 : i64
                      %71 = llvm.mul %67, %19 : i64
                      %72 = llvm.mul %71, %0 : i64
                      %73 = llvm.add %72, %69 : i64
                      %74 = llvm.mul %56, %1 : i64
                      %75 = llvm.add %73, %74 : i64
                      %76 = llvm.mul %19, %7 : i64
                      %77 = llvm.mul %21, %76 : i64
                      %78 = llvm.mul %77, %0 : i64
                      %79 = llvm.add %75, %78 : i64
                      %80 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                      %81 = llvm.getelementptr %arg8[%79] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                      %82 = llvm.addrspacecast %81 : !llvm.ptr to !llvm.ptr<1>
                      nvvm.cp.async.shared.global %80, %82, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                      %83 = llvm.mul %67, %18 : i64
                      %84 = llvm.mul %83, %0 : i64
                      %85 = llvm.add %84, %69 : i64
                      %86 = llvm.mul %20, %1 : i64
                      %87 = llvm.add %85, %86 : i64
                      %88 = llvm.mul %18, %7 : i64
                      %89 = llvm.mul %56, %88 : i64
                      %90 = llvm.mul %89, %0 : i64
                      %91 = llvm.add %87, %90 : i64
                      %92 = llvm.getelementptr %60[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                      %93 = llvm.getelementptr %arg9[%91] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                      %94 = llvm.addrspacecast %93 : !llvm.ptr to !llvm.ptr<1>
                      nvvm.cp.async.shared.global %92, %94, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    }
                  }
                }
                nvvm.cp.async.commit.group
              }
            }
          }
          nvvm.barrier0
        }
        nvvm.barrier0
        %32 = llvm.icmp "sge" %19, %13 : i64
        %33 = llvm.srem %27, %7  : i64
        %34 = llvm.icmp "eq" %33, %12 : i64
        %35 = llvm.and %32, %34  : i1
        scf.if %35 {
          %48 = llvm.add %19, %6 : i64
          %49 = llvm.sub %48, %11 : i64
          %50 = llvm.add %49, %13 : i64
          %51 = llvm.icmp "slt" %48, %12 : i64
          %52 = llvm.select %51, %50, %48 : i1, i64
          %53 = llvm.sdiv %52, %11  : i64
          %54 = llvm.mul %53, %11 : i64
          %55 = llvm.sub %19, %54 : i64
          %56 = llvm.add %55, %6 : i64
          %57 = llvm.mul %56, %2 : i64
          %58 = llvm.getelementptr %3[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %59 = llvm.getelementptr %4[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          nvvm.cp.async.wait.group 20
          %60 = llvm.load %26 : !llvm.ptr -> f32
          %61 = scf.for %arg17 = %17 to %22 step %16 iter_args(%arg18 = %60) -> (f32) {
            %62 = builtin.unrealized_conversion_cast %arg17 : index to i64
            %63 = llvm.mul %24, %1 : i64
            %64 = llvm.mul %62, %0 : i64
            %65 = llvm.add %63, %64 : i64
            %66 = llvm.getelementptr %58[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %67 = llvm.load %66 : !llvm.ptr<3> -> f32
            %68 = llvm.mul %62, %1 : i64
            %69 = llvm.mul %25, %0 : i64
            %70 = llvm.add %68, %69 : i64
            %71 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %72 = llvm.load %71 : !llvm.ptr<3> -> f32
            %73 = llvm.fmul %67, %72  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
            %74 = llvm.fadd %arg18, %73  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
            scf.yield %74 : f32
          }
          llvm.store %61, %26 : f32, !llvm.ptr
        }
        nvvm.barrier0
        %36 = llvm.load %26 : !llvm.ptr -> f32
        %37 = llvm.mul %20, %1 : i64
        %38 = llvm.mul %25, %0 : i64
        %39 = llvm.add %37, %38 : i64
        %40 = llvm.mul %24, %18 : i64
        %41 = llvm.mul %40, %0 : i64
        %42 = llvm.add %39, %41 : i64
        %43 = llvm.mul %18, %7 : i64
        %44 = llvm.mul %21, %43 : i64
        %45 = llvm.mul %44, %0 : i64
        %46 = llvm.add %42, %45 : i64
        %47 = llvm.getelementptr %arg7[%46] : (!llvm.ptr, i64) -> !llvm.ptr, i8
        llvm.store %36, %47 : f32, !llvm.ptr
        scf.reduce 
      } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
      scf.reduce 
    } {gpu.par.grid}
    llvm.return {polymer.stmt.name = "S35_llvm_return"}
  }
}
module attributes {dlti.dl_spec = #dlti.dl_spec<f80 = dense<128> : vector<2xi64>, i128 = dense<128> : vector<2xi64>, !llvm.ptr<272> = dense<64> : vector<4xi64>, i64 = dense<64> : vector<2xi64>, !llvm.ptr<270> = dense<32> : vector<4xi64>, f128 = dense<128> : vector<2xi64>, !llvm.ptr<271> = dense<32> : vector<4xi64>, f16 = dense<16> : vector<2xi64>, f64 = dense<64> : vector<2xi64>, i16 = dense<16> : vector<2xi64>, i32 = dense<32> : vector<2xi64>, i1 = dense<8> : vector<2xi64>, i8 = dense<8> : vector<2xi64>, !llvm.ptr = dense<64> : vector<4xi64>, "dlti.stack_alignment" = 128 : i64, "dlti.endianness" = "little">, gpu.container_module} {
  gpu.module @__mlir_gpu_module [#nvvm.target<chip = "sm_80">] {
    llvm.mlir.global external @shared_mem_1() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.mlir.global external @shared_mem_0() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.comdat @__llvm_global_comdat {
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi32EEvPfS0_S0_ii any
    }
    llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
      %0 = llvm.mlir.constant(4 : i64) : i64
      %1 = llvm.mlir.constant(64 : i64) : i64
      %2 = llvm.mlir.constant(1024 : i64) : i64
      %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
      %6 = llvm.mlir.constant(20 : i64) : i64
      %7 = llvm.mlir.constant(16 : i64) : i64
      %8 = llvm.mlir.constant(21 : i64) : i64
      %9 = llvm.mlir.constant(19 : i64) : i64
      %10 = llvm.mlir.constant(5 : i64) : i64
      %11 = llvm.mlir.constant(22 : i64) : i64
      %12 = llvm.mlir.constant(0 : i64) : i64
      %13 = llvm.mlir.constant(1 : i64) : i64
      %14 = builtin.unrealized_conversion_cast %arg0 : i64 to index
      %15 = builtin.unrealized_conversion_cast %arg1 : i64 to index
      %16 = builtin.unrealized_conversion_cast %13 : i64 to index
      %17 = builtin.unrealized_conversion_cast %12 : i64 to index
      %18 = llvm.sext %arg11 : i32 to i64
      %19 = llvm.sext %arg10 : i32 to i64
      scf.parallel (%arg12, %arg13) = (%17, %17) to (%15, %14) step (%16, %16) {
        %20 = builtin.unrealized_conversion_cast %arg13 : index to i64
        %21 = builtin.unrealized_conversion_cast %arg12 : index to i64
        %22 = builtin.unrealized_conversion_cast %7 : i64 to index
        scf.parallel (%arg14, %arg15, %arg16) = (%17, %17, %17) to (%22, %22, %16) step (%16, %16, %16) {
          %23 = builtin.unrealized_conversion_cast %arg16 : index to i64
          %24 = builtin.unrealized_conversion_cast %arg15 : index to i64
          %25 = builtin.unrealized_conversion_cast %arg14 : index to i64
          %26 = llvm.alloca %13 x f32 : (i64) -> !llvm.ptr
          llvm.store %5, %26 : f32, !llvm.ptr
          nvvm.barrier0
          %27 = llvm.sub %19, %13 : i64
          %28 = llvm.intr.smin(%27, %6)  : (i64, i64) -> i64
          %29 = llvm.add %28, %13 : i64
          scf.for %arg17 = %12 to %29 step %7  : i64 {
            %48 = llvm.sdiv %arg17, %7  : i64
            %49 = llvm.mul %arg17, %2 : i64
            %50 = llvm.getelementptr %3[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %51 = llvm.getelementptr %4[%49] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %52 = llvm.icmp "eq" %25, %12 : i64
            %53 = llvm.icmp "eq" %24, %12 : i64
            %54 = llvm.and %52, %53  : i1
            %55 = llvm.icmp "eq" %23, %12 : i64
            %56 = llvm.and %54, %55  : i1
            scf.if %56 {
              scf.for %arg18 = %17 to %22 step %16 {
                %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
                scf.for %arg19 = %17 to %22 step %16 {
                  %58 = builtin.unrealized_conversion_cast %arg19 : index to i64
                  scf.for %arg20 = %17 to %16 step %16 {
                    %59 = llvm.mul %58, %1 : i64
                    %60 = llvm.mul %57, %0 : i64
                    %61 = llvm.add %59, %60 : i64
                    %62 = llvm.mul %58, %19 : i64
                    %63 = llvm.mul %62, %0 : i64
                    %64 = llvm.add %63, %60 : i64
                    %65 = llvm.mul %48, %1 : i64
                    %66 = llvm.add %64, %65 : i64
                    %67 = llvm.mul %19, %7 : i64
                    %68 = llvm.mul %21, %67 : i64
                    %69 = llvm.mul %68, %0 : i64
                    %70 = llvm.add %66, %69 : i64
                    %71 = llvm.getelementptr %50[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %72 = llvm.getelementptr %arg8[%70] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %73 = llvm.addrspacecast %72 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %71, %73, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %74 = llvm.mul %58, %18 : i64
                    %75 = llvm.mul %74, %0 : i64
                    %76 = llvm.add %75, %60 : i64
                    %77 = llvm.mul %20, %1 : i64
                    %78 = llvm.add %76, %77 : i64
                    %79 = llvm.mul %18, %7 : i64
                    %80 = llvm.mul %48, %79 : i64
                    %81 = llvm.mul %80, %0 : i64
                    %82 = llvm.add %78, %81 : i64
                    %83 = llvm.getelementptr %51[%61] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %84 = llvm.getelementptr %arg9[%82] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %85 = llvm.addrspacecast %84 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %83, %85, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
            nvvm.barrier0
          }
          nvvm.barrier0
          %30 = llvm.add %19, %9 : i64
          %31 = llvm.add %30, %13 : i64
          scf.for %arg17 = %8 to %31 step %13  : i64 {
            %48 = llvm.sub %arg17, %10 : i64
            %49 = llvm.srem %48, %7  : i64
            %50 = llvm.icmp "eq" %49, %12 : i64
            scf.if %50 {
              %51 = llvm.urem %arg17, %11  : i64
              %52 = llvm.mul %51, %2 : i64
              %53 = llvm.getelementptr %3[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %54 = llvm.getelementptr %4[%52] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              nvvm.cp.async.wait.group 20
              %55 = llvm.load %26 : !llvm.ptr -> f32
              %56 = scf.for %arg18 = %17 to %22 step %16 iter_args(%arg19 = %55) -> (f32) {
                %57 = builtin.unrealized_conversion_cast %arg18 : index to i64
                %58 = llvm.mul %24, %1 : i64
                %59 = llvm.mul %57, %0 : i64
                %60 = llvm.add %58, %59 : i64
                %61 = llvm.getelementptr %53[%60] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %62 = llvm.load %61 : !llvm.ptr<3> -> f32
                %63 = llvm.mul %57, %1 : i64
                %64 = llvm.mul %25, %0 : i64
                %65 = llvm.add %63, %64 : i64
                %66 = llvm.getelementptr %54[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %67 = llvm.load %66 : !llvm.ptr<3> -> f32
                %68 = llvm.fmul %62, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
                %69 = llvm.fadd %arg19, %68  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
                scf.yield %69 : f32
              }
              llvm.store %56, %26 : f32, !llvm.ptr
            } else {
              %51 = llvm.add %arg17, %13 : i64
              %52 = llvm.icmp "sge" %19, %51 : i64
              %53 = llvm.srem %arg17, %7  : i64
              %54 = llvm.icmp "eq" %53, %12 : i64
              %55 = llvm.and %52, %54  : i1
              scf.if %55 {
                %56 = llvm.sdiv %arg17, %7  : i64
                %57 = llvm.urem %arg17, %11  : i64
                %58 = llvm.mul %57, %2 : i64
                %59 = llvm.getelementptr %3[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %60 = llvm.getelementptr %4[%58] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %61 = llvm.icmp "eq" %25, %12 : i64
                %62 = llvm.icmp "eq" %24, %12 : i64
                %63 = llvm.and %61, %62  : i1
                %64 = llvm.icmp "eq" %23, %12 : i64
                %65 = llvm.and %63, %64  : i1
                scf.if %65 {
                  scf.for %arg18 = %17 to %22 step %16 {
                    %66 = builtin.unrealized_conversion_cast %arg18 : index to i64
                    scf.for %arg19 = %17 to %22 step %16 {
                      %67 = builtin.unrealized_conversion_cast %arg19 : index to i64
                      scf.for %arg20 = %17 to %16 step %16 {
                        %68 = llvm.mul %67, %1 : i64
                        %69 = llvm.mul %66, %0 : i64
                        %70 = llvm.add %68, %69 : i64
                        %71 = llvm.mul %67, %19 : i64
                        %72 = llvm.mul %71, %0 : i64
                        %73 = llvm.add %72, %69 : i64
                        %74 = llvm.mul %56, %1 : i64
                        %75 = llvm.add %73, %74 : i64
                        %76 = llvm.mul %19, %7 : i64
                        %77 = llvm.mul %21, %76 : i64
                        %78 = llvm.mul %77, %0 : i64
                        %79 = llvm.add %75, %78 : i64
                        %80 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                        %81 = llvm.getelementptr %arg8[%79] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                        %82 = llvm.addrspacecast %81 : !llvm.ptr to !llvm.ptr<1>
                        nvvm.cp.async.shared.global %80, %82, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                        %83 = llvm.mul %67, %18 : i64
                        %84 = llvm.mul %83, %0 : i64
                        %85 = llvm.add %84, %69 : i64
                        %86 = llvm.mul %20, %1 : i64
                        %87 = llvm.add %85, %86 : i64
                        %88 = llvm.mul %18, %7 : i64
                        %89 = llvm.mul %56, %88 : i64
                        %90 = llvm.mul %89, %0 : i64
                        %91 = llvm.add %87, %90 : i64
                        %92 = llvm.getelementptr %60[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                        %93 = llvm.getelementptr %arg9[%91] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                        %94 = llvm.addrspacecast %93 : !llvm.ptr to !llvm.ptr<1>
                        nvvm.cp.async.shared.global %92, %94, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                      }
                    }
                  }
                  nvvm.cp.async.commit.group
                }
              }
            }
            nvvm.barrier0
          }
          nvvm.barrier0
          %32 = llvm.icmp "sge" %19, %13 : i64
          %33 = llvm.srem %27, %7  : i64
          %34 = llvm.icmp "eq" %33, %12 : i64
          %35 = llvm.and %32, %34  : i1
          scf.if %35 {
            %48 = llvm.add %19, %6 : i64
            %49 = llvm.sub %48, %11 : i64
            %50 = llvm.add %49, %13 : i64
            %51 = llvm.icmp "slt" %48, %12 : i64
            %52 = llvm.select %51, %50, %48 : i1, i64
            %53 = llvm.sdiv %52, %11  : i64
            %54 = llvm.mul %53, %11 : i64
            %55 = llvm.sub %19, %54 : i64
            %56 = llvm.add %55, %6 : i64
            %57 = llvm.mul %56, %2 : i64
            %58 = llvm.getelementptr %3[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %59 = llvm.getelementptr %4[%57] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            nvvm.cp.async.wait.group 20
            %60 = llvm.load %26 : !llvm.ptr -> f32
            %61 = scf.for %arg17 = %17 to %22 step %16 iter_args(%arg18 = %60) -> (f32) {
              %62 = builtin.unrealized_conversion_cast %arg17 : index to i64
              %63 = llvm.mul %24, %1 : i64
              %64 = llvm.mul %62, %0 : i64
              %65 = llvm.add %63, %64 : i64
              %66 = llvm.getelementptr %58[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %67 = llvm.load %66 : !llvm.ptr<3> -> f32
              %68 = llvm.mul %62, %1 : i64
              %69 = llvm.mul %25, %0 : i64
              %70 = llvm.add %68, %69 : i64
              %71 = llvm.getelementptr %59[%70] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %72 = llvm.load %71 : !llvm.ptr<3> -> f32
              %73 = llvm.fmul %67, %72  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
              %74 = llvm.fadd %arg18, %73  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
              scf.yield %74 : f32
            }
            llvm.store %61, %26 : f32, !llvm.ptr
          }
          nvvm.barrier0
          %36 = llvm.load %26 : !llvm.ptr -> f32
          %37 = llvm.mul %20, %1 : i64
          %38 = llvm.mul %25, %0 : i64
          %39 = llvm.add %37, %38 : i64
          %40 = llvm.mul %24, %18 : i64
          %41 = llvm.mul %40, %0 : i64
          %42 = llvm.add %39, %41 : i64
          %43 = llvm.mul %18, %7 : i64
          %44 = llvm.mul %21, %43 : i64
          %45 = llvm.mul %44, %0 : i64
          %46 = llvm.add %42, %45 : i64
          %47 = llvm.getelementptr %arg7[%46] : (!llvm.ptr, i64) -> !llvm.ptr, i8
          llvm.store %36, %47 : f32, !llvm.ptr
          scf.reduce 
        } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
        scf.reduce 
      } {gpu.par.grid}
      llvm.return {polymer.stmt.name = "S35_llvm_return"}
    }
  }
}

Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
