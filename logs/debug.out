gpu-affine-opt: Before opt:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.addi %12, %arg10 : i32
      %14 = arith.shli %7, %c4_i32 : i32
      %15 = arith.shli %arg11, %c4_i32 : i32
      %16 = arith.muli %10, %arg10 : i32
      %17 = arith.addi %16, %9 : i32
      %18 = arith.extui %10 : i32 to i64
      %19 = arith.extui %9 : i32 to i64
      %20 = llvm.getelementptr inbounds %5[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %21 = arith.muli %10, %arg11 : i32
      %22 = arith.addi %21, %9 : i32
      %23 = llvm.getelementptr inbounds %6[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %24:2 = scf.for %arg18 = %12 to %13 step %c16_i32 iter_args(%arg19 = %14, %arg20 = %0) -> (i32, f32)  : i32 {
        %31 = arith.addi %17, %arg18 : i32
        %32 = arith.extsi %31 : i32 to i64
        %33 = llvm.getelementptr inbounds %arg8[%32] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %34 = llvm.load %33 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %34, %20 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %35 = arith.addi %22, %arg19 : i32
        %36 = arith.extsi %35 : i32 to i64
        %37 = llvm.getelementptr inbounds %arg9[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %38 = llvm.load %37 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %38, %23 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %39 = scf.for %arg21 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg22 = %arg20) -> (f32)  : i32 {
          %41 = arith.extui %arg21 : i32 to i64
          %42 = llvm.getelementptr inbounds %5[0, %18, %41] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %43 = llvm.load %42 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %44 = llvm.getelementptr inbounds %6[0, %41, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %45 = llvm.load %44 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %46 = llvm.fmul %43, %45  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %47 = llvm.fadd %arg22, %46  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %47 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %40 = arith.addi %arg19, %15 : i32
        scf.yield %40, %39 : i32, f32
      }
      %25 = arith.muli %15, %8 : i32
      %26 = arith.addi %14, %9 : i32
      %27 = arith.addi %26, %21 : i32
      %28 = arith.addi %27, %25 : i32
      %29 = arith.extsi %28 : i32 to i64
      %30 = llvm.getelementptr inbounds %arg7[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %24#1, %30 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Removed IVs:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.shli %7, %c4_i32 : i32
      %14 = arith.shli %arg11, %c4_i32 : i32
      %15 = arith.muli %10, %arg10 : i32
      %16 = arith.addi %15, %9 : i32
      %17 = arith.extui %10 : i32 to i64
      %18 = arith.extui %9 : i32 to i64
      %19 = llvm.getelementptr inbounds %5[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %20 = arith.muli %10, %arg11 : i32
      %21 = arith.addi %20, %9 : i32
      %22 = llvm.getelementptr inbounds %6[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %23 = arith.subi %arg10, %c1_i32 : i32
      %24 = arith.divui %23, %c16_i32 : i32
      %25 = arith.addi %24, %c1_i32 : i32
      %26 = scf.for %arg18 = %c0_i32 to %25 step %c1_i32 iter_args(%arg19 = %0) -> (f32)  : i32 {
        %33 = arith.muli %arg18, %14 : i32
        %34 = arith.addi %33, %13 : i32
        %35 = arith.muli %arg18, %c16_i32 : i32
        %36 = arith.addi %12, %35 : i32
        %37 = arith.addi %16, %36 : i32
        %38 = arith.extsi %37 : i32 to i64
        %39 = llvm.getelementptr inbounds %arg8[%38] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %40 = llvm.load %39 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %40, %19 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %41 = arith.addi %21, %34 : i32
        %42 = arith.extsi %41 : i32 to i64
        %43 = llvm.getelementptr inbounds %arg9[%42] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %44 = llvm.load %43 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %44, %22 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %45 = scf.for %arg20 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg21 = %arg19) -> (f32)  : i32 {
          %46 = arith.extui %arg20 : i32 to i64
          %47 = llvm.getelementptr inbounds %5[0, %17, %46] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %48 = llvm.load %47 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %49 = llvm.getelementptr inbounds %6[0, %46, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %50 = llvm.load %49 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %51 = llvm.fmul %48, %50  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %52 = llvm.fadd %arg21, %51  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %52 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        scf.yield %45 : f32
      }
      %27 = arith.muli %14, %8 : i32
      %28 = arith.addi %13, %9 : i32
      %29 = arith.addi %28, %20 : i32
      %30 = arith.addi %29, %27 : i32
      %31 = arith.extsi %30 : i32 to i64
      %32 = llvm.getelementptr inbounds %arg7[%31] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %26, %32 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: To Affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.for %arg18 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] iter_args(%arg19 = %0) -> (f32) {
        %15 = affine.vector_load %2[(%arg16 * symbol(%7)) * 4 + %arg15 * 4 + %arg18 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %15, %alloca[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %16 = affine.vector_load %1[(%arg16 * symbol(%5)) * 4 + %arg15 * 4 + %arg12 * 64 + (%arg18 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %16, %alloca_0[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %17 = affine.for %arg20 = 0 to 16 iter_args(%arg21 = %arg19) -> (f32) {
          %18 = affine.vector_load %alloca[%arg16 * 64 + %arg20 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %19 = llvm.bitcast %18 : vector<4xi8> to f32
          %20 = affine.vector_load %alloca_0[%arg20 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %21 = llvm.bitcast %20 : vector<4xi8> to f32
          %22 = llvm.fmul %19, %21  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %23 = llvm.fadd %arg21, %22  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %23 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        affine.yield %17 : f32
      }
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Distributed:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c16 = arith.constant 16 : index
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca(%c16, %c16, %c1) : memref<?x?x?xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.vector_load %2[(%arg17 * symbol(%7)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %13, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %14 = affine.vector_load %1[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %14, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
        %14 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %13) -> (f32) {
          %15 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %16 = llvm.bitcast %15 : vector<4xi8> to f32
          %17 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %18 = llvm.bitcast %17 : vector<4xi8> to f32
          %19 = llvm.fmul %16, %18  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %20 = llvm.fadd %arg20, %19  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %20 : f32
        }
        affine.store %14, %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg10 : i32 to index
  %6 = arith.index_cast %arg1 : i64 to index
  %7 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%7), symbol(%6), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca() : memref<16x16x1xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%5] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.vector_load %2[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%5) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %9 = affine.vector_load %1[(%arg17 * symbol(%4)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %9, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
        %9 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %8) -> (f32) {
          %10 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %11 = llvm.bitcast %10 : vector<4xi8> to f32
          %12 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %13 = llvm.bitcast %12 : vector<4xi8> to f32
          %14 = llvm.fmul %11, %13  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %15 = llvm.fadd %arg20, %14  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %15 : f32
        }
        affine.store %9, %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %8 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
      %9 = llvm.bitcast %8 : f32 to vector<4xi8>
      affine.vector_store %9, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%4)) * 4 + (%arg13 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
Schedule:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S10_memref_alloca[i0, i1, i2]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S30_affine_load[i0, i1, i2, i3, i4, i5]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S34_affine_yield[i0, i1, i2]; S8_memref_alloca[i0, i1, i2]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S11_affine_store[i0, i1, i2, i3, i4, i5]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S33_affine_yield[i0, i1, i2, i3, i4, i5]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S12_affine_yield[i0, i1, i2, i3, i4, i5]; S9_memref_alloca[i0, i1, i2]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L16_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L15_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L14_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5]; S11_affine_store[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L0_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S29_affine_yield[i0, i1, i2, i3]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L10_affine_for[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L5_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L4_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L9_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L8_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L7_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                              child:
                                schedule: "[P0, P1, P2, P3] -> L6_affine_for[{ S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)] }]"
                                array_expansion: [ none ]
                                child:
                                  sequence:
                                  - filter: "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
                            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S30_affine_load[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L13_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L12_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L11_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S11_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
  - S12_affine_yield:
  - S13_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S14_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_7[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S15_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S16_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_10[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S17_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_6[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_9[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S18_affine_load:
        - read "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - must_write "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
  - S19_affine_store_var:
        - read "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
        - must_write "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S20_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_7[64i5 + 4i7] }"
        - must_write "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S21_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S22_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_10[4i4 + 64i7] }"
        - must_write "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S23_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
        - read "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S24_llvm_fmul:
        - must_write "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
  - S25_llvm_fadd:
        - must_write "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
  - S26_affine_yield:
        - must_write "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_13[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_14[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_15[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_16[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fmul_res_17[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fadd_res_18[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
  - S27_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - read "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S28_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_load_res_11[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_for_res_12[] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S29_affine_yield:
  - S30_affine_load:
        - read "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
        - must_write "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S31_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
        - read "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S32_affine_vector_store:
        - may_write "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, 0, i3, i4, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - read "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
  - S33_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_affine_load_res_19[] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_llvm_bitcast_res_20[] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
  - S35_llvm_return:
Schedule:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S0_llvm_mlir_constant[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3]; S8_memref_alloca[i0, i1, i2]; RS3_affine_parallel[i0, i1, i2]; S29_affine_yield[i0, i1, i2, i3]; S10_memref_alloca[i0, i1, i2]; S9_memref_alloca[i0, i1, i2]; RS0_affine_parallel[i0, i1, i2] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i2)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)]; RS3_affine_parallel[i0, i1, i2] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S0_llvm_mlir_constant[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S29_affine_yield:
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
  - S35_llvm_return:
  - RS0_affine_parallel:
        - must_write "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS1_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
  - RS2_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P2 > 0 and P3 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - must_write "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P2 > 0 and P3 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS3_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - may_write "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P2, P3, P0, P1] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] }
dep_order for A_llvm_func_arg_11_0 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_10_1 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_1_2 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_0_3 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_4 [P2, P3, P0, P1] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 > i3 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P2 and o3 > i3 and 16o3 < P0); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and o1 > i1 and o1 < P2) }
dep_order for A_memref_ataddr_res_5 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_6 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_7 [P2, P3, P0, P1] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0) }
dep_order for A_memref_ataddr_res_8 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_9 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_10 [P2, P3, P0, P1] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0) }
dep_order for A_affine_load_res_11 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_for_res_12 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_13 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_14 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_15 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_16 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_fmul_res_17 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_fadd_res_18 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_load_res_19 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_20 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_ataddr_res_21 [P2, P3, P0, P1] -> {  }
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P2, P3, P0, P1] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] }
dep_order for A_llvm_func_arg_11_0 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_10_1 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_1_2 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_func_arg_0_3 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_4 [P2, P3, P0, P1] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 > i3 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P2 and o3 > i3 and 16o3 < P0); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and o1 > i1 and o1 < P2) }
dep_order for A_memref_ataddr_res_5 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_6 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_7 [P2, P3, P0, P1] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0) }
dep_order for A_memref_ataddr_res_8 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_9 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_alloca_res_10 [P2, P3, P0, P1] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 and o0 > i0 and o0 < P3 and o1 >= 0 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and o1 > i1 and o1 < P2 and o3 >= 0 and 16o3 < P0) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0) }
dep_order for A_affine_load_res_11 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_for_res_12 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_13 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_14 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_vector_load_res_15 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_16 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_fmul_res_17 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_fadd_res_18 [P2, P3, P0, P1] -> {  }
dep_order for A_affine_load_res_19 [P2, P3, P0, P1] -> {  }
dep_order for A_llvm_bitcast_res_20 [P2, P3, P0, P1] -> {  }
dep_order for A_memref_ataddr_res_21 [P2, P3, P0, P1] -> {  }
tagged_reads [P2, P3, P0, P1] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> A_llvm_func_arg_0_3[]; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS1_affine_parallel[i0, i1, 0, i3] -> S15_affine_vector_load_Read0[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S13_affine_vector_load_Read0[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> A_llvm_func_arg_11_0[]; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> A_llvm_func_arg_1_2[]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> A_llvm_func_arg_10_1[] }
atagged_reads [P2, P3, P0, P1] -> { [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> A_llvm_func_arg_0_3[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> A_llvm_func_arg_10_1[]; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> A_llvm_func_arg_11_0[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> A_llvm_func_arg_1_2[]; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
reads [P2, P3, P0, P1] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]; RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }
async_reads [P0, P2, P3, P1] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }
tagged_may_writes [P2, P3, P0, P1] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }
atagged_may_writes [P2, P3, P0, P1] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
may_writes [P2, P3, P0, P1] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P2 > 0 and P3 > 0 and P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
tagged_must_writes [P0, P2, P3, P1] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
atagged_must_writes [P0, P2, P3, P1] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
must_writes [P0, P2, P3, P1] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
async_must_writes [P0, P2, P3, P1] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P2 > 0 and P3 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
tagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> S34_affine_yield1[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield0[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield2[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P3 and 0 <= i1 < P2 }
atagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P3 and 0 <= i1 < P2; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }
must_kills [P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P3 and 0 <= i1 < P2; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P3 and 0 <= i1 < P2; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }
live_in [P0, P2, P3, P1] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }
live_out [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P3 and 0 <= i1 < P2 }
independence [P2, P3, P0, P1] -> { S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : o2 < i2 or o2 > i2; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, i1, i2] : o0 < i0 or o0 > i0; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, i2] : o1 < i1 or o1 > i1; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, o2] : o2 > i2 or o2 < i2; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, i1, i2] : o0 > i0 or o0 < i0; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, i2] : o1 > i1 or o1 < i1; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, o2] : o2 < i2 or o2 > i2; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, i1, i2] : o0 < i0 or o0 > i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, i2] : o1 < i1 or o1 > i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, o2, i3] : o2 > i2 or o2 < i2; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[i0, i1, i2, o3] : o3 > i3 or o3 < i3; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, i2, i3] : o1 > i1 or o1 < i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, i1, i2, i3] : o0 > i0 or o0 < i0 }
dep_flow [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2 }
tagged_dep_flow [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0 }
atagged_dep_flow [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }
dep_false [P2, P3, P0, P1] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 <= -2 + P2 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, -1 + P2, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P2 > 0 and P0 > 0 and 0 <= i0 <= -2 + P3 and -16 + P0 <= 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 <= -2 + P2 and -16 + P0 <= 16i3 < P0; RS1_affine_parallel[i0, -1 + P2, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P2 > 0 and P0 > 0 and 0 <= i0 <= -2 + P3 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P3 and 0 <= i1 <= -2 + P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, -1 + P2, 0, i3] -> RS0_affine_parallel[1 + i0, 0, 0] : P2 > 0 and 0 <= i0 <= -2 + P3 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P3 and 0 <= i1 <= -2 + P2; RS3_affine_parallel[i0, -1 + P2, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P2 > 0 and 0 <= i0 <= -2 + P3; RS0_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 <= -2 + P2; RS0_affine_parallel[i0, -1 + P2, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P2 > 0 and P0 <= 0 and 0 <= i0 <= -2 + P3 }
dep_forced [P2, P3, P0, P1] -> {  }
dep_order [P2, P3, P0, P1] -> { RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 }
tagged_dep_order [P2, P3, P0, P1] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0 }
dep_async [P0, P2, P3, P1] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }
array_order [P2, P3, P0, P1] -> { RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2 and ((16i3 < P0 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P0)); RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2 and ((16i3 < P0 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P0)); RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 }
tagger [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> S6_arith_index_cast[]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> S4_arith_index_cast[]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> S7_arith_index_cast[]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> S5_arith_index_cast[] }
atagger [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> S4_arith_index_cast[]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> S7_arith_index_cast[]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> S6_arith_index_cast[]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> S5_arith_index_cast[]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]] -> RS3_affine_parallel[(i0), (i1), (i2)] }
schedule
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  array_expansion: [ none ]
  child:
    schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }]"
    permutable: 1
    array_expansion: [ none ]
    child:
      schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS3_affine_parallel[i0, i1, i2] -> [(0)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
          child:
            schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
            array_expansion: [ none ]
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
        - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Schedule constraints:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
validity: "[P2, P3, P0, P1] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2 }"
coincidence: "[P2, P3, P0, P1] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2 }"
condition: "[P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0 }"
conditional_validity: "[P2, P3, P0, P1] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P2 and i0 < o0 < P3 and 0 <= o1 < P2; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P3 and i1 >= 0 and i1 < o1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 and i0 < o0 < P3 and 0 <= o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P3 and i1 >= 0 and i3 >= 0 and 16i3 < P0 and i1 < o1 < P2 and o3 >= 0 and 16o3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and o3 > i3 and 16o3 < P0 }"
proximity: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2 }"
anti_proximity: "[P0, P2, P3, P1] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }"
live_range_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and -16 + P0 <= 16i3 < P0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P0 <= 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 <= -17 + P0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P0 > 0 and 0 <= i0 < P3 and 0 <= i1 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }"
live_range_maximal_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and (P0 >= 17 or P0 <= 0 or (0 < P0 <= 16)); [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0 }"
array_sizes: "[P0, P1, P2, P3] -> { A_memref_alloca_res_10[] -> [1024]; A_memref_alloca_res_4[] -> [1024]; A_memref_alloca_res_7[] -> [1024] }"
New Schedule:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  coincident: [ 1, 1 ]
  array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
  child:
    sequence:
    - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
      child:
        schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
        permutable: 1
        coincident: [ 1 ]
        array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
        child:
          set:
          - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
          - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
    - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Prefix schedule [P0, P1, P2, P3] -> {  }
[P0, P1, P2, P3] -> {  }
# YOU ARE HERE
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_4@0x1b827460
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> []; RS3_affine_parallel[i0, i1, i2] -> []; RS2_affine_parallel[i0, i1, i2, i3] -> []; RS0_affine_parallel[i0, i1, i2] -> [] }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  # YOU ARE HERE
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : (P0 > 0 and P2 > 0 and P3 > 0) or (P0 <= 0 and P2 > 0 and P3 > 0) or (P0 >= 17 and P2 > 0 and P3 > 0) or (P0 > 0 and P2 > 0 and P3 > 0) }
Deltas [P0, P1, P2, P3] -> { [] : (P0 > 0 and P2 > 0 and P3 > 0) or (P0 <= 0 and P2 > 0 and P3 > 0) or (P0 >= 17 and P2 > 0 and P3 > 0) or (P0 > 0 and P2 > 0 and P3 > 0) }
Array A_memref_alloca_res_4@0x1b827460
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P2 > 0 and P3 > 0 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P2 > 0 and P3 > 0 and P0 > 0 }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P2 > 0 and P3 > 0 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P2 > 0 and P3 > 0 and P0 > 0 }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    # YOU ARE HERE
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : (o0 = i0 and o1 = i1 and P0 > 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (o0 = i0 and o1 = i1 and P0 <= 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (o0 = i0 and o1 = i1 and P0 >= 17 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (o0 = i0 and o1 = i1 and P0 > 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : (i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and P0 <= 0 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and P0 >= 17 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0) }
Array A_memref_alloca_res_4@0x1b827460
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      # YOU ARE HERE
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P0 >= 17 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P0 > 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 1 and P0 > 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 1 and P0 <= 0 and i0 >= 0 and i0 < P2 and i1 >= 0 and i1 < P3) }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : (i0 = 0 and i1 = 0 and i2 = 0 and P0 >= 17 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 > 0 and P2 > 0 and P3 > 0) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 <= 0 and P2 > 0 and P3 > 0) }
Array A_memref_alloca_res_4@0x1b827460
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and P0 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P2 > 0 and P3 > 0 }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          # YOU ARE HERE
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : o0 = i0 and o1 = i1 and o2 = 21 + i2 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and i2 >= 0 and 16i2 < P0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P2 > 0 and P3 > 0 and P0 > 0 }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : o0 = i0 and o1 = i1 and o2 = 21 + i2 and i1 >= 0 and i1 < P3 and i0 >= 0 and i0 < P2 and i2 >= 0 and 16i2 < P0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P2 > 0 and P3 > 0 and P0 > 0 }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 0
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 0 and o3 = 1; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 21 + i3 and o3 = 0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = i3 and o3 = 0 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            # YOU ARE HERE
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 21 + i3 and o3 = 0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = i3 and o3 = 1 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                # YOU ARE HERE
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - # YOU ARE HERE
                  filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  child:
                    # YOU ARE HERE
                    leaf
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - # YOU ARE HERE
                  filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
                  child:
                    # YOU ARE HERE
                    leaf
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0 }
[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            set:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
              child:
                # YOU ARE HERE
                leaf
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          mark: "allocate_array"
          child:
            schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
            permutable: 1
            coincident: [ 1 ]
            array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
            child:
              set:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P3 and i1 >= 0 and i1 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          mark: "allocate_array"
          child:
            schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
            permutable: 1
            coincident: [ 1 ]
            array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
            child:
              set:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
        child:
          # YOU ARE HERE
          leaf
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1b7e6f30
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1b828590
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
New Schedule Prepared for GPU:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      mark: "allocate_array"
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
          child:
            mark: "allocate_array"
            child:
              schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
              permutable: 1
              coincident: [ 1 ]
              array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
              child:
                set:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
                  child:
                    sequence:
                    - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                    - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
                - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
New AST:
mark: grid_parallel@0x2
node:
  iterator:
    id: c0
  init:
    val: 0
  cond:
    op: lt
    args:
    - id: c0
    - id: P2@0x1b7edc20
  inc:
    val: 1
  body:
    iterator:
      id: c1
    init:
      val: 0
    cond:
      op: lt
      args:
      - id: c1
      - id: P3@0x1b7edd10
    inc:
      val: 1
    body:
      mark: allocate_array@0x1b9a3ea0
      node:
      - mark: allocate_array@0x1b9df4d0
        node:
        - 
          - user:
              op: call
              args:
              - id: RS0_affine_parallel@0x1b81fa40
              - id: c1
              - id: c0
              - val: 0
              - op: call
                args:
                - id: A_memref_alloca_res_4@0x1b827460
                - val: 0
                - val: 0
                - val: 0
          - iterator:
              id: c2
            init:
              val: 0
            cond:
              op: le
              args:
              - id: c2
              - op: min
                args:
                - val: 20
                - op: fdiv_q
                  args:
                  - op: sub
                    args:
                    - id: P0@0x1b7d5930
                    - val: 1
                  - val: 16
            inc:
              val: 1
            body:
              user:
                op: call
                args:
                - id: RS1_affine_parallel@0x1b7ea340
                - id: c1
                - id: c0
                - val: 0
                - id: c2
                - op: call
                  args:
                  - id: A_memref_alloca_res_7@0x1b828590
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: add
                      args:
                      - id: c2
                      - val: 21
                    - val: 22
                - op: call
                  args:
                  - id: A_memref_alloca_res_10@0x1b7e6f30
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: add
                      args:
                      - id: c2
                      - val: 21
                    - val: 22
        - iterator:
            id: c2
          init:
            val: 21
          cond:
            op: le
            args:
            - id: c2
            - op: add
              args:
              - op: fdiv_q
                args:
                - op: sub
                  args:
                  - id: P0@0x1b7d5930
                  - val: 1
                - val: 16
              - val: 21
          inc:
            val: 1
          body:
          - user:
              op: call
              args:
              - id: RS2_affine_parallel@0x1b800f40
              - id: c1
              - id: c0
              - val: 0
              - op: sub
                args:
                - id: c2
                - val: 21
              - op: call
                args:
                - id: A_memref_alloca_res_4@0x1b827460
                - val: 0
                - val: 0
                - val: 0
              - op: call
                args:
                - id: A_memref_alloca_res_7@0x1b828590
                - val: 0
                - val: 0
                - op: pdiv_r
                  args:
                  - op: sub
                    args:
                    - id: c2
                    - val: 1
                  - val: 22
              - op: call
                args:
                - id: A_memref_alloca_res_10@0x1b7e6f30
                - val: 0
                - val: 0
                - op: pdiv_r
                  args:
                  - op: sub
                    args:
                    - id: c2
                    - val: 1
                  - val: 22
          - guard:
              op: ge
              args:
              - id: P0@0x1b7d5930
              - op: add
                args:
                - op: mul
                  args:
                  - val: 16
                  - id: c2
                - val: 1
            then:
              user:
                op: call
                args:
                - id: RS1_affine_parallel@0x1b7ea340
                - id: c1
                - id: c0
                - val: 0
                - id: c2
                - op: call
                  args:
                  - id: A_memref_alloca_res_7@0x1b828590
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
                - op: call
                  args:
                  - id: A_memref_alloca_res_10@0x1b7e6f30
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
      - user:
          op: call
          args:
          - id: RS3_affine_parallel@0x1b800ca0
          - id: c1
          - id: c0
          - val: 0
          - op: call
            args:
            - id: A_memref_alloca_res_4@0x1b827460
            - val: 0
            - val: 0
New func:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %24 = "arith.index_cast"(%5) : (index) -> i64
    %25 = "arith.cmpi"(%24, %23) <{predicate = 5 : i64}> : (i64, i64) -> i1
    "scf.if"(%25) ({
      %119 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %120 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %121 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %122 = "arith.index_cast"(%121) : (i64) -> index
      %123 = "memref.subview"(%22, %122) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %124 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %125 = "arith.index_cast"(%124) : (i64) -> index
      %126 = "memref.subview"(%21, %125) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg33: index, %arg34: index, %arg35: index):
        %127 = "affine.vector_load"(%2, %arg34, %arg33, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%127, %123, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %128 = "affine.vector_load"(%1, %arg34, %arg33, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%128, %126, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    %26 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg30: index, %arg31: index, %arg32: index):
      "affine.store"(%0, %20, %arg30, %arg31, %arg32) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %27 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %28 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %29 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %30 = "arith.index_cast"(%5) : (index) -> i64
    %31 = "arith.subi"(%30, %29) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %32 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %33 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %34 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %35 = "arith.subi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %36 = "arith.addi"(%35, %33) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.cmpi"(%31, %34) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %38 = "arith.select"(%37, %36, %31) : (i1, i64, i64) -> i64
    %39 = "arith.divsi"(%38, %32) : (i64, i64) -> i64
    %40 = "arith.minsi"(%39, %28) : (i64, i64) -> i64
    %41 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %42 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %43 = "arith.addi"(%40, %42) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%27, %43, %41) ({
    ^bb0(%arg26: i64):
      %108 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %109 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %110 = "arith.subi"(%arg26, %109) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %111 = "arith.index_cast"(%110) : (i64) -> index
      %112 = "memref.subview"(%22, %111) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %113 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %114 = "arith.subi"(%arg26, %113) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %115 = "arith.index_cast"(%114) : (i64) -> index
      %116 = "memref.subview"(%21, %115) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg27: index, %arg28: index, %arg29: index):
        %117 = "affine.vector_load"(%2, %arg28, %arg27, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%117, %112, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %118 = "affine.vector_load"(%1, %arg28, %arg27, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%118, %116, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %44 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %45 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %46 = "arith.index_cast"(%5) : (index) -> i64
    %47 = "arith.subi"(%46, %45) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %48 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %49 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %50 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %51 = "arith.subi"(%47, %48) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %52 = "arith.addi"(%51, %49) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %53 = "arith.cmpi"(%47, %50) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %54 = "arith.select"(%53, %52, %47) : (i1, i64, i64) -> i64
    %55 = "arith.divsi"(%54, %48) : (i64, i64) -> i64
    %56 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %57 = "arith.addi"(%55, %56) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %58 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %59 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %60 = "arith.addi"(%57, %59) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%44, %60, %58) ({
    ^bb0(%arg17: i64):
      %64 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %65 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %66 = "arith.subi"(%arg17, %65) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %67 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %68 = "arith.subi"(%arg17, %67) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %70 = "arith.remui"(%68, %69) : (i64, i64) -> i64
      %71 = "arith.index_cast"(%70) : (i64) -> index
      %72 = "memref.subview"(%22, %71) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %73 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %74 = "arith.subi"(%arg17, %73) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %76 = "arith.remui"(%74, %75) : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%21, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg21: index, %arg22: index, %arg23: index):
        %100 = "affine.load"(%20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %101 = "affine.for"(%100) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg24: index, %arg25: f32):
          %102 = "affine.vector_load"(%72, %arg22, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %103 = "llvm.bitcast"(%102) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %104 = "affine.vector_load"(%78, %arg24, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %105 = "llvm.bitcast"(%104) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %106 = "llvm.fmul"(%103, %105) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %107 = "llvm.fadd"(%arg25, %106) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%107) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%101, %20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %79 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %80 = "arith.muli"(%79, %arg17) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %81 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %82 = "arith.addi"(%80, %81) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %83 = "arith.index_cast"(%5) : (index) -> i64
      %84 = "arith.cmpi"(%83, %82) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%84) ({
        %85 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %86 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %87 = "arith.subi"(%arg17, %86) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %89 = "arith.remui"(%87, %88) : (i64, i64) -> i64
        %90 = "arith.index_cast"(%89) : (i64) -> index
        %91 = "memref.subview"(%22, %90) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %92 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %93 = "arith.subi"(%arg17, %92) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %94 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %95 = "arith.remui"(%93, %94) : (i64, i64) -> i64
        %96 = "arith.index_cast"(%95) : (i64) -> index
        %97 = "memref.subview"(%21, %96) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          %98 = "affine.vector_load"(%2, %arg19, %arg18, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%98, %91, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %99 = "affine.vector_load"(%1, %arg19, %arg18, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%99, %97, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %61 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %62 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %63 = "llvm.bitcast"(%62) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%63, %3, %arg13, %arg14, %arg15, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
/scr/ivan/src/transformer-llvm-project/polly/lib/External/isl/isl_ctx.c:307: isl_ctx not freed as some objects still reference it
gpu-affine-opt: After opt:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %24 = "arith.index_cast"(%5) : (index) -> i64
    %25 = "arith.cmpi"(%24, %23) <{predicate = 5 : i64}> : (i64, i64) -> i1
    "scf.if"(%25) ({
      %119 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %120 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %121 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %122 = "arith.index_cast"(%121) : (i64) -> index
      %123 = "memref.subview"(%22, %122) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %124 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %125 = "arith.index_cast"(%124) : (i64) -> index
      %126 = "memref.subview"(%21, %125) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg33: index, %arg34: index, %arg35: index):
        %127 = "affine.vector_load"(%2, %arg34, %arg33, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%127, %123, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %128 = "affine.vector_load"(%1, %arg34, %arg33, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%128, %126, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    %26 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg30: index, %arg31: index, %arg32: index):
      "affine.store"(%0, %20, %arg30, %arg31, %arg32) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %27 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %28 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %29 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %30 = "arith.index_cast"(%5) : (index) -> i64
    %31 = "arith.subi"(%30, %29) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %32 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %33 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %34 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %35 = "arith.subi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %36 = "arith.addi"(%35, %33) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.cmpi"(%31, %34) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %38 = "arith.select"(%37, %36, %31) : (i1, i64, i64) -> i64
    %39 = "arith.divsi"(%38, %32) : (i64, i64) -> i64
    %40 = "arith.minsi"(%39, %28) : (i64, i64) -> i64
    %41 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %42 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %43 = "arith.addi"(%40, %42) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%27, %43, %41) ({
    ^bb0(%arg26: i64):
      %108 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %109 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %110 = "arith.subi"(%arg26, %109) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %111 = "arith.index_cast"(%110) : (i64) -> index
      %112 = "memref.subview"(%22, %111) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %113 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %114 = "arith.subi"(%arg26, %113) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %115 = "arith.index_cast"(%114) : (i64) -> index
      %116 = "memref.subview"(%21, %115) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg27: index, %arg28: index, %arg29: index):
        %117 = "affine.vector_load"(%2, %arg28, %arg27, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%117, %112, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %118 = "affine.vector_load"(%1, %arg28, %arg27, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%118, %116, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %44 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %45 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %46 = "arith.index_cast"(%5) : (index) -> i64
    %47 = "arith.subi"(%46, %45) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %48 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %49 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %50 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %51 = "arith.subi"(%47, %48) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %52 = "arith.addi"(%51, %49) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %53 = "arith.cmpi"(%47, %50) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %54 = "arith.select"(%53, %52, %47) : (i1, i64, i64) -> i64
    %55 = "arith.divsi"(%54, %48) : (i64, i64) -> i64
    %56 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %57 = "arith.addi"(%55, %56) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %58 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %59 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %60 = "arith.addi"(%57, %59) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%44, %60, %58) ({
    ^bb0(%arg17: i64):
      %64 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %65 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %66 = "arith.subi"(%arg17, %65) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %67 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %68 = "arith.subi"(%arg17, %67) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %70 = "arith.remui"(%68, %69) : (i64, i64) -> i64
      %71 = "arith.index_cast"(%70) : (i64) -> index
      %72 = "memref.subview"(%22, %71) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %73 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %74 = "arith.subi"(%arg17, %73) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %76 = "arith.remui"(%74, %75) : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%21, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg21: index, %arg22: index, %arg23: index):
        %100 = "affine.load"(%20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %101 = "affine.for"(%100) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg24: index, %arg25: f32):
          %102 = "affine.vector_load"(%72, %arg22, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %103 = "llvm.bitcast"(%102) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %104 = "affine.vector_load"(%78, %arg24, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %105 = "llvm.bitcast"(%104) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %106 = "llvm.fmul"(%103, %105) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %107 = "llvm.fadd"(%arg25, %106) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%107) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%101, %20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %79 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %80 = "arith.muli"(%79, %arg17) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %81 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %82 = "arith.addi"(%80, %81) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %83 = "arith.index_cast"(%5) : (index) -> i64
      %84 = "arith.cmpi"(%83, %82) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%84) ({
        %85 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %86 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %87 = "arith.subi"(%arg17, %86) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %89 = "arith.remui"(%87, %88) : (i64, i64) -> i64
        %90 = "arith.index_cast"(%89) : (i64) -> index
        %91 = "memref.subview"(%22, %90) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %92 = "arith.constant"() <{value = 1 : i64}> : () -> i64
        %93 = "arith.subi"(%arg17, %92) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %94 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %95 = "arith.remui"(%93, %94) : (i64, i64) -> i64
        %96 = "arith.index_cast"(%95) : (i64) -> index
        %97 = "memref.subview"(%21, %96) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          %98 = "affine.vector_load"(%2, %arg19, %arg18, %arg13, %arg12, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%98, %91, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %99 = "affine.vector_load"(%1, %arg19, %arg18, %arg12, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%99, %97, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %61 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %62 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %63 = "llvm.bitcast"(%62) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%63, %3, %arg13, %arg14, %arg15, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After shmem to alloca:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 21 : index}> : () -> index
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %10 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%15) : (index) -> i64
  %18 = "arith.index_cast"(%17) : (i64) -> index
  %19 = "arith.index_cast"(%16) : (index) -> i64
  %20 = "arith.index_cast"(%19) : (i64) -> index
  "scf.parallel"(%2, %2, %18, %20, %1, %1) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %21 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %22 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.index_cast"(%14) : (index) -> i64
    %25 = "arith.cmpi"(%24, %7) <{predicate = 5 : i64}> : (i64, i64) -> i1
    "scf.if"(%25) ({
      %84 = "memref.subview"(%23, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %85 = "memref.subview"(%22, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg33: index, %arg34: index, %arg35: index):
        %86 = "affine.vector_load"(%11, %arg34, %arg33, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%86, %84, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %87 = "affine.vector_load"(%10, %arg34, %arg33, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%87, %85, %arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }, {
    }) : (i1) -> ()
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg30: index, %arg31: index, %arg32: index):
      "affine.store"(%9, %21, %arg30, %arg31, %arg32) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %26 = "arith.index_cast"(%14) : (index) -> i64
    %27 = "arith.subi"(%26, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %28 = "arith.subi"(%27, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.addi"(%28, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %30 = "arith.cmpi"(%27, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %31 = "arith.select"(%30, %29, %27) : (i1, i64, i64) -> i64
    %32 = "arith.divsi"(%31, %4) : (i64, i64) -> i64
    %33 = "arith.minsi"(%32, %5) : (i64, i64) -> i64
    %34 = "arith.addi"(%33, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%7, %34, %7) ({
    ^bb0(%arg26: i64):
      %76 = "arith.subi"(%arg26, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %77 = "arith.index_cast"(%76) : (i64) -> index
      %78 = "memref.subview"(%23, %77) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %79 = "arith.subi"(%arg26, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %80 = "arith.index_cast"(%79) : (i64) -> index
      %81 = "memref.subview"(%22, %80) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg27: index, %arg28: index, %arg29: index):
        %82 = "affine.vector_load"(%11, %arg28, %arg27, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%82, %78, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %83 = "affine.vector_load"(%10, %arg28, %arg27, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%83, %81, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %35 = "arith.index_cast"(%14) : (index) -> i64
    %36 = "arith.subi"(%35, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %37 = "arith.subi"(%36, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %38 = "arith.addi"(%37, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %39 = "arith.cmpi"(%36, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %40 = "arith.select"(%39, %38, %36) : (i1, i64, i64) -> i64
    %41 = "arith.divsi"(%40, %4) : (i64, i64) -> i64
    %42 = "arith.addi"(%41, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %43 = "arith.addi"(%42, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%6, %43, %7) ({
    ^bb0(%arg17: i64):
      %46 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %47 = "arith.remui"(%46, %3) : (i64, i64) -> i64
      %48 = "arith.index_cast"(%47) : (i64) -> index
      %49 = "memref.subview"(%23, %48) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %50 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %51 = "arith.remui"(%50, %3) : (i64, i64) -> i64
      %52 = "arith.index_cast"(%51) : (i64) -> index
      %53 = "memref.subview"(%22, %52) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg21: index, %arg22: index, %arg23: index):
        %68 = "affine.load"(%21, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %69 = "affine.for"(%68) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg24: index, %arg25: f32):
          %70 = "affine.vector_load"(%49, %arg22, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %71 = "llvm.bitcast"(%70) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %72 = "affine.vector_load"(%53, %arg24, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %73 = "llvm.bitcast"(%72) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %74 = "llvm.fmul"(%71, %73) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %75 = "llvm.fadd"(%arg25, %74) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%75) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%69, %21, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %54 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %55 = "arith.addi"(%54, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %56 = "arith.index_cast"(%14) : (index) -> i64
      %57 = "arith.cmpi"(%56, %55) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%57) ({
        %58 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %59 = "arith.remui"(%58, %3) : (i64, i64) -> i64
        %60 = "arith.index_cast"(%59) : (i64) -> index
        %61 = "memref.subview"(%23, %60) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %62 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %63 = "arith.remui"(%62, %3) : (i64, i64) -> i64
        %64 = "arith.index_cast"(%63) : (i64) -> index
        %65 = "memref.subview"(%22, %64) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          %66 = "affine.vector_load"(%11, %arg19, %arg18, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%66, %61, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %67 = "affine.vector_load"(%10, %arg19, %arg18, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%67, %65, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %44 = "affine.load"(%21, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %45 = "llvm.bitcast"(%44) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%45, %12, %arg13, %arg14, %arg15, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After gpuify:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 21 : index}> : () -> index
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %10 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %13 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %15 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %17 = "arith.index_cast"(%15) : (index) -> i64
  %18 = "arith.index_cast"(%17) : (i64) -> index
  %19 = "arith.index_cast"(%16) : (index) -> i64
  %20 = "arith.index_cast"(%19) : (i64) -> index
  "scf.parallel"(%2, %2, %18, %20, %1, %1) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %21 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %22 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %23 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %24 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %25 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %26 = "arith.index_cast"(%14) : (index) -> i64
      %27 = "arith.cmpi"(%26, %7) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%27) ({
        %127 = "memref.subview"(%25, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %128 = "memref.subview"(%24, %0) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %129 = "affine.vector_load"(%11, %arg15, %arg14, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%129, %127, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %130 = "affine.vector_load"(%10, %arg15, %arg14, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%130, %128, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      %28 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %29 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %30 = "arith.index_cast"(%14) : (index) -> i64
      %31 = "arith.cmpi"(%30, %7) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "affine.store"(%9, %21, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %32 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %33 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %34 = "arith.index_cast"(%14) : (index) -> i64
      %35 = "arith.subi"(%34, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %36 = "arith.subi"(%35, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %37 = "arith.addi"(%36, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %38 = "arith.cmpi"(%35, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %39 = "arith.select"(%38, %37, %35) : (i1, i64, i64) -> i64
      %40 = "arith.divsi"(%39, %4) : (i64, i64) -> i64
      %41 = "arith.minsi"(%40, %5) : (i64, i64) -> i64
      %42 = "arith.addi"(%41, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%7, %42, %7) ({
      ^bb0(%arg20: i64):
        %119 = "arith.subi"(%arg20, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %120 = "arith.index_cast"(%119) : (i64) -> index
        %121 = "memref.subview"(%33, %120) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %122 = "arith.subi"(%arg20, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %123 = "arith.index_cast"(%122) : (i64) -> index
        %124 = "memref.subview"(%32, %123) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %125 = "affine.vector_load"(%11, %arg15, %arg14, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%125, %121, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %126 = "affine.vector_load"(%10, %arg15, %arg14, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%126, %124, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %43 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %44 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %45 = "arith.index_cast"(%14) : (index) -> i64
      %46 = "arith.subi"(%45, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %47 = "arith.subi"(%46, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %48 = "arith.addi"(%47, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %49 = "arith.cmpi"(%46, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %50 = "arith.select"(%49, %48, %46) : (i1, i64, i64) -> i64
      %51 = "arith.divsi"(%50, %4) : (i64, i64) -> i64
      %52 = "arith.addi"(%51, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %53 = "arith.addi"(%52, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%6, %53, %7) ({
      ^bb0(%arg17: i64):
        %89 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %90 = "arith.remui"(%89, %3) : (i64, i64) -> i64
        %91 = "arith.index_cast"(%90) : (i64) -> index
        %92 = "memref.subview"(%44, %91) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %93 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %94 = "arith.remui"(%93, %3) : (i64, i64) -> i64
        %95 = "arith.index_cast"(%94) : (i64) -> index
        %96 = "memref.subview"(%43, %95) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %97 = "affine.load"(%21, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %98 = "affine.for"(%97) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg18: index, %arg19: f32):
          %113 = "affine.vector_load"(%92, %arg15, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %114 = "llvm.bitcast"(%113) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %115 = "affine.vector_load"(%96, %arg18, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %116 = "llvm.bitcast"(%115) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %117 = "llvm.fmul"(%114, %116) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %118 = "llvm.fadd"(%arg19, %117) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%118) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%98, %21, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        %99 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %100 = "arith.addi"(%99, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %101 = "arith.index_cast"(%14) : (index) -> i64
        %102 = "arith.cmpi"(%101, %100) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%102) ({
          %103 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %104 = "arith.remui"(%103, %3) : (i64, i64) -> i64
          %105 = "arith.index_cast"(%104) : (i64) -> index
          %106 = "memref.subview"(%44, %105) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %107 = "arith.subi"(%arg17, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %108 = "arith.remui"(%107, %3) : (i64, i64) -> i64
          %109 = "arith.index_cast"(%108) : (i64) -> index
          %110 = "memref.subview"(%43, %109) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %111 = "affine.vector_load"(%11, %arg15, %arg14, %arg13, %arg12, %14) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%111, %106, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %112 = "affine.vector_load"(%10, %arg15, %arg14, %arg12, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%112, %110, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %54 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %55 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %56 = "arith.index_cast"(%14) : (index) -> i64
      %57 = "arith.subi"(%56, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %58 = "arith.subi"(%57, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %59 = "arith.addi"(%58, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %60 = "arith.cmpi"(%57, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %61 = "arith.select"(%60, %59, %57) : (i1, i64, i64) -> i64
      %62 = "arith.divsi"(%61, %4) : (i64, i64) -> i64
      %63 = "arith.addi"(%62, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.addi"(%63, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %65 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %66 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %67 = "arith.index_cast"(%14) : (index) -> i64
      %68 = "arith.cmpi"(%67, %7) <{predicate = 5 : i64}> : (i64, i64) -> i1
      %69 = "arith.index_cast"(%14) : (index) -> i64
      %70 = "arith.subi"(%69, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %71 = "arith.subi"(%70, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %72 = "arith.addi"(%71, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %73 = "arith.cmpi"(%70, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %74 = "arith.select"(%73, %72, %70) : (i1, i64, i64) -> i64
      %75 = "arith.divsi"(%74, %4) : (i64, i64) -> i64
      %76 = "arith.minsi"(%75, %5) : (i64, i64) -> i64
      %77 = "arith.addi"(%76, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %78 = "arith.index_cast"(%14) : (index) -> i64
      %79 = "arith.subi"(%78, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %80 = "arith.subi"(%79, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %81 = "arith.addi"(%80, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %82 = "arith.cmpi"(%79, %8) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %83 = "arith.select"(%82, %81, %79) : (i1, i64, i64) -> i64
      %84 = "arith.divsi"(%83, %4) : (i64, i64) -> i64
      %85 = "arith.addi"(%84, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %86 = "arith.addi"(%85, %7) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %87 = "affine.load"(%21, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %88 = "llvm.bitcast"(%87) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%88, %12, %arg13, %arg14, %arg15, %arg13, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() : () -> ()
    }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After expand subview:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %23 = "arith.index_cast"(%13) : (index) -> i64
      %24 = "arith.cmpi"(%23, %6) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%24) ({
        %93 = "memref.reinterpret_cast"(%22) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 21504>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %94 = "memref.reinterpret_cast"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 21504>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %95 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%95, %93, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %96 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%96, %94, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      "affine.store"(%8, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %25 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %26 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %27 = "arith.index_cast"(%13) : (index) -> i64
      %28 = "arith.subi"(%27, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %29 = "arith.subi"(%28, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %30 = "arith.addi"(%29, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %31 = "arith.cmpi"(%28, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %32 = "arith.select"(%31, %30, %28) : (i1, i64, i64) -> i64
      %33 = "arith.divsi"(%32, %3) : (i64, i64) -> i64
      %34 = "arith.minsi"(%33, %4) : (i64, i64) -> i64
      %35 = "arith.addi"(%34, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%6, %35, %6) ({
      ^bb0(%arg20: i64):
        %83 = "arith.subi"(%arg20, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %84 = "arith.index_cast"(%83) : (i64) -> index
        %85 = "affine.apply"(%84) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %86 = "memref.reinterpret_cast"(%26, %85) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %87 = "arith.subi"(%arg20, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.index_cast"(%87) : (i64) -> index
        %89 = "affine.apply"(%88) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %90 = "memref.reinterpret_cast"(%25, %89) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %91 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%91, %86, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %92 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%92, %90, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %36 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %37 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %38 = "arith.index_cast"(%13) : (index) -> i64
      %39 = "arith.subi"(%38, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %40 = "arith.subi"(%39, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %41 = "arith.addi"(%40, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %42 = "arith.cmpi"(%39, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %43 = "arith.select"(%42, %41, %39) : (i1, i64, i64) -> i64
      %44 = "arith.divsi"(%43, %3) : (i64, i64) -> i64
      %45 = "arith.addi"(%44, %5) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %46 = "arith.addi"(%45, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%5, %46, %6) ({
      ^bb0(%arg17: i64):
        %49 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %50 = "arith.remui"(%49, %2) : (i64, i64) -> i64
        %51 = "arith.index_cast"(%50) : (i64) -> index
        %52 = "affine.apply"(%51) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %53 = "memref.reinterpret_cast"(%37, %52) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %54 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %55 = "arith.remui"(%54, %2) : (i64, i64) -> i64
        %56 = "arith.index_cast"(%55) : (i64) -> index
        %57 = "affine.apply"(%56) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %58 = "memref.reinterpret_cast"(%36, %57) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %59 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %60 = "affine.for"(%59) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg18: index, %arg19: f32):
          %77 = "affine.vector_load"(%53, %arg15, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %78 = "llvm.bitcast"(%77) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %79 = "affine.vector_load"(%58, %arg18, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %80 = "llvm.bitcast"(%79) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %81 = "llvm.fmul"(%78, %80) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %82 = "llvm.fadd"(%arg19, %81) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%82) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%60, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        %61 = "arith.muli"(%arg17, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %62 = "arith.addi"(%61, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %63 = "arith.index_cast"(%13) : (index) -> i64
        %64 = "arith.cmpi"(%63, %62) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%64) ({
          %65 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %66 = "arith.remui"(%65, %2) : (i64, i64) -> i64
          %67 = "arith.index_cast"(%66) : (i64) -> index
          %68 = "affine.apply"(%67) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %69 = "memref.reinterpret_cast"(%37, %68) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %70 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %71 = "arith.remui"(%70, %2) : (i64, i64) -> i64
          %72 = "arith.index_cast"(%71) : (i64) -> index
          %73 = "affine.apply"(%72) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %74 = "memref.reinterpret_cast"(%36, %73) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %75 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%75, %69, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %76 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%76, %74, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %47 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %48 = "llvm.bitcast"(%47) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%48, %11, %arg13, %arg14, %arg15, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() : () -> ()
    }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After rar:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<f32>
      %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %23 = "arith.index_cast"(%13) : (index) -> i64
      %24 = "arith.cmpi"(%23, %6) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%24) ({
        %93 = "memref.reinterpret_cast"(%22) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 21504>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %94 = "memref.reinterpret_cast"(%21) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 21504>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %95 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%95, %93, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %96 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%96, %94, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "nvvm.barrier0"() : () -> ()
      "memref.store"(%8, %20) <{nontemporal = false}> : (f32, memref<f32>) -> ()
      "nvvm.barrier0"() : () -> ()
      %25 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %26 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %27 = "arith.index_cast"(%13) : (index) -> i64
      %28 = "arith.subi"(%27, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %29 = "arith.subi"(%28, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %30 = "arith.addi"(%29, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %31 = "arith.cmpi"(%28, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %32 = "arith.select"(%31, %30, %28) : (i1, i64, i64) -> i64
      %33 = "arith.divsi"(%32, %3) : (i64, i64) -> i64
      %34 = "arith.minsi"(%33, %4) : (i64, i64) -> i64
      %35 = "arith.addi"(%34, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%6, %35, %6) ({
      ^bb0(%arg20: i64):
        %83 = "arith.subi"(%arg20, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %84 = "arith.index_cast"(%83) : (i64) -> index
        %85 = "affine.apply"(%84) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %86 = "memref.reinterpret_cast"(%26, %85) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %87 = "arith.subi"(%arg20, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.index_cast"(%87) : (i64) -> index
        %89 = "affine.apply"(%88) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %90 = "memref.reinterpret_cast"(%25, %89) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %91 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%91, %86, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %92 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%92, %90, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %36 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %37 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %38 = "arith.index_cast"(%13) : (index) -> i64
      %39 = "arith.subi"(%38, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %40 = "arith.subi"(%39, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %41 = "arith.addi"(%40, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %42 = "arith.cmpi"(%39, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %43 = "arith.select"(%42, %41, %39) : (i1, i64, i64) -> i64
      %44 = "arith.divsi"(%43, %3) : (i64, i64) -> i64
      %45 = "arith.addi"(%44, %5) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %46 = "arith.addi"(%45, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%5, %46, %6) ({
      ^bb0(%arg17: i64):
        %49 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %50 = "arith.remui"(%49, %2) : (i64, i64) -> i64
        %51 = "arith.index_cast"(%50) : (i64) -> index
        %52 = "affine.apply"(%51) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %53 = "memref.reinterpret_cast"(%37, %52) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %54 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %55 = "arith.remui"(%54, %2) : (i64, i64) -> i64
        %56 = "arith.index_cast"(%55) : (i64) -> index
        %57 = "affine.apply"(%56) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %58 = "memref.reinterpret_cast"(%36, %57) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %59 = "memref.load"(%20) <{nontemporal = false}> : (memref<f32>) -> f32
        %60 = "affine.for"(%59) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg18: index, %arg19: f32):
          %77 = "affine.vector_load"(%53, %arg15, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %78 = "llvm.bitcast"(%77) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %79 = "affine.vector_load"(%58, %arg18, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %80 = "llvm.bitcast"(%79) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %81 = "llvm.fmul"(%78, %80) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %82 = "llvm.fadd"(%arg19, %81) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%82) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "memref.store"(%60, %20) <{nontemporal = false}> : (f32, memref<f32>) -> ()
        "nvvm.barrier0"() : () -> ()
        %61 = "arith.muli"(%arg17, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %62 = "arith.addi"(%61, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %63 = "arith.index_cast"(%13) : (index) -> i64
        %64 = "arith.cmpi"(%63, %62) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%64) ({
          %65 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %66 = "arith.remui"(%65, %2) : (i64, i64) -> i64
          %67 = "arith.index_cast"(%66) : (i64) -> index
          %68 = "affine.apply"(%67) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %69 = "memref.reinterpret_cast"(%37, %68) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %70 = "arith.subi"(%arg17, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
          %71 = "arith.remui"(%70, %2) : (i64, i64) -> i64
          %72 = "arith.index_cast"(%71) : (i64) -> index
          %73 = "affine.apply"(%72) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %74 = "memref.reinterpret_cast"(%36, %73) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %75 = "affine.vector_load"(%10, %arg15, %arg14, %arg13, %arg12, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%75, %69, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %76 = "affine.vector_load"(%9, %arg15, %arg14, %arg12, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%76, %74, %arg15, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %47 = "memref.load"(%20) <{nontemporal = false}> : (memref<f32>) -> f32
      %48 = "llvm.bitcast"(%47) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%48, %11, %arg13, %arg14, %arg15, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() : () -> ()
    }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After lower affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c22_i64 = arith.constant 22 : i64
  %c16_i64 = arith.constant 16 : i64
  %c20_i64 = arith.constant 20 : i64
  %c21_i64 = arith.constant 21 : i64
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %0 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 {polymer.stmt.name = "S4_arith_index_cast"} : i32 to index
  %5 = arith.index_cast %arg10 {polymer.stmt.name = "S5_arith_index_cast"} : i32 to index
  %6 = arith.index_cast %arg1 {polymer.stmt.name = "S6_arith_index_cast"} : i64 to index
  %7 = arith.index_cast %arg0 {polymer.stmt.name = "S7_arith_index_cast"} : i64 to index
  %8 = arith.index_cast %6 : index to i64
  %9 = arith.index_cast %8 : i64 to index
  %10 = arith.index_cast %7 : index to i64
  %11 = arith.index_cast %10 : i64 to index
  scf.parallel (%arg12, %arg13) = (%c0, %c0) to (%9, %11) step (%c1, %c1) {
    %c0_0 = arith.constant 0 : index
    %c16 = arith.constant 16 : index
    %c0_1 = arith.constant 0 : index
    %c16_2 = arith.constant 16 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %c1_5 = arith.constant 1 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    scf.parallel (%arg14, %arg15, %arg16) = (%c0_0, %c0_1, %c0_3) to (%c16, %c16_2, %c1_4) step (%c1_5, %c1_6, %c1_7) {
      %alloca = memref.alloca() : memref<f32>
      %12 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %13 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %14 = arith.index_cast %5 : index to i64
      %15 = arith.cmpi sge, %14, %c1_i64 : i64
      scf.if %15 {
        %reinterpret_cast = memref.reinterpret_cast %13 to offset: [21504], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %reinterpret_cast_11 = memref.reinterpret_cast %12 to offset: [21504], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %50 = arith.muli %arg15, %5 : index
        %c4_12 = arith.constant 4 : index
        %51 = arith.muli %50, %c4_12 : index
        %c4_13 = arith.constant 4 : index
        %52 = arith.muli %arg14, %c4_13 : index
        %53 = arith.addi %51, %52 : index
        %c64_14 = arith.constant 64 : index
        %54 = arith.muli %arg12, %c64_14 : index
        %55 = arith.addi %53, %54 : index
        %c16_15 = arith.constant 16 : index
        %56 = arith.muli %5, %c16_15 : index
        %57 = arith.muli %arg13, %56 : index
        %c4_16 = arith.constant 4 : index
        %58 = arith.muli %57, %c4_16 : index
        %59 = arith.addi %55, %58 : index
        %60 = vector.load %2[%59] {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : memref<?xi8>, vector<4xi8>
        %c64_17 = arith.constant 64 : index
        %61 = arith.muli %arg15, %c64_17 : index
        %c4_18 = arith.constant 4 : index
        %62 = arith.muli %arg14, %c4_18 : index
        %63 = arith.addi %61, %62 : index
        vector.store %60, %reinterpret_cast[%63] {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
        %64 = arith.muli %arg15, %4 : index
        %c4_19 = arith.constant 4 : index
        %65 = arith.muli %64, %c4_19 : index
        %c4_20 = arith.constant 4 : index
        %66 = arith.muli %arg14, %c4_20 : index
        %67 = arith.addi %65, %66 : index
        %c64_21 = arith.constant 64 : index
        %68 = arith.muli %arg13, %c64_21 : index
        %69 = arith.addi %67, %68 : index
        %c16_22 = arith.constant 16 : index
        %70 = arith.muli %4, %c16_22 : index
        %71 = arith.muli %arg12, %70 : index
        %c4_23 = arith.constant 4 : index
        %72 = arith.muli %71, %c4_23 : index
        %73 = arith.addi %69, %72 : index
        %74 = vector.load %1[%73] {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : memref<?xi8>, vector<4xi8>
        %c64_24 = arith.constant 64 : index
        %75 = arith.muli %arg15, %c64_24 : index
        %c4_25 = arith.constant 4 : index
        %76 = arith.muli %arg14, %c4_25 : index
        %77 = arith.addi %75, %76 : index
        vector.store %74, %reinterpret_cast_11[%77] {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
      }
      nvvm.barrier0
      memref.store %0, %alloca[] : memref<f32>
      nvvm.barrier0
      %16 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %17 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %18 = arith.index_cast %5 : index to i64
      %19 = arith.subi %18, %c1_i64 : i64
      %20 = arith.subi %19, %c16_i64 : i64
      %21 = arith.addi %20, %c1_i64 : i64
      %22 = arith.cmpi slt, %19, %c0_i64 : i64
      %23 = arith.select %22, %21, %19 : i64
      %24 = arith.divsi %23, %c16_i64 : i64
      %25 = arith.minsi %24, %c20_i64 : i64
      %26 = arith.addi %25, %c1_i64 : i64
      scf.for %arg17 = %c1_i64 to %26 step %c1_i64  : i64 {
        %50 = arith.subi %arg17, %c1_i64 : i64
        %51 = arith.index_cast %50 : i64 to index
        %c1024 = arith.constant 1024 : index
        %52 = arith.muli %51, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %17 to offset: [%52], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %53 = arith.subi %arg17, %c1_i64 : i64
        %54 = arith.index_cast %53 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %55 = arith.muli %54, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %16 to offset: [%55], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %56 = arith.muli %arg15, %5 : index
        %c4_13 = arith.constant 4 : index
        %57 = arith.muli %56, %c4_13 : index
        %c4_14 = arith.constant 4 : index
        %58 = arith.muli %arg14, %c4_14 : index
        %59 = arith.addi %57, %58 : index
        %c64_15 = arith.constant 64 : index
        %60 = arith.muli %arg12, %c64_15 : index
        %61 = arith.addi %59, %60 : index
        %c16_16 = arith.constant 16 : index
        %62 = arith.muli %5, %c16_16 : index
        %63 = arith.muli %arg13, %62 : index
        %c4_17 = arith.constant 4 : index
        %64 = arith.muli %63, %c4_17 : index
        %65 = arith.addi %61, %64 : index
        %66 = vector.load %2[%65] {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : memref<?xi8>, vector<4xi8>
        %c64_18 = arith.constant 64 : index
        %67 = arith.muli %arg15, %c64_18 : index
        %c4_19 = arith.constant 4 : index
        %68 = arith.muli %arg14, %c4_19 : index
        %69 = arith.addi %67, %68 : index
        vector.store %66, %reinterpret_cast[%69] {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
        %70 = arith.muli %arg15, %4 : index
        %c4_20 = arith.constant 4 : index
        %71 = arith.muli %70, %c4_20 : index
        %c4_21 = arith.constant 4 : index
        %72 = arith.muli %arg14, %c4_21 : index
        %73 = arith.addi %71, %72 : index
        %c64_22 = arith.constant 64 : index
        %74 = arith.muli %arg13, %c64_22 : index
        %75 = arith.addi %73, %74 : index
        %c16_23 = arith.constant 16 : index
        %76 = arith.muli %4, %c16_23 : index
        %77 = arith.muli %arg12, %76 : index
        %c4_24 = arith.constant 4 : index
        %78 = arith.muli %77, %c4_24 : index
        %79 = arith.addi %75, %78 : index
        %80 = vector.load %1[%79] {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : memref<?xi8>, vector<4xi8>
        %c64_25 = arith.constant 64 : index
        %81 = arith.muli %arg15, %c64_25 : index
        %c4_26 = arith.constant 4 : index
        %82 = arith.muli %arg14, %c4_26 : index
        %83 = arith.addi %81, %82 : index
        vector.store %80, %reinterpret_cast_12[%83] {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
      }
      nvvm.barrier0
      %27 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %28 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %29 = arith.index_cast %5 : index to i64
      %30 = arith.subi %29, %c1_i64 : i64
      %31 = arith.subi %30, %c16_i64 : i64
      %32 = arith.addi %31, %c1_i64 : i64
      %33 = arith.cmpi slt, %30, %c0_i64 : i64
      %34 = arith.select %33, %32, %30 : i64
      %35 = arith.divsi %34, %c16_i64 : i64
      %36 = arith.addi %35, %c21_i64 : i64
      %37 = arith.addi %36, %c1_i64 : i64
      scf.for %arg17 = %c21_i64 to %37 step %c1_i64  : i64 {
        %50 = arith.subi %arg17, %c1_i64 : i64
        %51 = arith.remui %50, %c22_i64 : i64
        %52 = arith.index_cast %51 : i64 to index
        %c1024 = arith.constant 1024 : index
        %53 = arith.muli %52, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %28 to offset: [%53], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %54 = arith.subi %arg17, %c1_i64 : i64
        %55 = arith.remui %54, %c22_i64 : i64
        %56 = arith.index_cast %55 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %57 = arith.muli %56, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %27 to offset: [%57], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %58 = memref.load %alloca[] : memref<f32>
        %c0_13 = arith.constant 0 : index
        %c16_14 = arith.constant 16 : index
        %c1_15 = arith.constant 1 : index
        %59 = scf.for %arg18 = %c0_13 to %c16_14 step %c1_15 iter_args(%arg19 = %58) -> (f32) {
          %c64_16 = arith.constant 64 : index
          %64 = arith.muli %arg15, %c64_16 : index
          %c4_17 = arith.constant 4 : index
          %65 = arith.muli %arg18, %c4_17 : index
          %66 = arith.addi %64, %65 : index
          %67 = vector.load %reinterpret_cast[%66] {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %68 = llvm.bitcast %67 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %c64_18 = arith.constant 64 : index
          %69 = arith.muli %arg18, %c64_18 : index
          %c4_19 = arith.constant 4 : index
          %70 = arith.muli %arg14, %c4_19 : index
          %71 = arith.addi %69, %70 : index
          %72 = vector.load %reinterpret_cast_12[%71] {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %73 = llvm.bitcast %72 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %74 = llvm.fmul %68, %73  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %75 = llvm.fadd %arg19, %74  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %75 : f32
        }
        memref.store %59, %alloca[] : memref<f32>
        nvvm.barrier0
        %60 = arith.muli %arg17, %c16_i64 : i64
        %61 = arith.addi %60, %c1_i64 : i64
        %62 = arith.index_cast %5 : index to i64
        %63 = arith.cmpi sge, %62, %61 : i64
        scf.if %63 {
          %64 = arith.subi %arg17, %c1_i64 : i64
          %65 = arith.remui %64, %c22_i64 : i64
          %66 = arith.index_cast %65 : i64 to index
          %c1024_16 = arith.constant 1024 : index
          %67 = arith.muli %66, %c1024_16 : index
          %reinterpret_cast_17 = memref.reinterpret_cast %28 to offset: [%67], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          %68 = arith.subi %arg17, %c1_i64 : i64
          %69 = arith.remui %68, %c22_i64 : i64
          %70 = arith.index_cast %69 : i64 to index
          %c1024_18 = arith.constant 1024 : index
          %71 = arith.muli %70, %c1024_18 : index
          %reinterpret_cast_19 = memref.reinterpret_cast %27 to offset: [%71], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          %72 = arith.muli %arg15, %5 : index
          %c4_20 = arith.constant 4 : index
          %73 = arith.muli %72, %c4_20 : index
          %c4_21 = arith.constant 4 : index
          %74 = arith.muli %arg14, %c4_21 : index
          %75 = arith.addi %73, %74 : index
          %c64_22 = arith.constant 64 : index
          %76 = arith.muli %arg12, %c64_22 : index
          %77 = arith.addi %75, %76 : index
          %c16_23 = arith.constant 16 : index
          %78 = arith.muli %5, %c16_23 : index
          %79 = arith.muli %arg13, %78 : index
          %c4_24 = arith.constant 4 : index
          %80 = arith.muli %79, %c4_24 : index
          %81 = arith.addi %77, %80 : index
          %82 = vector.load %2[%81] {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : memref<?xi8>, vector<4xi8>
          %c64_25 = arith.constant 64 : index
          %83 = arith.muli %arg15, %c64_25 : index
          %c4_26 = arith.constant 4 : index
          %84 = arith.muli %arg14, %c4_26 : index
          %85 = arith.addi %83, %84 : index
          vector.store %82, %reinterpret_cast_17[%85] {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %86 = arith.muli %arg15, %4 : index
          %c4_27 = arith.constant 4 : index
          %87 = arith.muli %86, %c4_27 : index
          %c4_28 = arith.constant 4 : index
          %88 = arith.muli %arg14, %c4_28 : index
          %89 = arith.addi %87, %88 : index
          %c64_29 = arith.constant 64 : index
          %90 = arith.muli %arg13, %c64_29 : index
          %91 = arith.addi %89, %90 : index
          %c16_30 = arith.constant 16 : index
          %92 = arith.muli %4, %c16_30 : index
          %93 = arith.muli %arg12, %92 : index
          %c4_31 = arith.constant 4 : index
          %94 = arith.muli %93, %c4_31 : index
          %95 = arith.addi %91, %94 : index
          %96 = vector.load %1[%95] {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : memref<?xi8>, vector<4xi8>
          %c64_32 = arith.constant 64 : index
          %97 = arith.muli %arg15, %c64_32 : index
          %c4_33 = arith.constant 4 : index
          %98 = arith.muli %arg14, %c4_33 : index
          %99 = arith.addi %97, %98 : index
          vector.store %96, %reinterpret_cast_19[%99] {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
        }
      }
      nvvm.barrier0
      %38 = memref.load %alloca[] : memref<f32>
      %39 = llvm.bitcast %38 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %c64 = arith.constant 64 : index
      %40 = arith.muli %arg13, %c64 : index
      %c4 = arith.constant 4 : index
      %41 = arith.muli %arg14, %c4 : index
      %42 = arith.addi %40, %41 : index
      %43 = arith.muli %arg15, %4 : index
      %c4_8 = arith.constant 4 : index
      %44 = arith.muli %43, %c4_8 : index
      %45 = arith.addi %42, %44 : index
      %c16_9 = arith.constant 16 : index
      %46 = arith.muli %4, %c16_9 : index
      %47 = arith.muli %arg13, %46 : index
      %c4_10 = arith.constant 4 : index
      %48 = arith.muli %47, %c4_10 : index
      %49 = arith.addi %45, %48 : index
      vector.store %39, %3[%49] {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : memref<?xi8>, vector<4xi8>
      scf.reduce 
    } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: After lower accesses:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(1 : index) : i64
  %1 = builtin.unrealized_conversion_cast %0 : i64 to index
  %2 = llvm.mlir.constant(0 : index) : i64
  %3 = builtin.unrealized_conversion_cast %2 : i64 to index
  %4 = llvm.mlir.constant(22 : i64) : i64
  %5 = llvm.mlir.constant(16 : i64) : i64
  %6 = llvm.mlir.constant(20 : i64) : i64
  %7 = llvm.mlir.constant(21 : i64) : i64
  %8 = llvm.mlir.constant(1 : i64) : i64
  %9 = llvm.mlir.constant(0 : i64) : i64
  %10 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %11 = llvm.sext %arg11 : i32 to i64
  %12 = llvm.sext %arg10 : i32 to i64
  %13 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %14 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  scf.parallel (%arg12, %arg13) = (%3, %3) to (%13, %14) step (%1, %1) {
    %15 = builtin.unrealized_conversion_cast %arg13 : index to i64
    %16 = builtin.unrealized_conversion_cast %arg12 : index to i64
    %17 = llvm.mlir.constant(0 : index) : i64
    %18 = builtin.unrealized_conversion_cast %17 : i64 to index
    %19 = llvm.mlir.constant(16 : index) : i64
    %20 = builtin.unrealized_conversion_cast %19 : i64 to index
    %21 = llvm.mlir.constant(0 : index) : i64
    %22 = builtin.unrealized_conversion_cast %21 : i64 to index
    %23 = llvm.mlir.constant(16 : index) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(0 : index) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(1 : index) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(1 : index) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(1 : index) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(1 : index) : i64
    %34 = builtin.unrealized_conversion_cast %33 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%18, %22, %26) to (%20, %24, %28) step (%30, %32, %34) {
      %35 = builtin.unrealized_conversion_cast %arg14 : index to i64
      %36 = builtin.unrealized_conversion_cast %arg15 : index to i64
      %37 = llvm.mlir.constant(1 : index) : i64
      %38 = llvm.alloca %37 x f32 : (i64) -> !llvm.ptr
      %39 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %40 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %41 = llvm.icmp "sge" %12, %8 : i64
      scf.if %41 {
        %82 = llvm.mlir.constant(21504 : i64) : i64
        %83 = llvm.getelementptr %40[%82] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %84 = llvm.mlir.constant(21504 : i64) : i64
        %85 = llvm.getelementptr %39[%84] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %86 = llvm.mul %36, %12 : i64
        %87 = llvm.mlir.constant(4 : index) : i64
        %88 = llvm.mul %86, %87 : i64
        %89 = llvm.mlir.constant(4 : index) : i64
        %90 = llvm.mul %35, %89 : i64
        %91 = llvm.add %88, %90 : i64
        %92 = llvm.mlir.constant(64 : index) : i64
        %93 = llvm.mul %16, %92 : i64
        %94 = llvm.add %91, %93 : i64
        %95 = llvm.mlir.constant(16 : index) : i64
        %96 = llvm.mul %12, %95 : i64
        %97 = llvm.mul %15, %96 : i64
        %98 = llvm.mlir.constant(4 : index) : i64
        %99 = llvm.mul %97, %98 : i64
        %100 = llvm.add %94, %99 : i64
        %101 = llvm.load %arg8 : !llvm.ptr -> f32
        %102 = llvm.bitcast %101 : f32 to vector<4xi8>
        %103 = llvm.mlir.constant(64 : index) : i64
        %104 = llvm.mul %36, %103 : i64
        %105 = llvm.mlir.constant(4 : index) : i64
        %106 = llvm.mul %35, %105 : i64
        %107 = llvm.add %104, %106 : i64
        %108 = llvm.bitcast %102 : vector<4xi8> to f32
        llvm.store %108, %83 : f32, !llvm.ptr<3>
        %109 = llvm.mul %36, %11 : i64
        %110 = llvm.mlir.constant(4 : index) : i64
        %111 = llvm.mul %109, %110 : i64
        %112 = llvm.mlir.constant(4 : index) : i64
        %113 = llvm.mul %35, %112 : i64
        %114 = llvm.add %111, %113 : i64
        %115 = llvm.mlir.constant(64 : index) : i64
        %116 = llvm.mul %15, %115 : i64
        %117 = llvm.add %114, %116 : i64
        %118 = llvm.mlir.constant(16 : index) : i64
        %119 = llvm.mul %11, %118 : i64
        %120 = llvm.mul %16, %119 : i64
        %121 = llvm.mlir.constant(4 : index) : i64
        %122 = llvm.mul %120, %121 : i64
        %123 = llvm.add %117, %122 : i64
        %124 = llvm.load %arg9 : !llvm.ptr -> f32
        %125 = llvm.bitcast %124 : f32 to vector<4xi8>
        %126 = llvm.mlir.constant(64 : index) : i64
        %127 = llvm.mul %36, %126 : i64
        %128 = llvm.mlir.constant(4 : index) : i64
        %129 = llvm.mul %35, %128 : i64
        %130 = llvm.add %127, %129 : i64
        %131 = llvm.bitcast %125 : vector<4xi8> to f32
        llvm.store %131, %85 : f32, !llvm.ptr<3>
      }
      nvvm.barrier0
      %42 = llvm.getelementptr %38[] : (!llvm.ptr) -> !llvm.ptr, f32
      llvm.store %10, %42 : f32, !llvm.ptr
      nvvm.barrier0
      %43 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %44 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %45 = llvm.sub %12, %8 : i64
      %46 = llvm.sub %45, %5 : i64
      %47 = llvm.add %46, %8 : i64
      %48 = llvm.icmp "slt" %45, %9 : i64
      %49 = llvm.select %48, %47, %45 : i1, i64
      %50 = llvm.sdiv %49, %5  : i64
      %51 = llvm.intr.smin(%50, %6)  : (i64, i64) -> i64
      %52 = llvm.add %51, %8 : i64
      scf.for %arg17 = %8 to %52 step %8  : i64 {
        %82 = llvm.sub %arg17, %8 : i64
        %83 = llvm.mlir.constant(1024 : index) : i64
        %84 = llvm.mul %82, %83 : i64
        %85 = llvm.getelementptr %44[%84] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %86 = llvm.sub %arg17, %8 : i64
        %87 = llvm.mlir.constant(1024 : index) : i64
        %88 = llvm.mul %86, %87 : i64
        %89 = llvm.getelementptr %43[%88] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %90 = llvm.mul %36, %12 : i64
        %91 = llvm.mlir.constant(4 : index) : i64
        %92 = llvm.mul %90, %91 : i64
        %93 = llvm.mlir.constant(4 : index) : i64
        %94 = llvm.mul %35, %93 : i64
        %95 = llvm.add %92, %94 : i64
        %96 = llvm.mlir.constant(64 : index) : i64
        %97 = llvm.mul %16, %96 : i64
        %98 = llvm.add %95, %97 : i64
        %99 = llvm.mlir.constant(16 : index) : i64
        %100 = llvm.mul %12, %99 : i64
        %101 = llvm.mul %15, %100 : i64
        %102 = llvm.mlir.constant(4 : index) : i64
        %103 = llvm.mul %101, %102 : i64
        %104 = llvm.add %98, %103 : i64
        %105 = llvm.load %arg8 : !llvm.ptr -> f32
        %106 = llvm.bitcast %105 : f32 to vector<4xi8>
        %107 = llvm.mlir.constant(64 : index) : i64
        %108 = llvm.mul %36, %107 : i64
        %109 = llvm.mlir.constant(4 : index) : i64
        %110 = llvm.mul %35, %109 : i64
        %111 = llvm.add %108, %110 : i64
        %112 = llvm.bitcast %106 : vector<4xi8> to f32
        llvm.store %112, %85 : f32, !llvm.ptr<3>
        %113 = llvm.mul %36, %11 : i64
        %114 = llvm.mlir.constant(4 : index) : i64
        %115 = llvm.mul %113, %114 : i64
        %116 = llvm.mlir.constant(4 : index) : i64
        %117 = llvm.mul %35, %116 : i64
        %118 = llvm.add %115, %117 : i64
        %119 = llvm.mlir.constant(64 : index) : i64
        %120 = llvm.mul %15, %119 : i64
        %121 = llvm.add %118, %120 : i64
        %122 = llvm.mlir.constant(16 : index) : i64
        %123 = llvm.mul %11, %122 : i64
        %124 = llvm.mul %16, %123 : i64
        %125 = llvm.mlir.constant(4 : index) : i64
        %126 = llvm.mul %124, %125 : i64
        %127 = llvm.add %121, %126 : i64
        %128 = llvm.load %arg9 : !llvm.ptr -> f32
        %129 = llvm.bitcast %128 : f32 to vector<4xi8>
        %130 = llvm.mlir.constant(64 : index) : i64
        %131 = llvm.mul %36, %130 : i64
        %132 = llvm.mlir.constant(4 : index) : i64
        %133 = llvm.mul %35, %132 : i64
        %134 = llvm.add %131, %133 : i64
        %135 = llvm.bitcast %129 : vector<4xi8> to f32
        llvm.store %135, %89 : f32, !llvm.ptr<3>
      }
      nvvm.barrier0
      %53 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %54 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %55 = llvm.sub %12, %8 : i64
      %56 = llvm.sub %55, %5 : i64
      %57 = llvm.add %56, %8 : i64
      %58 = llvm.icmp "slt" %55, %9 : i64
      %59 = llvm.select %58, %57, %55 : i1, i64
      %60 = llvm.sdiv %59, %5  : i64
      %61 = llvm.add %60, %7 : i64
      %62 = llvm.add %61, %8 : i64
      scf.for %arg17 = %7 to %62 step %8  : i64 {
        %82 = llvm.sub %arg17, %8 : i64
        %83 = llvm.urem %82, %4  : i64
        %84 = llvm.mlir.constant(1024 : index) : i64
        %85 = llvm.mul %83, %84 : i64
        %86 = llvm.getelementptr %54[%85] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %87 = llvm.sub %arg17, %8 : i64
        %88 = llvm.urem %87, %4  : i64
        %89 = llvm.mlir.constant(1024 : index) : i64
        %90 = llvm.mul %88, %89 : i64
        %91 = llvm.getelementptr %53[%90] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %92 = llvm.getelementptr %38[] : (!llvm.ptr) -> !llvm.ptr, f32
        %93 = llvm.load %92 : !llvm.ptr -> f32
        %94 = llvm.mlir.constant(0 : index) : i64
        %95 = builtin.unrealized_conversion_cast %94 : i64 to index
        %96 = llvm.mlir.constant(16 : index) : i64
        %97 = builtin.unrealized_conversion_cast %96 : i64 to index
        %98 = llvm.mlir.constant(1 : index) : i64
        %99 = builtin.unrealized_conversion_cast %98 : i64 to index
        %100 = scf.for %arg18 = %95 to %97 step %99 iter_args(%arg19 = %93) -> (f32) {
          %105 = builtin.unrealized_conversion_cast %arg18 : index to i64
          %106 = llvm.mlir.constant(64 : index) : i64
          %107 = llvm.mul %36, %106 : i64
          %108 = llvm.mlir.constant(4 : index) : i64
          %109 = llvm.mul %105, %108 : i64
          %110 = llvm.add %107, %109 : i64
          %111 = llvm.load %86 : !llvm.ptr<3> -> f32
          %112 = llvm.bitcast %111 : f32 to vector<4xi8>
          %113 = llvm.bitcast %112 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %114 = llvm.mlir.constant(64 : index) : i64
          %115 = llvm.mul %105, %114 : i64
          %116 = llvm.mlir.constant(4 : index) : i64
          %117 = llvm.mul %35, %116 : i64
          %118 = llvm.add %115, %117 : i64
          %119 = llvm.load %91 : !llvm.ptr<3> -> f32
          %120 = llvm.bitcast %119 : f32 to vector<4xi8>
          %121 = llvm.bitcast %120 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %122 = llvm.fmul %113, %121  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %123 = llvm.fadd %arg19, %122  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %123 : f32
        }
        %101 = llvm.getelementptr %38[] : (!llvm.ptr) -> !llvm.ptr, f32
        llvm.store %100, %101 : f32, !llvm.ptr
        nvvm.barrier0
        %102 = llvm.mul %arg17, %5 : i64
        %103 = llvm.add %102, %8 : i64
        %104 = llvm.icmp "sge" %12, %103 : i64
        scf.if %104 {
          %105 = llvm.sub %arg17, %8 : i64
          %106 = llvm.urem %105, %4  : i64
          %107 = llvm.mlir.constant(1024 : index) : i64
          %108 = llvm.mul %106, %107 : i64
          %109 = llvm.getelementptr %54[%108] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %110 = llvm.sub %arg17, %8 : i64
          %111 = llvm.urem %110, %4  : i64
          %112 = llvm.mlir.constant(1024 : index) : i64
          %113 = llvm.mul %111, %112 : i64
          %114 = llvm.getelementptr %53[%113] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %115 = llvm.mul %36, %12 : i64
          %116 = llvm.mlir.constant(4 : index) : i64
          %117 = llvm.mul %115, %116 : i64
          %118 = llvm.mlir.constant(4 : index) : i64
          %119 = llvm.mul %35, %118 : i64
          %120 = llvm.add %117, %119 : i64
          %121 = llvm.mlir.constant(64 : index) : i64
          %122 = llvm.mul %16, %121 : i64
          %123 = llvm.add %120, %122 : i64
          %124 = llvm.mlir.constant(16 : index) : i64
          %125 = llvm.mul %12, %124 : i64
          %126 = llvm.mul %15, %125 : i64
          %127 = llvm.mlir.constant(4 : index) : i64
          %128 = llvm.mul %126, %127 : i64
          %129 = llvm.add %123, %128 : i64
          %130 = llvm.load %arg8 : !llvm.ptr -> f32
          %131 = llvm.bitcast %130 : f32 to vector<4xi8>
          %132 = llvm.mlir.constant(64 : index) : i64
          %133 = llvm.mul %36, %132 : i64
          %134 = llvm.mlir.constant(4 : index) : i64
          %135 = llvm.mul %35, %134 : i64
          %136 = llvm.add %133, %135 : i64
          %137 = llvm.bitcast %131 : vector<4xi8> to f32
          llvm.store %137, %109 : f32, !llvm.ptr<3>
          %138 = llvm.mul %36, %11 : i64
          %139 = llvm.mlir.constant(4 : index) : i64
          %140 = llvm.mul %138, %139 : i64
          %141 = llvm.mlir.constant(4 : index) : i64
          %142 = llvm.mul %35, %141 : i64
          %143 = llvm.add %140, %142 : i64
          %144 = llvm.mlir.constant(64 : index) : i64
          %145 = llvm.mul %15, %144 : i64
          %146 = llvm.add %143, %145 : i64
          %147 = llvm.mlir.constant(16 : index) : i64
          %148 = llvm.mul %11, %147 : i64
          %149 = llvm.mul %16, %148 : i64
          %150 = llvm.mlir.constant(4 : index) : i64
          %151 = llvm.mul %149, %150 : i64
          %152 = llvm.add %146, %151 : i64
          %153 = llvm.load %arg9 : !llvm.ptr -> f32
          %154 = llvm.bitcast %153 : f32 to vector<4xi8>
          %155 = llvm.mlir.constant(64 : index) : i64
          %156 = llvm.mul %36, %155 : i64
          %157 = llvm.mlir.constant(4 : index) : i64
          %158 = llvm.mul %35, %157 : i64
          %159 = llvm.add %156, %158 : i64
          %160 = llvm.bitcast %154 : vector<4xi8> to f32
          llvm.store %160, %114 : f32, !llvm.ptr<3>
        }
      }
      nvvm.barrier0
      %63 = llvm.getelementptr %38[] : (!llvm.ptr) -> !llvm.ptr, f32
      %64 = llvm.load %63 : !llvm.ptr -> f32
      %65 = llvm.bitcast %64 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %66 = llvm.mlir.constant(64 : index) : i64
      %67 = llvm.mul %15, %66 : i64
      %68 = llvm.mlir.constant(4 : index) : i64
      %69 = llvm.mul %35, %68 : i64
      %70 = llvm.add %67, %69 : i64
      %71 = llvm.mul %36, %11 : i64
      %72 = llvm.mlir.constant(4 : index) : i64
      %73 = llvm.mul %71, %72 : i64
      %74 = llvm.add %70, %73 : i64
      %75 = llvm.mlir.constant(16 : index) : i64
      %76 = llvm.mul %11, %75 : i64
      %77 = llvm.mul %15, %76 : i64
      %78 = llvm.mlir.constant(4 : index) : i64
      %79 = llvm.mul %77, %78 : i64
      %80 = llvm.add %74, %79 : i64
      %81 = llvm.bitcast %65 : vector<4xi8> to f32
      llvm.store %81, %arg7 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(1024 : index) : i64
  %1 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
  %2 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
  %3 = llvm.mlir.constant(16 : index) : i64
  %4 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %5 = llvm.mlir.constant(0 : i64) : i64
  %6 = llvm.mlir.constant(1 : i64) : i64
  %7 = llvm.mlir.constant(21 : i64) : i64
  %8 = llvm.mlir.constant(20 : i64) : i64
  %9 = llvm.mlir.constant(16 : i64) : i64
  %10 = llvm.mlir.constant(22 : i64) : i64
  %11 = llvm.mlir.constant(0 : index) : i64
  %12 = llvm.mlir.constant(1 : index) : i64
  %13 = builtin.unrealized_conversion_cast %12 : i64 to index
  %14 = builtin.unrealized_conversion_cast %11 : i64 to index
  %15 = llvm.sext %arg10 : i32 to i64
  %16 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %17 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  scf.parallel (%arg12, %arg13) = (%14, %14) to (%16, %17) step (%13, %13) {
    %18 = builtin.unrealized_conversion_cast %3 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%14, %14, %14) to (%18, %18, %13) step (%13, %13, %13) {
      %19 = llvm.alloca %12 x f32 : (i64) -> !llvm.ptr
      %20 = llvm.icmp "sge" %15, %6 : i64
      scf.if %20 {
        %33 = llvm.getelementptr %1[21504] : (!llvm.ptr<3>) -> !llvm.ptr<3>, i8
        %34 = llvm.getelementptr %2[21504] : (!llvm.ptr<3>) -> !llvm.ptr<3>, i8
        %35 = llvm.load %arg8 : !llvm.ptr -> f32
        llvm.store %35, %33 : f32, !llvm.ptr<3>
        %36 = llvm.load %arg9 : !llvm.ptr -> f32
        llvm.store %36, %34 : f32, !llvm.ptr<3>
      }
      nvvm.barrier0
      %21 = llvm.getelementptr %19[] : (!llvm.ptr) -> !llvm.ptr, f32
      llvm.store %4, %21 : f32, !llvm.ptr
      nvvm.barrier0
      %22 = llvm.sub %15, %6 : i64
      %23 = llvm.sub %22, %9 : i64
      %24 = llvm.add %23, %6 : i64
      %25 = llvm.icmp "slt" %22, %5 : i64
      %26 = llvm.select %25, %24, %22 : i1, i64
      %27 = llvm.sdiv %26, %9  : i64
      %28 = llvm.intr.smin(%27, %8)  : (i64, i64) -> i64
      %29 = llvm.add %28, %6 : i64
      scf.for %arg17 = %6 to %29 step %6  : i64 {
        %33 = llvm.sub %arg17, %6 : i64
        %34 = llvm.mul %33, %0 : i64
        %35 = llvm.getelementptr %1[%34] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %36 = llvm.getelementptr %2[%34] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %37 = llvm.load %arg8 : !llvm.ptr -> f32
        llvm.store %37, %35 : f32, !llvm.ptr<3>
        %38 = llvm.load %arg9 : !llvm.ptr -> f32
        llvm.store %38, %36 : f32, !llvm.ptr<3>
      }
      nvvm.barrier0
      %30 = llvm.add %27, %7 : i64
      %31 = llvm.add %30, %6 : i64
      scf.for %arg17 = %7 to %31 step %6  : i64 {
        %33 = llvm.sub %arg17, %6 : i64
        %34 = llvm.urem %33, %10  : i64
        %35 = llvm.mul %34, %0 : i64
        %36 = llvm.getelementptr %1[%35] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %37 = llvm.getelementptr %2[%35] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %38 = llvm.load %21 : !llvm.ptr -> f32
        %39 = scf.for %arg18 = %14 to %18 step %13 iter_args(%arg19 = %38) -> (f32) {
          %43 = llvm.load %36 : !llvm.ptr<3> -> f32
          %44 = llvm.load %37 : !llvm.ptr<3> -> f32
          %45 = llvm.fmul %43, %44  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %46 = llvm.fadd %arg19, %45  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %46 : f32
        }
        llvm.store %39, %21 : f32, !llvm.ptr
        nvvm.barrier0
        %40 = llvm.mul %arg17, %9 : i64
        %41 = llvm.add %40, %6 : i64
        %42 = llvm.icmp "sge" %15, %41 : i64
        scf.if %42 {
          %43 = llvm.load %arg8 : !llvm.ptr -> f32
          llvm.store %43, %36 : f32, !llvm.ptr<3>
          %44 = llvm.load %arg9 : !llvm.ptr -> f32
          llvm.store %44, %37 : f32, !llvm.ptr<3>
        }
      }
      nvvm.barrier0
      %32 = llvm.load %21 : !llvm.ptr -> f32
      llvm.store %32, %arg7 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
module attributes {dlti.dl_spec = #dlti.dl_spec<#dlti.dl_entry<f80, dense<128> : vector<2xi64>>, #dlti.dl_entry<i128, dense<128> : vector<2xi64>>, #dlti.dl_entry<!llvm.ptr<272>, dense<64> : vector<4xi64>>, #dlti.dl_entry<i64, dense<64> : vector<2xi64>>, #dlti.dl_entry<!llvm.ptr<270>, dense<32> : vector<4xi64>>, #dlti.dl_entry<f128, dense<128> : vector<2xi64>>, #dlti.dl_entry<!llvm.ptr<271>, dense<32> : vector<4xi64>>, #dlti.dl_entry<f16, dense<16> : vector<2xi64>>, #dlti.dl_entry<f64, dense<64> : vector<2xi64>>, #dlti.dl_entry<i16, dense<16> : vector<2xi64>>, #dlti.dl_entry<i32, dense<32> : vector<2xi64>>, #dlti.dl_entry<i1, dense<8> : vector<2xi64>>, #dlti.dl_entry<i8, dense<8> : vector<2xi64>>, #dlti.dl_entry<!llvm.ptr, dense<64> : vector<4xi64>>, #dlti.dl_entry<"dlti.stack_alignment", 128 : i64>, #dlti.dl_entry<"dlti.endianness", "little">>, gpu.container_module} {
  gpu.module @__mlir_gpu_module [#nvvm.target<chip = "sm_80">]  {
    llvm.mlir.global external @shared_mem_1() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.mlir.global external @shared_mem_0() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.comdat @__llvm_global_comdat {
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi32EEvPfS0_S0_ii any
    }
    llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
      %0 = llvm.mlir.constant(1024 : index) : i64
      %1 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %2 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %3 = llvm.mlir.constant(16 : index) : i64
      %4 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
      %5 = llvm.mlir.constant(0 : i64) : i64
      %6 = llvm.mlir.constant(1 : i64) : i64
      %7 = llvm.mlir.constant(21 : i64) : i64
      %8 = llvm.mlir.constant(20 : i64) : i64
      %9 = llvm.mlir.constant(16 : i64) : i64
      %10 = llvm.mlir.constant(22 : i64) : i64
      %11 = llvm.mlir.constant(0 : index) : i64
      %12 = llvm.mlir.constant(1 : index) : i64
      %13 = builtin.unrealized_conversion_cast %12 : i64 to index
      %14 = builtin.unrealized_conversion_cast %11 : i64 to index
      %15 = llvm.sext %arg10 : i32 to i64
      %16 = builtin.unrealized_conversion_cast %arg1 : i64 to index
      %17 = builtin.unrealized_conversion_cast %arg0 : i64 to index
      scf.parallel (%arg12, %arg13) = (%14, %14) to (%16, %17) step (%13, %13) {
        %18 = builtin.unrealized_conversion_cast %3 : i64 to index
        scf.parallel (%arg14, %arg15, %arg16) = (%14, %14, %14) to (%18, %18, %13) step (%13, %13, %13) {
          %19 = llvm.alloca %12 x f32 : (i64) -> !llvm.ptr
          %20 = llvm.icmp "sge" %15, %6 : i64
          scf.if %20 {
            %33 = llvm.getelementptr %1[21504] : (!llvm.ptr<3>) -> !llvm.ptr<3>, i8
            %34 = llvm.getelementptr %2[21504] : (!llvm.ptr<3>) -> !llvm.ptr<3>, i8
            %35 = llvm.load %arg8 : !llvm.ptr -> f32
            llvm.store %35, %33 : f32, !llvm.ptr<3>
            %36 = llvm.load %arg9 : !llvm.ptr -> f32
            llvm.store %36, %34 : f32, !llvm.ptr<3>
          }
          nvvm.barrier0
          %21 = llvm.getelementptr %19[] : (!llvm.ptr) -> !llvm.ptr, f32
          llvm.store %4, %21 : f32, !llvm.ptr
          nvvm.barrier0
          %22 = llvm.sub %15, %6 : i64
          %23 = llvm.sub %22, %9 : i64
          %24 = llvm.add %23, %6 : i64
          %25 = llvm.icmp "slt" %22, %5 : i64
          %26 = llvm.select %25, %24, %22 : i1, i64
          %27 = llvm.sdiv %26, %9  : i64
          %28 = llvm.intr.smin(%27, %8)  : (i64, i64) -> i64
          %29 = llvm.add %28, %6 : i64
          scf.for %arg17 = %6 to %29 step %6  : i64 {
            %33 = llvm.sub %arg17, %6 : i64
            %34 = llvm.mul %33, %0 : i64
            %35 = llvm.getelementptr %1[%34] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %36 = llvm.getelementptr %2[%34] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %37 = llvm.load %arg8 : !llvm.ptr -> f32
            llvm.store %37, %35 : f32, !llvm.ptr<3>
            %38 = llvm.load %arg9 : !llvm.ptr -> f32
            llvm.store %38, %36 : f32, !llvm.ptr<3>
          }
          nvvm.barrier0
          %30 = llvm.add %27, %7 : i64
          %31 = llvm.add %30, %6 : i64
          scf.for %arg17 = %7 to %31 step %6  : i64 {
            %33 = llvm.sub %arg17, %6 : i64
            %34 = llvm.urem %33, %10  : i64
            %35 = llvm.mul %34, %0 : i64
            %36 = llvm.getelementptr %1[%35] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %37 = llvm.getelementptr %2[%35] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %38 = llvm.load %21 : !llvm.ptr -> f32
            %39 = scf.for %arg18 = %14 to %18 step %13 iter_args(%arg19 = %38) -> (f32) {
              %43 = llvm.load %36 : !llvm.ptr<3> -> f32
              %44 = llvm.load %37 : !llvm.ptr<3> -> f32
              %45 = llvm.fmul %43, %44  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
              %46 = llvm.fadd %arg19, %45  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
              scf.yield %46 : f32
            }
            llvm.store %39, %21 : f32, !llvm.ptr
            nvvm.barrier0
            %40 = llvm.mul %arg17, %9 : i64
            %41 = llvm.add %40, %6 : i64
            %42 = llvm.icmp "sge" %15, %41 : i64
            scf.if %42 {
              %43 = llvm.load %arg8 : !llvm.ptr -> f32
              llvm.store %43, %36 : f32, !llvm.ptr<3>
              %44 = llvm.load %arg9 : !llvm.ptr -> f32
              llvm.store %44, %37 : f32, !llvm.ptr<3>
            }
          }
          nvvm.barrier0
          %32 = llvm.load %21 : !llvm.ptr -> f32
          llvm.store %32, %arg7 : f32, !llvm.ptr
          scf.reduce 
        } {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"}
        scf.reduce 
      } {gpu.par.grid}
      llvm.return {polymer.stmt.name = "S35_llvm_return"}
    }
  }
}

Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S0_llvm_mlir_constant[] }
Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P3 and 0 <= i1 < P2 and i3 >= 0 and 16i3 < P0; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P3 and 0 <= i1 < P2; S0_llvm_mlir_constant[] }
