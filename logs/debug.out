gpu-affine-opt: Before opt:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.addi %12, %arg10 : i32
      %14 = arith.shli %7, %c4_i32 : i32
      %15 = arith.shli %arg11, %c4_i32 : i32
      %16 = arith.muli %10, %arg10 : i32
      %17 = arith.addi %16, %9 : i32
      %18 = arith.extui %10 : i32 to i64
      %19 = arith.extui %9 : i32 to i64
      %20 = llvm.getelementptr inbounds %5[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %21 = arith.muli %10, %arg11 : i32
      %22 = arith.addi %21, %9 : i32
      %23 = llvm.getelementptr inbounds %6[0, %18, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %24:2 = scf.for %arg18 = %12 to %13 step %c16_i32 iter_args(%arg19 = %14, %arg20 = %0) -> (i32, f32)  : i32 {
        %31 = arith.addi %17, %arg18 : i32
        %32 = arith.extsi %31 : i32 to i64
        %33 = llvm.getelementptr inbounds %arg8[%32] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %34 = llvm.load %33 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %34, %20 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %35 = arith.addi %22, %arg19 : i32
        %36 = arith.extsi %35 : i32 to i64
        %37 = llvm.getelementptr inbounds %arg9[%36] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %38 = llvm.load %37 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %38, %23 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %39 = scf.for %arg21 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg22 = %arg20) -> (f32)  : i32 {
          %41 = arith.extui %arg21 : i32 to i64
          %42 = llvm.getelementptr inbounds %5[0, %18, %41] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %43 = llvm.load %42 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %44 = llvm.getelementptr inbounds %6[0, %41, %19] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %45 = llvm.load %44 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %46 = llvm.fmul %43, %45  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %47 = llvm.fadd %arg22, %46  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %47 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %40 = arith.addi %arg19, %15 : i32
        scf.yield %40, %39 : i32, f32
      }
      %25 = arith.muli %15, %8 : i32
      %26 = arith.addi %14, %9 : i32
      %27 = arith.addi %26, %21 : i32
      %28 = arith.addi %27, %25 : i32
      %29 = arith.extsi %28 : i32 to i64
      %30 = llvm.getelementptr inbounds %arg7[%29] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %24#1, %30 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Removed IVs:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c4_i32 = arith.constant 4 : i32
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %c16_i32 = arith.constant 16 : i32
  %c0_i32 = arith.constant 0 : i32
  %c1_i32 = arith.constant 1 : i32
  %1 = arith.index_cast %arg1 : i64 to index
  %2 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%2), symbol(%1), 1) {
    %3 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    %4 = llvm.alloca %c1_i32 x !llvm.array<16 x array<16 x f32>> : (i32) -> !llvm.ptr<3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %5 = llvm.addrspacecast %3 : !llvm.ptr<3> to !llvm.ptr
      %6 = llvm.addrspacecast %4 : !llvm.ptr<3> to !llvm.ptr
      %7 = arith.index_cast %arg12 : index to i32
      %8 = arith.index_cast %arg13 : index to i32
      %9 = arith.index_cast %arg15 : index to i32
      %10 = arith.index_cast %arg16 : index to i32
      %11 = arith.shli %arg10, %c4_i32 : i32
      %12 = arith.muli %11, %8 : i32
      %13 = arith.shli %7, %c4_i32 : i32
      %14 = arith.shli %arg11, %c4_i32 : i32
      %15 = arith.muli %10, %arg10 : i32
      %16 = arith.addi %15, %9 : i32
      %17 = arith.extui %10 : i32 to i64
      %18 = arith.extui %9 : i32 to i64
      %19 = llvm.getelementptr inbounds %5[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %20 = arith.muli %10, %arg11 : i32
      %21 = arith.addi %20, %9 : i32
      %22 = llvm.getelementptr inbounds %6[0, %17, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
      %23 = arith.subi %arg10, %c1_i32 : i32
      %24 = arith.divui %23, %c16_i32 : i32
      %25 = arith.addi %24, %c1_i32 : i32
      %26 = scf.for %arg18 = %c0_i32 to %25 step %c1_i32 iter_args(%arg19 = %0) -> (f32)  : i32 {
        %33 = arith.muli %arg18, %14 : i32
        %34 = arith.addi %33, %13 : i32
        %35 = arith.muli %arg18, %c16_i32 : i32
        %36 = arith.addi %12, %35 : i32
        %37 = arith.addi %16, %36 : i32
        %38 = arith.extsi %37 : i32 to i64
        %39 = llvm.getelementptr inbounds %arg8[%38] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %40 = llvm.load %39 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %40, %19 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        %41 = arith.addi %21, %34 : i32
        %42 = arith.extsi %41 : i32 to i64
        %43 = llvm.getelementptr inbounds %arg9[%42] : (!llvm.ptr, i64) -> !llvm.ptr, f32
        %44 = llvm.load %43 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
        llvm.store %44, %22 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %45 = scf.for %arg20 = %c0_i32 to %c16_i32 step %c1_i32 iter_args(%arg21 = %arg19) -> (f32)  : i32 {
          %46 = arith.extui %arg20 : i32 to i64
          %47 = llvm.getelementptr inbounds %5[0, %17, %46] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %48 = llvm.load %47 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %49 = llvm.getelementptr inbounds %6[0, %46, %18] : (!llvm.ptr, i64, i64) -> !llvm.ptr, !llvm.array<16 x array<16 x f32>>
          %50 = llvm.load %49 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : !llvm.ptr -> f32
          %51 = llvm.fmul %48, %50  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %52 = llvm.fadd %arg21, %51  {fastmathFlags = #llvm.fastmath<contract>} : f32
          scf.yield %52 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        scf.yield %45 : f32
      }
      %27 = arith.muli %14, %8 : i32
      %28 = arith.addi %13, %9 : i32
      %29 = arith.addi %28, %20 : i32
      %30 = arith.addi %29, %27 : i32
      %31 = arith.extsi %30 : i32 to i64
      %32 = llvm.getelementptr inbounds %arg7[%31] : (!llvm.ptr, i64) -> !llvm.ptr, f32
      llvm.store %26, %32 {alignment = 4 : i64, tbaa = [#llvm.tbaa_tag<base_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, access_type = <id = "float", members = {<#llvm.tbaa_type_desc<id = "omnipotent char", members = {<#llvm.tbaa_root<id = "Simple C++ TBAA">, 0>}>, 0>}>, offset = 0>]} : f32, !llvm.ptr
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: To Affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.for %arg18 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] iter_args(%arg19 = %0) -> (f32) {
        %15 = affine.vector_load %2[(%arg16 * symbol(%7)) * 4 + %arg15 * 4 + %arg18 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %15, %alloca[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %16 = affine.vector_load %1[(%arg16 * symbol(%5)) * 4 + %arg15 * 4 + %arg12 * 64 + (%arg18 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %16, %alloca_0[%arg16 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        %17 = affine.for %arg20 = 0 to 16 iter_args(%arg21 = %arg19) -> (f32) {
          %18 = affine.vector_load %alloca[%arg16 * 64 + %arg20 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %19 = llvm.bitcast %18 : vector<4xi8> to f32
          %20 = affine.vector_load %alloca_0[%arg20 * 64 + %arg15 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %21 = llvm.bitcast %20 : vector<4xi8> to f32
          %22 = llvm.fmul %19, %21  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %23 = llvm.fadd %arg21, %22  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %23 : f32
        }
        "affine.barrier"(%arg15, %arg16, %arg17) : (index, index, index) -> ()
        affine.yield %17 : f32
      }
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Distributed:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c16 = arith.constant 16 : index
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg11 : i32 to index
  %6 = arith.index_cast %arg10 : i32 to index
  %7 = arith.index_cast %arg10 : i32 to index
  %8 = arith.index_cast %arg11 : i32 to index
  %9 = arith.index_cast %arg11 : i32 to index
  %10 = arith.index_cast %arg10 : i32 to index
  %11 = arith.index_cast %arg1 : i64 to index
  %12 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%12), symbol(%11), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca(%c16, %c16, %c1) : memref<?x?x?xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%10] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.vector_load %2[(%arg17 * symbol(%7)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%6) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %13, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %14 = affine.vector_load %1[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %14, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %13 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
        %14 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %13) -> (f32) {
          %15 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %16 = llvm.bitcast %15 : vector<4xi8> to f32
          %17 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %18 = llvm.bitcast %17 : vector<4xi8> to f32
          %19 = llvm.fmul %16, %18  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %20 = llvm.fadd %arg20, %19  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %20 : f32
        }
        affine.store %14, %alloca_1[%arg16, %arg17, %arg18] : memref<?x?x?xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %13 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<?x?x?xf32, 16>
      %14 = llvm.bitcast %13 : f32 to vector<4xi8>
      affine.vector_store %14, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%9)) * 4 + (%arg13 * (symbol(%8) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(0.000000e+00 : f32) : f32
  %1 = "memref.ataddr"(%arg9) : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 : i32 to index
  %5 = arith.index_cast %arg10 : i32 to index
  %6 = arith.index_cast %arg1 : i64 to index
  %7 = arith.index_cast %arg0 : i64 to index
  affine.parallel (%arg12, %arg13, %arg14) = (0, 0, 0) to (symbol(%7), symbol(%6), 1) {
    %alloca = memref.alloca() : memref<1024xi8, 3>
    %alloca_0 = memref.alloca() : memref<1024xi8, 3>
    %alloca_1 = memref.alloca() : memref<16x16x1xf32, 16>
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      affine.store %0, %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
    } {gpu.par.block}
    affine.for %arg15 = 0 to affine_map<()[s0] -> ((s0 - 1) floordiv 16 + 1)>()[%5] {
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.vector_load %2[(%arg17 * symbol(%5)) * 4 + %arg16 * 4 + %arg15 * 64 + (%arg13 * (symbol(%5) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %8, %alloca[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
        %9 = affine.vector_load %1[(%arg17 * symbol(%4)) * 4 + %arg16 * 4 + %arg12 * 64 + (%arg15 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
        affine.vector_store %9, %alloca_0[%arg17 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
      } {gpu.par.block}
      affine.parallel (%arg16, %arg17, %arg18) = (0, 0, 0) to (16, 16, 1) {
        %8 = affine.load %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
        %9 = affine.for %arg19 = 0 to 16 iter_args(%arg20 = %8) -> (f32) {
          %10 = affine.vector_load %alloca[%arg17 * 64 + %arg19 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %11 = llvm.bitcast %10 : vector<4xi8> to f32
          %12 = affine.vector_load %alloca_0[%arg19 * 64 + %arg16 * 4] {polymer.access.type = f32} : memref<1024xi8, 3>, vector<4xi8>
          %13 = llvm.bitcast %12 : vector<4xi8> to f32
          %14 = llvm.fmul %11, %13  {fastmathFlags = #llvm.fastmath<contract>} : f32
          %15 = llvm.fadd %arg20, %14  {fastmathFlags = #llvm.fastmath<contract>} : f32
          affine.yield %15 : f32
        }
        affine.store %9, %alloca_1[%arg16, %arg17, %arg18] : memref<16x16x1xf32, 16>
      } {gpu.par.block}
    }
    affine.parallel (%arg15, %arg16, %arg17) = (0, 0, 0) to (16, 16, 1) {
      %8 = affine.load %alloca_1[%arg15, %arg16, %arg17] : memref<16x16x1xf32, 16>
      %9 = llvm.bitcast %8 : f32 to vector<4xi8>
      affine.vector_store %9, %3[%arg12 * 64 + %arg15 * 4 + (%arg16 * symbol(%4)) * 4 + (%arg13 * (symbol(%4) * 16)) * 4] {polymer.access.type = f32} : memref<?xi8>, vector<4xi8>
    } {gpu.par.block}
  } {gpu.par.grid}
  llvm.return
}
Schedule:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S10_memref_alloca[i0, i1, i2]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S30_affine_load[i0, i1, i2, i3, i4, i5]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S34_affine_yield[i0, i1, i2]; S8_memref_alloca[i0, i1, i2]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S11_affine_store[i0, i1, i2, i3, i4, i5]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S33_affine_yield[i0, i1, i2, i3, i4, i5]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S12_affine_yield[i0, i1, i2, i3, i4, i5]; S9_memref_alloca[i0, i1, i2]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L16_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i0)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i0)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i0)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i0)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L15_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i1)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i1)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i1)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i1)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L14_affine_parallel[{ S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i2)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i2)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i2)]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)]; S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i2)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5]; S11_affine_store[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L0_affine_parallel[{ S12_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S11_affine_store[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S12_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S29_affine_yield[i0, i1, i2, i3]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L10_affine_for[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i3)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6]; S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L5_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L4_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S27_affine_store[i0, i1, i2, i3, i4, i5, i6]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                    child:
                      schedule: "[P0, P1, P2, P3] -> L9_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i4)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i4)] }]"
                      permutable: 1
                      array_expansion: [ none ]
                      child:
                        schedule: "[P0, P1, P2, P3] -> L8_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i5)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i5)] }]"
                        permutable: 1
                        array_expansion: [ none ]
                        child:
                          schedule: "[P0, P1, P2, P3] -> L7_affine_parallel[{ S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)]; S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] -> [(i6)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i6)] }]"
                          permutable: 1
                          array_expansion: [ none ]
                          child:
                            sequence:
                            - filter: "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                              child:
                                schedule: "[P0, P1, P2, P3] -> L6_affine_for[{ S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)]; S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> [(i7)] }]"
                                array_expansion: [ none ]
                                child:
                                  sequence:
                                  - filter: "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] }"
                                  - filter: "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] }"
                            - filter: "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] }"
                            - filter: "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, i2, i3, i4, i5, i6] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5]; S30_affine_load[i0, i1, i2, i3, i4, i5] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L13_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i3)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i3)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i3)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i3)] }]"
                permutable: 1
                array_expansion: [ none ]
                child:
                  schedule: "[P0, P1, P2, P3] -> L12_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i4)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i4)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i4)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i4)] }]"
                  permutable: 1
                  array_expansion: [ none ]
                  child:
                    schedule: "[P0, P1, P2, P3] -> L11_affine_parallel[{ S30_affine_load[i0, i1, i2, i3, i4, i5] -> [(i5)]; S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> [(i5)]; S33_affine_yield[i0, i1, i2, i3, i4, i5] -> [(i5)]; S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> [(i5)] }]"
                    permutable: 1
                    array_expansion: [ none ]
                    child:
                      sequence:
                      - filter: "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] }"
                      - filter: "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, i2, i3, i4, i5] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S25_llvm_fadd[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S7_arith_index_cast[]; S32_affine_vector_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S18_affine_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S19_affine_store_var[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S31_llvm_bitcast[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S1_memref_ataddr[]; S21_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S23_llvm_bitcast[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S24_llvm_fmul[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S5_arith_index_cast[]; S14_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S30_affine_load[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S22_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S16_affine_vector_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S11_affine_store[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S4_arith_index_cast[]; S27_affine_store[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S33_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15; S0_llvm_mlir_constant[]; S12_affine_yield[i0, i1, 0, i3, i4, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S20_affine_vector_load[i0, i1, 0, i3, i4, i5, 0, i7] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15; S3_memref_ataddr[]; S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S11_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S11_affine_store[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
  - S12_affine_yield:
  - S13_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S13_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S14_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_7[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S14_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_6[] }"
  - S15_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, 0, i3, i4, i5, 0] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - must_write "[P0, P1, P2, P3] -> { S15_affine_vector_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S16_affine_vector_store:
        - must_write "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_10[4i4 + 64i5] }"
        - read "[P0, P1, P2, P3] -> { S16_affine_vector_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_vector_load_res_9[] }"
  - S17_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_6[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S17_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_vector_load_res_9[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S18_affine_load:
        - read "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - must_write "[P0, P1, P2, P3] -> { S18_affine_load[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
  - S19_affine_store_var:
        - read "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_load_res_11[] }"
        - must_write "[P0, P1, P2, P3] -> { S19_affine_store_var[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S20_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_7[64i5 + 4i7] }"
        - must_write "[P0, P1, P2, P3] -> { S20_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S21_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S21_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_13[] }"
  - S22_affine_vector_load:
        - read "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_memref_alloca_res_10[4i4 + 64i7] }"
        - must_write "[P0, P1, P2, P3] -> { S22_affine_vector_load[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S23_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
        - read "[P0, P1, P2, P3] -> { S23_llvm_bitcast[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_vector_load_res_15[] }"
  - S24_llvm_fmul:
        - must_write "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_14[] }"
        - read "[P0, P1, P2, P3] -> { S24_llvm_fmul[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_bitcast_res_16[] }"
  - S25_llvm_fadd:
        - must_write "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S25_llvm_fadd[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fmul_res_17[] }"
  - S26_affine_yield:
        - must_write "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_affine_for_res_12[] }"
        - read "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, i2, i3, i4, i5, i6, i7] -> A_llvm_fadd_res_18[] }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_13[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_14[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_affine_vector_load_res_15[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_bitcast_res_16[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fmul_res_17[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S26_affine_yield[i0, i1, 0, i3, i4, i5, 0, i7] -> A_llvm_fadd_res_18[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 and 0 <= i7 <= 15 }"
  - S27_affine_store:
        - must_write "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_memref_alloca_res_4[i4, i5, i6] }"
        - read "[P0, P1, P2, P3] -> { S27_affine_store[i0, i1, i2, i3, i4, i5, i6] -> A_affine_for_res_12[] }"
  - S28_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_load_res_11[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S28_affine_yield[i0, i1, 0, i3, i4, i5, 0] -> A_affine_for_res_12[] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= i4 <= 15 and 0 <= i5 <= 15 }"
  - S29_affine_yield:
  - S30_affine_load:
        - read "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_memref_alloca_res_4[i3, i4, i5] }"
        - must_write "[P0, P1, P2, P3] -> { S30_affine_load[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S31_llvm_bitcast:
        - must_write "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
        - read "[P0, P1, P2, P3] -> { S31_llvm_bitcast[i0, i1, i2, i3, i4, i5] -> A_affine_load_res_19[] }"
  - S32_affine_vector_store:
        - may_write "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, 0, i3, i4, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - read "[P0, P1, P2, P3] -> { S32_affine_vector_store[i0, i1, i2, i3, i4, i5] -> A_llvm_bitcast_res_20[] }"
  - S33_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_affine_load_res_19[] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
        - kill "[P0, P1, P2, P3] -> { S33_affine_yield[i0, i1, 0, i3, i4, 0] -> A_llvm_bitcast_res_20[] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= i3 <= 15 and 0 <= i4 <= 15 }"
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
  - S35_llvm_return:
Schedule:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  sequence:
  - filter: "[P0, P1, P2, P3] -> { S0_llvm_mlir_constant[] }"
  - filter: "[P0, P1, P2, P3] -> { S1_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S2_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S3_memref_ataddr[] }"
  - filter: "[P0, P1, P2, P3] -> { S4_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S5_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S6_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S7_arith_index_cast[] }"
  - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2]; RS2_affine_parallel[i0, i1, i2, i3]; S8_memref_alloca[i0, i1, i2]; RS1_affine_parallel[i0, i1, i2, i3]; RS3_affine_parallel[i0, i1, i2]; S29_affine_yield[i0, i1, i2, i3]; S10_memref_alloca[i0, i1, i2]; S9_memref_alloca[i0, i1, i2]; RS0_affine_parallel[i0, i1, i2] }"
    child:
      schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S9_memref_alloca[i0, i1, i2] -> [(i0)]; S8_memref_alloca[i0, i1, i2] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; S29_affine_yield[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; S10_memref_alloca[i0, i1, i2] -> [(i0)]; S34_affine_yield[i0, i1, i2] -> [(i0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S9_memref_alloca[i0, i1, i2] -> [(i1)]; S8_memref_alloca[i0, i1, i2] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; S29_affine_yield[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; S10_memref_alloca[i0, i1, i2] -> [(i1)]; S34_affine_yield[i0, i1, i2] -> [(i1)] }]"
        permutable: 1
        array_expansion: [ none ]
        child:
          schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS0_affine_parallel[i0, i1, i2] -> [(i2)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S9_memref_alloca[i0, i1, i2] -> [(i2)]; S8_memref_alloca[i0, i1, i2] -> [(i2)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i2)]; S29_affine_yield[i0, i1, i2, i3] -> [(i2)]; RS3_affine_parallel[i0, i1, i2] -> [(i2)]; S10_memref_alloca[i0, i1, i2] -> [(i2)]; S34_affine_yield[i0, i1, i2] -> [(i2)] }]"
          permutable: 1
          array_expansion: [ none ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S9_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S10_memref_alloca[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3]; RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)]; S29_affine_yield[i0, i1, i2, i3] -> [(i3)] }]"
                array_expansion: [ none ]
                child:
                  sequence:
                  - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  - filter: "[P0, P1, P2, P3] -> { S29_affine_yield[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
            - filter: "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, i2] }"
Accesses:
domain: "[P0, P1, P2, P3] -> { S4_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S1_memref_ataddr[]; RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
accesses:
  - S0_llvm_mlir_constant:
  - S1_memref_ataddr:
  - S2_memref_ataddr:
  - S3_memref_ataddr:
  - S4_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[] }"
  - S5_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S5_arith_index_cast[] -> A_llvm_func_arg_10_1[] }"
  - S6_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }"
  - S7_arith_index_cast:
        - read "[P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[] }"
  - S8_memref_alloca:
  - S9_memref_alloca:
  - S10_memref_alloca:
  - S29_affine_yield:
  - S34_affine_yield:
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
        - kill "[P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
  - S35_llvm_return:
  - RS0_affine_parallel:
        - must_write "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS1_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
        - must_write "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
  - RS2_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - read "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }"
        - must_write "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
  - RS3_affine_parallel:
        - read "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }"
        - may_write "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] }
dep_order for A_llvm_func_arg_11_0 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_10_1 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_1_2 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_0_3 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_4 [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 > i3 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P0 and o3 > i3 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0) }
dep_order for A_memref_ataddr_res_5 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_6 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_7 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_memref_ataddr_res_8 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_9 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_10 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_affine_load_res_11 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_for_res_12 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_13 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_14 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_15 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_16 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fmul_res_17 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fadd_res_18 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_load_res_19 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_20 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_ataddr_res_21 [P0, P1, P2, P3] -> {  }
ReductionTagMap: [P0, P1, P2, P3] -> {  }
TaggedStmtDomain: [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] }
dep_order for A_llvm_func_arg_11_0 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_10_1 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_1_2 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_func_arg_0_3 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_4 [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 > i3 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and o3 < i3) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and o1 > i1 and o1 < P0 and o3 > i3 and 16o3 < P2); RS2_affine_parallel[i0, i1, i2, i3] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0); RS3_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and o1 > i1 and o1 < P0) }
dep_order for A_memref_ataddr_res_5 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_6 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_7 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_memref_ataddr_res_8 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_9 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_alloca_res_10 [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, o3] : (i2 = 0 and o2 = 0 and i0 >= 0 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 and o0 > i0 and o0 < P1 and o1 >= 0 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and o1 > i1 and o1 < P0 and o3 >= 0 and 16o3 < P2) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2) }
dep_order for A_affine_load_res_11 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_for_res_12 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_13 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_14 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_vector_load_res_15 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_16 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fmul_res_17 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_fadd_res_18 [P0, P1, P2, P3] -> {  }
dep_order for A_affine_load_res_19 [P0, P1, P2, P3] -> {  }
dep_order for A_llvm_bitcast_res_20 [P0, P1, P2, P3] -> {  }
dep_order for A_memref_ataddr_res_21 [P0, P1, P2, P3] -> {  }
tagged_reads [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> A_llvm_func_arg_10_1[]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> A_llvm_func_arg_11_0[]; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> A_llvm_func_arg_0_3[]; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> A_llvm_func_arg_1_2[]; [RS1_affine_parallel[i0, i1, 0, i3] -> S15_affine_vector_load_Read0[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S13_affine_vector_load_Read0[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
atagged_reads [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[]] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> A_llvm_func_arg_1_2[]; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[]] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> A_llvm_func_arg_10_1[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> A_llvm_func_arg_0_3[]; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> A_llvm_func_arg_11_0[] }
reads [P0, P1, P2, P3] -> { S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : P0 > 0 and P1 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and P0 > 0 and P1 > 0 and P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[] }
async_reads [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
tagged_may_writes [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
atagged_may_writes [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[]] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
may_writes [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
tagged_must_writes [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
atagged_must_writes [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
must_writes [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and 0 <= o0 <= 15 and 0 <= o1 <= 15; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 and 0 <= o0 <= 15 and 0 <= o1 <= 15 }
async_must_writes [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[o0] : (o0) mod 4 = 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and -3 <= o0 <= 1020 and 64*floor((3 + o0)/64) <= o0 }
tagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> S34_affine_yield1[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield0[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> S34_affine_yield2[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0 }
atagged_must_kills [P0, P1, P2, P3] -> { [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[]] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[]] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0; [S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[]] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
must_kills [P0, P1, P2, P3] -> { S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_10[o0] : 0 <= i0 < P1 and 0 <= i1 < P0; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_4[o0, o1, o2] : 0 <= i0 < P1 and 0 <= i1 < P0; S34_affine_yield[i0, i1, 0] -> A_memref_alloca_res_7[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
live_in [P0, P1, P2, P3] -> { S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]; S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_8[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]; S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]; RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_ataddr_res_5[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
live_out [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> A_memref_ataddr_res_21[o0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
independence [P0, P1, P2, P3] -> { S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S8_memref_alloca[i0, i1, i2] -> S8_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, o2] : o2 < i2 or o2 > i2; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, i1, i2] : o0 < i0 or o0 > i0; RS0_affine_parallel[i0, i1, i2] -> RS0_affine_parallel[o0, o1, i2] : o1 < i1 or o1 > i1; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, o2] : o2 > i2 or o2 < i2; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, i1, i2] : o0 > i0 or o0 < i0; RS3_affine_parallel[i0, i1, i2] -> RS3_affine_parallel[o0, o1, i2] : o1 > i1 or o1 < i1; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S10_memref_alloca[i0, i1, i2] -> S10_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, o2] : o2 < i2 or o2 > i2; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, i1, i2] : o0 < i0 or o0 > i0; S9_memref_alloca[i0, i1, i2] -> S9_memref_alloca[o0, o1, i2] : o1 < i1 or o1 > i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS1_affine_parallel[i0, i1, i2, i3] -> RS1_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, o2, i3] : o2 > i2 or o2 < i2; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[i0, i1, i2, o3] : o3 > i3 or o3 < i3; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, o1, i2, i3] : o1 > i1 or o1 < i1; RS2_affine_parallel[i0, i1, i2, i3] -> RS2_affine_parallel[o0, i1, i2, i3] : o0 > i0 or o0 < i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, o2] : o2 < i2 or o2 > i2; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, i1, i2] : o0 < i0 or o0 > i0; S34_affine_yield[i0, i1, i2] -> S34_affine_yield[o0, o1, i2] : o1 < i1 or o1 > i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, o2, i3] : o2 > i2 or o2 < i2; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[i0, i1, i2, o3] : o3 > i3 or o3 < i3; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, o1, i2, i3] : o1 > i1 or o1 < i1; S29_affine_yield[i0, i1, i2, i3] -> S29_affine_yield[o0, i1, i2, i3] : o0 > i0 or o0 < i0 }
dep_flow [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }
tagged_dep_flow [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }
atagged_dep_flow [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
dep_false [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, -1 + P0, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P0 > 0 and P2 > 0 and 0 <= i0 <= -2 + P1 and -16 + P2 <= 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, 1 + i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and -16 + P2 <= 16i3 < P2; RS1_affine_parallel[i0, -1 + P0, 0, i3] -> RS1_affine_parallel[1 + i0, 0, 0, 0] : P0 > 0 and P2 > 0 and 0 <= i0 <= -2 + P1 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P1 and 0 <= i1 <= -2 + P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, -1 + P0, 0, i3] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and 0 <= i0 <= -2 + P1 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : 0 <= i0 < P1 and 0 <= i1 <= -2 + P0; RS3_affine_parallel[i0, -1 + P0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and 0 <= i0 <= -2 + P1; RS0_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, 1 + i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 <= -2 + P0; RS0_affine_parallel[i0, -1 + P0, 0] -> RS0_affine_parallel[1 + i0, 0, 0] : P0 > 0 and P2 <= 0 and 0 <= i0 <= -2 + P1 }
dep_forced [P0, P1, P2, P3] -> {  }
dep_order [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 }
tagged_dep_order [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2 }
dep_async [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }
array_order [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS3_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS1_affine_parallel[i0, i1, 0, o3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[o0, o1, 0, o3] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and ((16i3 < P2 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P2)); RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, o1, 0, o3] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and ((16i3 < P2 and 0 <= o3 < i3) or (i3 >= 0 and o3 > i3 and 16o3 < P2)); RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[o0, o1, 0] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS0_affine_parallel[i0, o1, 0] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 }
tagger [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> S18_affine_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS0_affine_parallel[i0, i1, i2] -> S11_affine_store_Write0[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S14_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S6_arith_index_cast[] -> S6_arith_index_cast_Read0[]] -> S6_arith_index_cast[]; [S4_arith_index_cast[] -> S4_arith_index_cast_Read0[]] -> S4_arith_index_cast[]; [RS2_affine_parallel[i0, i1, i2, i3] -> S27_affine_store_Write0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S20_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield2[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield1[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S15_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> S34_affine_yield0[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> S22_affine_vector_load_Read0[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S7_arith_index_cast[] -> S7_arith_index_cast_Read0[]] -> S7_arith_index_cast[]; [RS3_affine_parallel[i0, i1, i2] -> S32_affine_vector_store_MayWrite0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS3_affine_parallel[i0, i1, i2] -> S30_affine_load_Read0[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S16_affine_vector_store_Write0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> S13_affine_vector_load_Read0[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> S5_arith_index_cast_Read0[]] -> S5_arith_index_cast[] }
atagger [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS0_affine_parallel[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_8[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_4[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> RS3_affine_parallel[(i0), (i1), (i2)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_10[]] -> S34_affine_yield[(i0), (i1), (i2)]; [S4_arith_index_cast[] -> A_llvm_func_arg_11_0[]] -> S4_arith_index_cast[]; [S7_arith_index_cast[] -> A_llvm_func_arg_0_3[]] -> S7_arith_index_cast[]; [S6_arith_index_cast[] -> A_llvm_func_arg_1_2[]] -> S6_arith_index_cast[]; [S34_affine_yield[i0, i1, i2] -> A_memref_alloca_res_7[]] -> S34_affine_yield[(i0), (i1), (i2)]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_ataddr_res_5[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> RS2_affine_parallel[(i0), (i1), (i2), (i3)]; [S5_arith_index_cast[] -> A_llvm_func_arg_10_1[]] -> S5_arith_index_cast[]; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> RS1_affine_parallel[(i0), (i1), (i2), (i3)]; [RS3_affine_parallel[i0, i1, i2] -> A_memref_ataddr_res_21[]] -> RS3_affine_parallel[(i0), (i1), (i2)] }
schedule
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  schedule: "[P0, P1, P2, P3] -> L3_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  array_expansion: [ none ]
  child:
    schedule: "[P0, P1, P2, P3] -> L2_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }]"
    permutable: 1
    array_expansion: [ none ]
    child:
      schedule: "[P0, P1, P2, P3] -> L1_affine_parallel[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(0)]; RS3_affine_parallel[i0, i1, i2] -> [(0)]; RS0_affine_parallel[i0, i1, i2] -> [(0)] }]"
      permutable: 1
      array_expansion: [ none ]
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
          child:
            schedule: "[P0, P1, P2, P3] -> L0_affine_for[{ RS2_affine_parallel[i0, i1, i2, i3] -> [(i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
            array_expansion: [ none ]
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
        - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Schedule constraints:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
validity: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }"
coincidence: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }"
condition: "[P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> S18_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS0_affine_parallel[i0, i1, 0] -> S11_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> S16_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS1_affine_parallel[i0, i1, 0, i3] -> S14_affine_vector_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S27_affine_store_Write0[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> S18_affine_load_Read0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2 }"
conditional_validity: "[P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[o0, o1, 0, o3] -> S27_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, o1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS2_affine_parallel[i0, i1, 0, o3] -> S27_affine_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S30_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S20_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S14_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[o0, o1, 0] -> S11_affine_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S18_affine_load_Read0[]] -> [RS0_affine_parallel[i0, o1, 0] -> S11_affine_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[o0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : i0 >= 0 and 0 <= i1 < P0 and i0 < o0 < P1 and 0 <= o1 < P0; [RS3_affine_parallel[i0, i1, 0] -> S32_affine_vector_store_MayWrite0[]] -> [RS3_affine_parallel[i0, o1, 0] -> S32_affine_vector_store_MayWrite0[]] : 0 <= i0 < P1 and i1 >= 0 and i1 < o1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[o0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : i0 >= 0 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 and i0 < o0 < P1 and 0 <= o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, o1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and i1 >= 0 and i3 >= 0 and 16i3 < P2 and i1 < o1 < P0 and o3 >= 0 and 16o3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> S22_affine_vector_load_Read0[]] -> [RS1_affine_parallel[i0, i1, 0, o3] -> S16_affine_vector_store_Write0[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and o3 > i3 and 16o3 < P2 }"
proximity: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] -> RS2_affine_parallel[i0, i1, 0, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] -> RS3_affine_parallel[i0, i1, 0] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; RS2_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, 1 + i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, 0] -> RS3_affine_parallel[i0, i1, 0] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0 }"
anti_proximity: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, 0, i3] -> RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
live_range_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and -16 + P2 <= 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : P2 <= 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 1 + i3] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[i0, i1, 0, 0] -> A_memref_alloca_res_4[]] : P2 > 0 and 0 <= i0 < P1 and 0 <= i1 < P0; [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
live_range_maximal_span: "[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_7[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; [RS0_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[i0, i1, 0] -> A_memref_alloca_res_4[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and (P2 >= 17 or P2 <= 0 or (0 < P2 <= 16)); [RS1_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[i0, i1, 0, i3] -> A_memref_alloca_res_10[]] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2 }"
array_sizes: "[P0, P1, P2, P3] -> { A_memref_alloca_res_10[] -> [1024]; A_memref_alloca_res_4[] -> [1024]; A_memref_alloca_res_7[] -> [1024] }"
New Schedule:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
  permutable: 1
  coincident: [ 1, 1 ]
  array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
  child:
    sequence:
    - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
    - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
      child:
        schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
        permutable: 1
        coincident: [ 1 ]
        array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
        child:
          sequence:
          - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
          - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
    - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Prefix schedule [P0, P1, P2, P3] -> {  }
[P0, P1, P2, P3] -> {  }
# YOU ARE HERE
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_4@0x1fda7740
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> {  }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> []; RS3_affine_parallel[i0, i1, i2] -> []; RS2_affine_parallel[i0, i1, i2, i3] -> []; RS0_affine_parallel[i0, i1, i2] -> [] }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  # YOU ARE HERE
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Deltas [P0, P1, P2, P3] -> { [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : (P0 > 0 and P1 > 0 and P2 > 0) or (P0 > 0 and P1 > 0 and P2 <= 0) or (P0 > 0 and P1 > 0 and P2 >= 17) or (P0 > 0 and P1 > 0 and P2 > 0) }
Array A_memref_alloca_res_4@0x1fda7740
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> []; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> []; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [] }
Applied schedule [P0, P1, P2, P3] -> { [] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [] : P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[] -> []] -> [] : P0 > 0 and P1 > 0 and P2 > 0 }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS0_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS3_affine_parallel[i0, i1, i2] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] -> [] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    # YOU ARE HERE
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : (o0 = i0 and o1 = i1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (o0 = i0 and o1 = i1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 <= 0) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 >= 17) or (i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0) }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Array A_memref_alloca_res_4@0x1fda7740
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> {  }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; RS0_affine_parallel[i0, i1, i2] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      # YOU ARE HERE
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 2 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : (i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 1 and o0 = i0 and o1 = i1 and o2 = 2 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and o0 = i0 and o1 = i1 and o2 = 2 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : (i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 >= 17) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and i2 = 1 and P0 > 0 and P1 > 0 and P2 > 0) or (i0 = 0 and i1 = 0 and i2 = 2 and P0 > 0 and P1 > 0 and P2 <= 0) }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : (i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and P2 >= 17 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 1 and i3 = i0 and i4 = i1 and i5 = 2 and o0 = 0 and o1 = 0 and o2 = 1 and P2 > 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) or (i2 = 0 and i3 = i0 and i4 = i1 and i5 = 2 and o0 = 0 and o1 = 0 and o2 = 2 and P2 <= 0 and i0 >= 0 and i0 < P0 and i1 >= 0 and i1 < P1) }
Array A_memref_alloca_res_4@0x1fda7740
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 2; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 1 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : i2 = 1 and o0 = i0 and o1 = i1 and o2 = 1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i2 = 1 and i3 = i0 and i4 = i1 and i5 = 1 and o0 = 0 and o1 = 0 and o2 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        child:
          # YOU ARE HERE
          leaf
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1] -> [o0, o1] : o0 = i0 and o1 = i1 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Deltas [P0, P1, P2, P3] -> { [i0, i1] : i0 = 0 and i1 = 0 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1] -> [i2, i3]] -> [o0, o1] : i2 = i0 and i3 = i1 and o0 = 0 and o1 = 0 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and P2 > 0 }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          # YOU ARE HERE
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : o0 = i0 and o1 = i1 and o2 = 21 + i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and 16i2 < P2 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i3 = i0 and i4 = i1 and i5 = 21 + i2 and o0 = 0 and o1 = 0 and o2 = 21 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and 16i2 < P2 }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 0
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
Applied schedule [P0, P1, P2, P3] -> { [i0, i1, i2] -> [o0, o1, o2] : o0 = i0 and o1 = i1 and o2 = 21 + i2 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and 16i2 < P2 }
Deltas [P0, P1, P2, P3] -> { [i0, i1, i2] : i0 = 0 and i1 = 0 and i2 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
Deltas map [P0, P1, P2, P3] -> { [[i0, i1, i2] -> [i3, i4, i5]] -> [o0, o1, o2] : i3 = i0 and i4 = i1 and i5 = 21 + i2 and o0 = 0 and o1 = 0 and o2 = 21 and i1 >= 0 and i1 < P1 and i0 >= 0 and i0 < P0 and i2 >= 0 and 16i2 < P2 }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 0
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = 21 + i3 and o3 = 0; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2, o3] : o0 = i1 and o1 = i0 and o2 = i3 and o3 = 1 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            # YOU ARE HERE
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = 21 + i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
              child:
                # YOU ARE HERE
                leaf
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = 21 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - # YOU ARE HERE
              filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_7[]; A_memref_alloca_res_10[]; A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : o0 = i1 and o1 = i0 and o2 = i3 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1, o2] : i2 = 0 and o0 = i1 and o1 = i0 and o2 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
          permutable: 1
          coincident: [ 1 ]
          array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
          child:
            sequence:
            - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
            - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
              child:
                # YOU ARE HERE
                leaf
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Unallocated [P0, P1, P2, P3] -> {  }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[]; A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          mark: "allocate_array"
          child:
            schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
            permutable: 1
            coincident: [ 1 ]
            array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - # YOU ARE HERE
        filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Prefix schedule [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : o0 = i1 and o1 = i0 }
[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] -> [o0, o1] : i2 = 0 and o0 = i1 and o1 = i0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      sequence:
      - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
      - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
        child:
          mark: "allocate_array"
          child:
            schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
            permutable: 1
            coincident: [ 1 ]
            array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
            child:
              sequence:
              - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
              - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
      - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
        child:
          # YOU ARE HERE
          leaf
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_10@0x1fd66b40
 coincidence: 1
Tagged prefix schedule [P0, P1, P2, P3] -> { [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0, o1] : o0 = i1 and o1 = i0 }
Applied schedule [P0, P1, P2, P3] -> {  }
Deltas [P0, P1, P2, P3] -> {  }
Deltas map [P0, P1, P2, P3] -> {  }
Array A_memref_alloca_res_7@0x1fda9490
 coincidence: 1
Unallocated [P0, P1, P2, P3] -> { A_memref_alloca_res_10[]; A_memref_alloca_res_7[] }
Allocated [P0, P1, P2, P3] -> { A_memref_alloca_res_4[] }
ToExpand [P0, P1, P2, P3] -> {  }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; RS2_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, i2, i3] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, i2] : i2 = 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 }
Domain [P0, P1, P2, P3] -> {  }
New Schedule Prepared for GPU:
domain: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; RS2_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS1_affine_parallel[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; RS3_affine_parallel[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }"
child:
  mark: "grid_parallel"
  child:
    schedule: "[P0, P1, P2, P3] -> [{ RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }, { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }]"
    permutable: 1
    coincident: [ 1, 1 ]
    array_expansion: [ [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ], [ A_memref_alloca_res_10: 1, A_memref_alloca_res_7: 1, A_memref_alloca_res_4: 1 ] ]
    child:
      mark: "allocate_array"
      child:
        sequence:
        - filter: "[P0, P1, P2, P3] -> { RS0_affine_parallel[i0, i1, i2] }"
        - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3]; RS1_affine_parallel[i0, i1, i2, i3] }"
          child:
            mark: "allocate_array"
            child:
              schedule: "[P0, P1, P2, P3] -> [{ RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }]"
              permutable: 1
              coincident: [ 1 ]
              array_expansion: [ [ A_memref_alloca_res_10: 22, A_memref_alloca_res_7: 22, A_memref_alloca_res_4: 1 ] ]
              child:
                sequence:
                - filter: "[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] }"
                  child:
                    mark: "async_wait_group"
                - filter: "[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] }"
        - filter: "[P0, P1, P2, P3] -> { RS3_affine_parallel[i0, i1, i2] }"
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and 16i3 >= -16 + P2 and 16i3 < P2 and i3 >= 0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 <= 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 1 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and 16i3 >= -16 + P2 and 16i3 < P2 and i3 >= 0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS3_affine_parallel[o0, o1, o2] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and P2 <= 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 1 + i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 <= -17 + P2; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_4[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = 0 and P2 > 0 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0; [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : o0 = 21 + i0 and P0 > 0 and P1 > 0 and i0 >= 0 and 16i0 < P2 }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS2_affine_parallel@0x1fd82e90
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_10[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : o0 = 21 + i0 and P0 > 0 and P1 > 0 and i0 >= 0 and 16i0 < P2 }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : o0 = 21 + i0 and P0 > 0 and P1 > 0 and i0 >= 0 and 16i0 < P2 }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
RS2_affine_parallel@0x1fd82e90
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [RS2_affine_parallel[o0, o1, o2, o3] -> A_memref_alloca_res_7[]] : i2 = 0 and o0 = i0 and o1 = i1 and o2 = 0 and o3 = i3 and i0 >= 0 and i0 < P1 and i1 >= 0 and i1 < P0 and i3 >= 0 and 16i3 < P2 }
[P0, P1, P2, P3] -> { [i0] -> [o0] : o0 = 21 + i0 and P0 > 0 and P1 > 0 and i0 >= 0 and 16i0 < P2 }
[P0, P1, P2, P3] -> { [i0] : i0 = 21 and P0 > 0 and P1 > 0 and P2 > 0 }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
RS2_affine_parallel@0x1fd82e90
New AST:
mark: grid_parallel@0x2
node:
  iterator:
    id: c0
  init:
    val: 0
  cond:
    op: lt
    args:
    - id: c0
    - id: P0@0x1fd6d330
  inc:
    val: 1
  body:
    iterator:
      id: c1
    init:
      val: 0
    cond:
      op: lt
      args:
      - id: c1
      - id: P1@0x1fd6d420
    inc:
      val: 1
    body:
      mark: allocate_array@0x1fe63f00
      node:
      - user:
          op: call
          args:
          - id: RS0_affine_parallel@0x1fd52330
          - id: c1
          - id: c0
          - val: 0
          - op: call
            args:
            - id: A_memref_alloca_res_4@0x1fda7740
            - val: 0
            - val: 0
      - mark: allocate_array@0x1ff7c6c0
        node:
        - iterator:
            id: c2
          init:
            val: 0
          cond:
            op: le
            args:
            - id: c2
            - op: min
              args:
              - val: 20
              - op: fdiv_q
                args:
                - op: sub
                  args:
                  - id: P2@0x1fdad480
                  - val: 1
                - val: 16
          inc:
            val: 1
          body:
            user:
              op: call
              args:
              - id: RS1_affine_parallel@0x1fd6a370
              - id: c1
              - id: c0
              - val: 0
              - id: c2
              - op: call
                args:
                - id: A_memref_alloca_res_7@0x1fda9490
                - val: 0
                - val: 0
                - op: pdiv_r
                  args:
                  - op: add
                    args:
                    - id: c2
                    - val: 21
                  - val: 22
              - op: call
                args:
                - id: A_memref_alloca_res_10@0x1fd66b40
                - val: 0
                - val: 0
                - op: pdiv_r
                  args:
                  - op: add
                    args:
                    - id: c2
                    - val: 21
                  - val: 22
        - iterator:
            id: c2
          init:
            val: 21
          cond:
            op: le
            args:
            - id: c2
            - op: add
              args:
              - op: fdiv_q
                args:
                - op: sub
                  args:
                  - id: P2@0x1fdad480
                  - val: 1
                - val: 16
              - val: 21
          inc:
            val: 1
          body:
          - mark: async_wait_group@0x1ff3d850
            node:
              user:
                op: call
                args:
                - id: RS2_affine_parallel@0x1fd82e90
                - id: c1
                - id: c0
                - val: 0
                - op: sub
                  args:
                  - id: c2
                  - val: 21
                - op: call
                  args:
                  - id: A_memref_alloca_res_4@0x1fda7740
                  - val: 0
                  - val: 0
                  - val: 0
                - op: call
                  args:
                  - id: A_memref_alloca_res_7@0x1fda9490
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
                - op: call
                  args:
                  - id: A_memref_alloca_res_10@0x1fd66b40
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
          - guard:
              op: ge
              args:
              - id: P2@0x1fdad480
              - op: add
                args:
                - op: mul
                  args:
                  - val: 16
                  - id: c2
                - val: 1
            then:
              user:
                op: call
                args:
                - id: RS1_affine_parallel@0x1fd6a370
                - id: c1
                - id: c0
                - val: 0
                - id: c2
                - op: call
                  args:
                  - id: A_memref_alloca_res_7@0x1fda9490
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
                - op: call
                  args:
                  - id: A_memref_alloca_res_10@0x1fd66b40
                  - val: 0
                  - val: 0
                  - op: pdiv_r
                    args:
                    - op: sub
                      args:
                      - id: c2
                      - val: 1
                    - val: 22
      - user:
          op: call
          args:
          - id: RS3_affine_parallel@0x1fd845c0
          - id: c1
          - id: c0
          - val: 0
          - op: call
            args:
            - id: A_memref_alloca_res_4@0x1fda7740
            - val: 0
            - val: 0
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i1)]; RS3_affine_parallel[i0, i1, i2] -> [(i1)]; RS0_affine_parallel[i0, i1, i2] -> [(i1)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i1 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { RS1_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS2_affine_parallel[i0, i1, i2, i3] -> [(i0)]; RS3_affine_parallel[i0, i1, i2] -> [(i0)]; RS0_affine_parallel[i0, i1, i2] -> [(i0)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS0_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0; [RS3_affine_parallel[i0, i1, i2] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i0 }
RS3_affine_parallel@0x1fd845c0
RS0_affine_parallel@0x1fd52330
RS2_affine_parallel@0x1fd82e90
RS1_affine_parallel@0x1fd6a370
[P0, P1, P2, P3] -> { RS2_affine_parallel[i0, i1, i2, i3] -> [(21 + i3)]; RS1_affine_parallel[i0, i1, i2, i3] -> [(i3)] }
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_10[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
RS2_affine_parallel@0x1fd82e90
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_7[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
RS2_affine_parallel@0x1fd82e90
[P0, P1, P2, P3] -> { [RS1_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = i3; [RS2_affine_parallel[i0, i1, i2, i3] -> A_memref_alloca_res_4[]] -> [o0] : o0 = 21 + i3 }
RS1_affine_parallel@0x1fd6a370
RS2_affine_parallel@0x1fd82e90
New func:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg30: index, %arg31: index, %arg32: index):
      "affine.store"(%0, %20, %arg30, %arg31, %arg32) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %30 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %32 = "arith.subi"(%28, %29) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %33 = "arith.addi"(%32, %30) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %34 = "arith.cmpi"(%28, %31) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %35 = "arith.select"(%34, %33, %28) : (i1, i64, i64) -> i64
    %36 = "arith.divsi"(%35, %29) : (i64, i64) -> i64
    %37 = "arith.minsi"(%36, %25) : (i64, i64) -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %40, %38) ({
    ^bb0(%arg26: i64):
      %98 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %99 = "arith.index_cast"(%arg26) : (i64) -> index
      %100 = "memref.subview"(%23, %99) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %101 = "arith.index_cast"(%arg26) : (i64) -> index
      %102 = "memref.subview"(%22, %101) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %103 = "arith.index_cast"(%arg26) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg27: index, %arg28: index, %arg29: index):
        %104 = "affine.vector_load"(%2, %arg28, %arg27, %arg12, %103, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%104, %100, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        %105 = "affine.vector_load"(%1, %arg28, %arg27, %103, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
        "affine.vector_store"(%105, %102, %arg28, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %41 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %42 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %43 = "arith.index_cast"(%5) : (index) -> i64
    %44 = "arith.subi"(%43, %42) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %45 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %46 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %47 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %48 = "arith.subi"(%44, %45) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %49 = "arith.addi"(%48, %46) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %50 = "arith.cmpi"(%44, %47) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %51 = "arith.select"(%50, %49, %44) : (i1, i64, i64) -> i64
    %52 = "arith.divsi"(%51, %45) : (i64, i64) -> i64
    %53 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %54 = "arith.addi"(%52, %53) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %56 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %57 = "arith.addi"(%54, %56) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%41, %57, %55) ({
    ^bb0(%arg17: i64):
      "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
      %61 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %62 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %63 = "arith.subi"(%arg17, %62) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.remui"(%arg17, %64) : (i64, i64) -> i64
      %66 = "arith.index_cast"(%65) : (i64) -> index
      %67 = "memref.subview"(%23, %66) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %68 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %69 = "arith.remui"(%arg17, %68) : (i64, i64) -> i64
      %70 = "arith.index_cast"(%69) : (i64) -> index
      %71 = "memref.subview"(%22, %70) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg21: index, %arg22: index, %arg23: index):
        %90 = "affine.load"(%20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        "affine.store_var"(%90, %91) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
        %91 = "affine.for"(%90) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg24: index, %arg25: f32):
          %92 = "affine.vector_load"(%67, %arg22, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %93 = "llvm.bitcast"(%92) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %94 = "affine.vector_load"(%71, %arg24, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %95 = "llvm.bitcast"(%94) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %96 = "llvm.fmul"(%93, %95) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %97 = "llvm.fadd"(%arg25, %96) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%97) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%91, %20, %arg21, %arg22, %arg23) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %72 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %73 = "arith.muli"(%72, %arg17) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %74 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %75 = "arith.addi"(%73, %74) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %76 = "arith.index_cast"(%5) : (index) -> i64
      %77 = "arith.cmpi"(%76, %75) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%77) ({
        %78 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %80 = "arith.remui"(%arg17, %79) : (i64, i64) -> i64
        %81 = "arith.index_cast"(%80) : (i64) -> index
        %82 = "memref.subview"(%23, %81) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %84 = "arith.remui"(%arg17, %83) : (i64, i64) -> i64
        %85 = "arith.index_cast"(%84) : (i64) -> index
        %86 = "memref.subview"(%22, %85) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %87 = "arith.index_cast"(%arg17) : (i64) -> index
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          %88 = "affine.vector_load"(%2, %arg19, %arg18, %arg12, %87, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S13_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%88, %82, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S14_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          %89 = "affine.vector_load"(%1, %arg19, %arg18, %87, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S15_affine_vector_load"} : (memref<?xi8>, index, index, index, index, index) -> vector<4xi8>
          "affine.vector_store"(%89, %86, %arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S16_affine_vector_store"} : (vector<4xi8>, memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> ()
          "affine.yield"() {polymer.stmt.name = "S17_affine_yield"} : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %58 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %59 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %60 = "llvm.bitcast"(%59) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%60, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
Found copy
Found copy
Found copy
Found copy
Converted to async:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg36: index, %arg37: index, %arg38: index):
      "affine.store"(%0, %20, %arg36, %arg37, %arg38) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %30 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %32 = "arith.subi"(%28, %29) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %33 = "arith.addi"(%32, %30) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %34 = "arith.cmpi"(%28, %31) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %35 = "arith.select"(%34, %33, %28) : (i1, i64, i64) -> i64
    %36 = "arith.divsi"(%35, %29) : (i64, i64) -> i64
    %37 = "arith.minsi"(%36, %25) : (i64, i64) -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %40, %38) ({
    ^bb0(%arg29: i64):
      %102 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %103 = "arith.index_cast"(%arg29) : (i64) -> index
      %104 = "memref.subview"(%23, %103) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %105 = "arith.index_cast"(%arg29) : (i64) -> index
      %106 = "memref.subview"(%22, %105) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %107 = "arith.index_cast"(%arg29) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg30: index, %arg31: index, %arg32: index):
        "affine.if"(%arg30, %arg31, %arg32) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg33: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg34: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg35: index):
                %108 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %109 = "affine.apply"(%arg34, %arg33, %arg12, %107, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %110 = "nvgpu.device_async_copy"(%104, %108, %2, %109) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %111 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %112 = "affine.apply"(%arg34, %arg33, %107, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %113 = "nvgpu.device_async_copy"(%106, %111, %1, %112) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %41 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %42 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %43 = "arith.index_cast"(%5) : (index) -> i64
    %44 = "arith.subi"(%43, %42) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %45 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %46 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %47 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %48 = "arith.subi"(%44, %45) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %49 = "arith.addi"(%48, %46) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %50 = "arith.cmpi"(%44, %47) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %51 = "arith.select"(%50, %49, %44) : (i1, i64, i64) -> i64
    %52 = "arith.divsi"(%51, %45) : (i64, i64) -> i64
    %53 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %54 = "arith.addi"(%52, %53) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %56 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %57 = "arith.addi"(%54, %56) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%41, %57, %55) ({
    ^bb0(%arg17: i64):
      %61 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %62 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %63 = "arith.subi"(%arg17, %62) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.remui"(%arg17, %64) : (i64, i64) -> i64
      %66 = "arith.index_cast"(%65) : (i64) -> index
      %67 = "memref.subview"(%23, %66) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %68 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %69 = "arith.remui"(%arg17, %68) : (i64, i64) -> i64
      %70 = "arith.index_cast"(%69) : (i64) -> index
      %71 = "memref.subview"(%22, %70) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg24: index, %arg25: index, %arg26: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %94 = "affine.load"(%20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        "affine.store_var"(%94, %95) <{type = "for.iv.init"}> {polymer.stmt.name = "S19_affine_store_var"} : (f32, f32) -> ()
        %95 = "affine.for"(%94) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg27: index, %arg28: f32):
          %96 = "affine.vector_load"(%67, %arg25, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %97 = "llvm.bitcast"(%96) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %98 = "affine.vector_load"(%71, %arg27, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %99 = "llvm.bitcast"(%98) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %100 = "llvm.fmul"(%97, %99) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %101 = "llvm.fadd"(%arg28, %100) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%101) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%95, %20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %72 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %73 = "arith.muli"(%72, %arg17) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %74 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %75 = "arith.addi"(%73, %74) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %76 = "arith.index_cast"(%5) : (index) -> i64
      %77 = "arith.cmpi"(%76, %75) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%77) ({
        %78 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %80 = "arith.remui"(%arg17, %79) : (i64, i64) -> i64
        %81 = "arith.index_cast"(%80) : (i64) -> index
        %82 = "memref.subview"(%23, %81) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %84 = "arith.remui"(%arg17, %83) : (i64, i64) -> i64
        %85 = "arith.index_cast"(%84) : (i64) -> index
        %86 = "memref.subview"(%22, %85) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %87 = "arith.index_cast"(%arg17) : (i64) -> index
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          "affine.if"(%arg18, %arg19, %arg20) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg21: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg22: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg23: index):
                  %88 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %89 = "affine.apply"(%arg22, %arg21, %arg12, %87, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %90 = "nvgpu.device_async_copy"(%82, %88, %2, %89) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %91 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %92 = "affine.apply"(%arg22, %arg21, %87, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %93 = "nvgpu.device_async_copy"(%86, %91, %1, %92) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "affine.yield"() : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %58 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %59 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %60 = "llvm.bitcast"(%59) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%60, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
/scr/ivan/src/transformer-llvm-project/polly/lib/External/isl/isl_ctx.c:307: isl_ctx not freed as some objects still reference it
gpu-affine-opt: After opt:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %5 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %6 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %7 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %8 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %9 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %10 = "arith.index_cast"(%6) : (index) -> i64
  %11 = "arith.index_cast"(%8) : (i64) -> index
  %12 = "arith.index_cast"(%10) : (i64) -> index
  %13 = "arith.index_cast"(%9) : (i64) -> index
  %14 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %15 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %16 = "arith.index_cast"(%7) : (index) -> i64
  %17 = "arith.index_cast"(%14) : (i64) -> index
  %18 = "arith.index_cast"(%16) : (i64) -> index
  %19 = "arith.index_cast"(%15) : (i64) -> index
  "scf.parallel"(%11, %17, %12, %18, %13, %19) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    %21 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg36: index, %arg37: index, %arg38: index):
      "affine.store"(%0, %20, %arg36, %arg37, %arg38) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %22 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %23 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<22x1024xi8, 3>
    %24 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %25 = "arith.constant"() <{value = 20 : i64}> : () -> i64
    %26 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %27 = "arith.index_cast"(%5) : (index) -> i64
    %28 = "arith.subi"(%27, %26) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %29 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %30 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %31 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %32 = "arith.subi"(%28, %29) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %33 = "arith.addi"(%32, %30) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %34 = "arith.cmpi"(%28, %31) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %35 = "arith.select"(%34, %33, %28) : (i1, i64, i64) -> i64
    %36 = "arith.divsi"(%35, %29) : (i64, i64) -> i64
    %37 = "arith.minsi"(%36, %25) : (i64, i64) -> i64
    %38 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %39 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%24, %40, %38) ({
    ^bb0(%arg29: i64):
      %102 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %103 = "arith.index_cast"(%arg29) : (i64) -> index
      %104 = "memref.subview"(%23, %103) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %105 = "arith.index_cast"(%arg29) : (i64) -> index
      %106 = "memref.subview"(%22, %105) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %107 = "arith.index_cast"(%arg29) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg30: index, %arg31: index, %arg32: index):
        "affine.if"(%arg30, %arg31, %arg32) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg33: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg34: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg35: index):
                %108 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %109 = "affine.apply"(%arg34, %arg33, %arg12, %107, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %110 = "nvgpu.device_async_copy"(%104, %108, %2, %109) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %111 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %112 = "affine.apply"(%arg34, %arg33, %107, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %113 = "nvgpu.device_async_copy"(%106, %111, %1, %112) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %41 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %42 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %43 = "arith.index_cast"(%5) : (index) -> i64
    %44 = "arith.subi"(%43, %42) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %45 = "arith.constant"() <{value = 16 : i64}> : () -> i64
    %46 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %47 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    %48 = "arith.subi"(%44, %45) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %49 = "arith.addi"(%48, %46) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %50 = "arith.cmpi"(%44, %47) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %51 = "arith.select"(%50, %49, %44) : (i1, i64, i64) -> i64
    %52 = "arith.divsi"(%51, %45) : (i64, i64) -> i64
    %53 = "arith.constant"() <{value = 21 : i64}> : () -> i64
    %54 = "arith.addi"(%52, %53) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %55 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %56 = "arith.constant"() <{value = 1 : i64}> : () -> i64
    %57 = "arith.addi"(%54, %56) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%41, %57, %55) ({
    ^bb0(%arg17: i64):
      %61 = "arith.constant"() <{value = 0 : i64}> : () -> i64
      %62 = "arith.constant"() <{value = 21 : i64}> : () -> i64
      %63 = "arith.subi"(%arg17, %62) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %64 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %65 = "arith.remui"(%arg17, %64) : (i64, i64) -> i64
      %66 = "arith.index_cast"(%65) : (i64) -> index
      %67 = "memref.subview"(%23, %66) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %68 = "arith.constant"() <{value = 22 : i64}> : () -> i64
      %69 = "arith.remui"(%arg17, %68) : (i64, i64) -> i64
      %70 = "arith.index_cast"(%69) : (i64) -> index
      %71 = "memref.subview"(%22, %70) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg24: index, %arg25: index, %arg26: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %94 = "affine.load"(%20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %95 = "affine.for"(%94) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg27: index, %arg28: f32):
          %96 = "affine.vector_load"(%67, %arg25, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %97 = "llvm.bitcast"(%96) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %98 = "affine.vector_load"(%71, %arg27, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %99 = "llvm.bitcast"(%98) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %100 = "llvm.fmul"(%97, %99) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %101 = "llvm.fadd"(%arg28, %100) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%101) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%95, %20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %72 = "arith.constant"() <{value = 16 : i64}> : () -> i64
      %73 = "arith.muli"(%72, %arg17) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %74 = "arith.constant"() <{value = 1 : i64}> : () -> i64
      %75 = "arith.addi"(%73, %74) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %76 = "arith.index_cast"(%5) : (index) -> i64
      %77 = "arith.cmpi"(%76, %75) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%77) ({
        %78 = "arith.constant"() <{value = 0 : i64}> : () -> i64
        %79 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %80 = "arith.remui"(%arg17, %79) : (i64, i64) -> i64
        %81 = "arith.index_cast"(%80) : (i64) -> index
        %82 = "memref.subview"(%23, %81) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %83 = "arith.constant"() <{value = 22 : i64}> : () -> i64
        %84 = "arith.remui"(%arg17, %83) : (i64, i64) -> i64
        %85 = "arith.index_cast"(%84) : (i64) -> index
        %86 = "memref.subview"(%22, %85) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %87 = "arith.index_cast"(%arg17) : (i64) -> index
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          "affine.if"(%arg18, %arg19, %arg20) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg21: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg22: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg23: index):
                  %88 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %89 = "affine.apply"(%arg22, %arg21, %arg12, %87, %5) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %90 = "nvgpu.device_async_copy"(%82, %88, %2, %89) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %91 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %92 = "affine.apply"(%arg22, %arg21, %87, %arg13, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %93 = "nvgpu.device_async_copy"(%86, %91, %1, %92) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "affine.yield"() : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %58 = "arith.constant"() <{value = 0 : i64}> : () -> i64
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %59 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %60 = "llvm.bitcast"(%59) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%60, %3, %arg13, %arg14, %arg15, %arg12, %4) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After shmem to alloca:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg36: index, %arg37: index, %arg38: index):
      "affine.store"(%8, %20, %arg36, %arg37, %arg38) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
    %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
    %23 = "arith.index_cast"(%13) : (index) -> i64
    %24 = "arith.subi"(%23, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %25 = "arith.subi"(%24, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %26 = "arith.addi"(%25, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %27 = "arith.cmpi"(%24, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %28 = "arith.select"(%27, %26, %24) : (i1, i64, i64) -> i64
    %29 = "arith.divsi"(%28, %4) : (i64, i64) -> i64
    %30 = "arith.minsi"(%29, %5) : (i64, i64) -> i64
    %31 = "arith.addi"(%30, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%7, %31, %6) ({
    ^bb0(%arg29: i64):
      %74 = "arith.index_cast"(%arg29) : (i64) -> index
      %75 = "memref.subview"(%22, %74) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %76 = "arith.index_cast"(%arg29) : (i64) -> index
      %77 = "memref.subview"(%21, %76) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %78 = "arith.index_cast"(%arg29) : (i64) -> index
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg30: index, %arg31: index, %arg32: index):
        "affine.if"(%arg30, %arg31, %arg32) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg33: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg34: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg35: index):
                %79 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %80 = "affine.apply"(%arg34, %arg33, %arg12, %78, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %81 = "nvgpu.device_async_copy"(%75, %79, %10, %80) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %82 = "affine.apply"(%arg34, %arg33) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %83 = "affine.apply"(%arg34, %arg33, %78, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %84 = "nvgpu.device_async_copy"(%77, %82, %9, %83) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "affine.yield"() : () -> ()
      }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    %32 = "arith.index_cast"(%13) : (index) -> i64
    %33 = "arith.subi"(%32, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %34 = "arith.subi"(%33, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %35 = "arith.addi"(%34, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %36 = "arith.cmpi"(%33, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
    %37 = "arith.select"(%36, %35, %33) : (i1, i64, i64) -> i64
    %38 = "arith.divsi"(%37, %4) : (i64, i64) -> i64
    %39 = "arith.addi"(%38, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    %40 = "arith.addi"(%39, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
    "scf.for"(%3, %40, %6) ({
    ^bb0(%arg17: i64):
      %43 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
      %44 = "arith.index_cast"(%43) : (i64) -> index
      %45 = "memref.subview"(%22, %44) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      %46 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
      %47 = "arith.index_cast"(%46) : (i64) -> index
      %48 = "memref.subview"(%21, %47) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
      "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
      ^bb0(%arg24: index, %arg25: index, %arg26: index):
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %66 = "affine.load"(%20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %67 = "affine.for"(%66) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg27: index, %arg28: f32):
          %68 = "affine.vector_load"(%45, %arg25, %arg27) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %69 = "llvm.bitcast"(%68) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %70 = "affine.vector_load"(%48, %arg27, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %71 = "llvm.bitcast"(%70) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %72 = "llvm.fmul"(%69, %71) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %73 = "llvm.fadd"(%arg28, %72) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%73) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%67, %20, %arg24, %arg25, %arg26) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "affine.yield"() {polymer.stmt.name = "S28_affine_yield"} : () -> ()
      }) {gpu.par.block, polymer.stmt.name = "RS2_affine_parallel"} : () -> ()
      %49 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %50 = "arith.addi"(%49, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %51 = "arith.index_cast"(%13) : (index) -> i64
      %52 = "arith.cmpi"(%51, %50) <{predicate = 5 : i64}> : (i64, i64) -> i1
      "scf.if"(%52) ({
        %53 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %54 = "arith.index_cast"(%53) : (i64) -> index
        %55 = "memref.subview"(%22, %54) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %56 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %57 = "arith.index_cast"(%56) : (i64) -> index
        %58 = "memref.subview"(%21, %57) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %59 = "arith.index_cast"(%arg17) : (i64) -> index
        "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
        ^bb0(%arg18: index, %arg19: index, %arg20: index):
          "affine.if"(%arg18, %arg19, %arg20) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg21: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg22: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg23: index):
                  %60 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %61 = "affine.apply"(%arg22, %arg21, %arg12, %59, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %62 = "nvgpu.device_async_copy"(%55, %60, %10, %61) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %63 = "affine.apply"(%arg22, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %64 = "affine.apply"(%arg22, %arg21, %59, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %65 = "nvgpu.device_async_copy"(%58, %63, %9, %64) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "affine.yield"() : () -> ()
        }) {gpu.par.block, polymer.stmt.async.copy, polymer.stmt.name = "RS1_affine_parallel"} : () -> ()
        "scf.yield"() : () -> ()
      }, {
      }) : (i1) -> ()
      "scf.yield"() : () -> ()
    }) : (i64, i64, i64) -> ()
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %41 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %42 = "llvm.bitcast"(%41) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%42, %11, %arg13, %arg14, %arg15, %arg12, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S33_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS3_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After gpuify:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      "affine.store"(%8, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %23 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %24 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %25 = "arith.index_cast"(%13) : (index) -> i64
      %26 = "arith.subi"(%25, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %27 = "arith.subi"(%26, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %28 = "arith.addi"(%27, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %29 = "arith.cmpi"(%26, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %30 = "arith.select"(%29, %28, %26) : (i1, i64, i64) -> i64
      %31 = "arith.divsi"(%30, %4) : (i64, i64) -> i64
      %32 = "arith.minsi"(%31, %5) : (i64, i64) -> i64
      %33 = "arith.addi"(%32, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%7, %33, %6) ({
      ^bb0(%arg23: i64):
        %109 = "arith.index_cast"(%arg23) : (i64) -> index
        %110 = "memref.subview"(%24, %109) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %111 = "arith.index_cast"(%arg23) : (i64) -> index
        %112 = "memref.subview"(%23, %111) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %113 = "arith.index_cast"(%arg23) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg24: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg25: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg26: index):
                %114 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %115 = "affine.apply"(%arg25, %arg24, %arg12, %113, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %116 = "nvgpu.device_async_copy"(%110, %114, %10, %115) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %117 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %118 = "affine.apply"(%arg25, %arg24, %113, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %119 = "nvgpu.device_async_copy"(%112, %117, %9, %118) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %34 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %35 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %36 = "arith.index_cast"(%13) : (index) -> i64
      %37 = "arith.subi"(%36, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %38 = "arith.subi"(%37, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %39 = "arith.addi"(%38, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %40 = "arith.cmpi"(%37, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %41 = "arith.select"(%40, %39, %37) : (i1, i64, i64) -> i64
      %42 = "arith.divsi"(%41, %4) : (i64, i64) -> i64
      %43 = "arith.addi"(%42, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %44 = "arith.addi"(%43, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%3, %44, %6) ({
      ^bb0(%arg17: i64):
        %78 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %79 = "arith.index_cast"(%78) : (i64) -> index
        %80 = "memref.subview"(%35, %79) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %81 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %82 = "arith.index_cast"(%81) : (i64) -> index
        %83 = "memref.subview"(%34, %82) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %84 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %85 = "affine.for"(%84) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg21: index, %arg22: f32):
          %103 = "affine.vector_load"(%80, %arg15, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %104 = "llvm.bitcast"(%103) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %105 = "affine.vector_load"(%83, %arg21, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %106 = "llvm.bitcast"(%105) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %107 = "llvm.fmul"(%104, %106) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %108 = "llvm.fadd"(%arg22, %107) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%108) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%85, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        %86 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %87 = "arith.addi"(%86, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %88 = "arith.index_cast"(%13) : (index) -> i64
        %89 = "arith.cmpi"(%88, %87) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%89) ({
          %90 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %91 = "arith.index_cast"(%90) : (i64) -> index
          %92 = "memref.subview"(%35, %91) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %93 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %94 = "arith.index_cast"(%93) : (i64) -> index
          %95 = "memref.subview"(%34, %94) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0>, static_sizes = array<i64: 1, 1024>, static_strides = array<i64: 1, 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %96 = "arith.index_cast"(%arg17) : (i64) -> index
          "affine.if"(%arg14, %arg15, %arg16) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg18: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg19: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg20: index):
                  %97 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %98 = "affine.apply"(%arg19, %arg18, %arg12, %96, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %99 = "nvgpu.device_async_copy"(%92, %97, %10, %98) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %100 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %101 = "affine.apply"(%arg19, %arg18, %96, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %102 = "nvgpu.device_async_copy"(%95, %100, %9, %101) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %45 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %46 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %47 = "arith.index_cast"(%13) : (index) -> i64
      %48 = "arith.subi"(%47, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %49 = "arith.subi"(%48, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %50 = "arith.addi"(%49, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %51 = "arith.cmpi"(%48, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %52 = "arith.select"(%51, %50, %48) : (i1, i64, i64) -> i64
      %53 = "arith.divsi"(%52, %4) : (i64, i64) -> i64
      %54 = "arith.addi"(%53, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %55 = "arith.addi"(%54, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %56 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %57 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %58 = "arith.index_cast"(%13) : (index) -> i64
      %59 = "arith.subi"(%58, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %60 = "arith.subi"(%59, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %61 = "arith.addi"(%60, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %62 = "arith.cmpi"(%59, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %63 = "arith.select"(%62, %61, %59) : (i1, i64, i64) -> i64
      %64 = "arith.divsi"(%63, %4) : (i64, i64) -> i64
      %65 = "arith.minsi"(%64, %5) : (i64, i64) -> i64
      %66 = "arith.addi"(%65, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %67 = "arith.index_cast"(%13) : (index) -> i64
      %68 = "arith.subi"(%67, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %69 = "arith.subi"(%68, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %70 = "arith.addi"(%69, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %71 = "arith.cmpi"(%68, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %72 = "arith.select"(%71, %70, %68) : (i1, i64, i64) -> i64
      %73 = "arith.divsi"(%72, %4) : (i64, i64) -> i64
      %74 = "arith.addi"(%73, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %75 = "arith.addi"(%74, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %76 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %77 = "llvm.bitcast"(%76) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%77, %11, %arg13, %arg14, %arg15, %arg12, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After expand subview:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> {polymer.stmt.name = "S10_memref_alloca"} : () -> memref<16x16x1xf32, 16>
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      "affine.store"(%8, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S11_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
      "nvvm.barrier0"() : () -> ()
      %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %23 = "arith.index_cast"(%13) : (index) -> i64
      %24 = "arith.subi"(%23, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %25 = "arith.subi"(%24, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %26 = "arith.addi"(%25, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %27 = "arith.cmpi"(%24, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %28 = "arith.select"(%27, %26, %24) : (i1, i64, i64) -> i64
      %29 = "arith.divsi"(%28, %4) : (i64, i64) -> i64
      %30 = "arith.minsi"(%29, %5) : (i64, i64) -> i64
      %31 = "arith.addi"(%30, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%7, %31, %6) ({
      ^bb0(%arg23: i64):
        %80 = "arith.index_cast"(%arg23) : (i64) -> index
        %81 = "affine.apply"(%80) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %82 = "memref.reinterpret_cast"(%22, %81) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %83 = "arith.index_cast"(%arg23) : (i64) -> index
        %84 = "affine.apply"(%83) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %85 = "memref.reinterpret_cast"(%21, %84) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %86 = "arith.index_cast"(%arg23) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg24: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg25: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg26: index):
                %87 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %88 = "affine.apply"(%arg25, %arg24, %arg12, %86, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %89 = "nvgpu.device_async_copy"(%82, %87, %10, %88) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %90 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %91 = "affine.apply"(%arg25, %arg24, %86, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %92 = "nvgpu.device_async_copy"(%85, %90, %9, %91) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %32 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %33 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %34 = "arith.index_cast"(%13) : (index) -> i64
      %35 = "arith.subi"(%34, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %36 = "arith.subi"(%35, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %37 = "arith.addi"(%36, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %38 = "arith.cmpi"(%35, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %39 = "arith.select"(%38, %37, %35) : (i1, i64, i64) -> i64
      %40 = "arith.divsi"(%39, %4) : (i64, i64) -> i64
      %41 = "arith.addi"(%40, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %42 = "arith.addi"(%41, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%3, %42, %6) ({
      ^bb0(%arg17: i64):
        %45 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %46 = "arith.index_cast"(%45) : (i64) -> index
        %47 = "affine.apply"(%46) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %48 = "memref.reinterpret_cast"(%33, %47) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %49 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %50 = "arith.index_cast"(%49) : (i64) -> index
        %51 = "affine.apply"(%50) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %52 = "memref.reinterpret_cast"(%32, %51) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %53 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S18_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
        %54 = "affine.for"(%53) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg21: index, %arg22: f32):
          %74 = "affine.vector_load"(%48, %arg15, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %75 = "llvm.bitcast"(%74) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %76 = "affine.vector_load"(%52, %arg21, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %77 = "llvm.bitcast"(%76) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %78 = "llvm.fmul"(%75, %77) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %79 = "llvm.fadd"(%arg22, %78) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%79) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "affine.store"(%54, %20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S27_affine_store"} : (f32, memref<16x16x1xf32, 16>, index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        %55 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %56 = "arith.addi"(%55, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %57 = "arith.index_cast"(%13) : (index) -> i64
        %58 = "arith.cmpi"(%57, %56) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%58) ({
          %59 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %60 = "arith.index_cast"(%59) : (i64) -> index
          %61 = "affine.apply"(%60) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %62 = "memref.reinterpret_cast"(%33, %61) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %63 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %64 = "arith.index_cast"(%63) : (i64) -> index
          %65 = "affine.apply"(%64) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %66 = "memref.reinterpret_cast"(%32, %65) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %67 = "arith.index_cast"(%arg17) : (i64) -> index
          "affine.if"(%arg14, %arg15, %arg16) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg18: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg19: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg20: index):
                  %68 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %69 = "affine.apply"(%arg19, %arg18, %arg12, %67, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %70 = "nvgpu.device_async_copy"(%62, %68, %10, %69) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %71 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %72 = "affine.apply"(%arg19, %arg18, %67, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %73 = "nvgpu.device_async_copy"(%66, %71, %9, %72) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %43 = "affine.load"(%20, %arg14, %arg15, %arg16) <{map = affine_map<(d0, d1, d2) -> (d0, d1, d2)>}> {polymer.stmt.name = "S30_affine_load"} : (memref<16x16x1xf32, 16>, index, index, index) -> f32
      %44 = "llvm.bitcast"(%43) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%44, %11, %arg13, %arg14, %arg15, %arg12, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After rar:
"llvm.func"() <{CConv = #llvm.cconv<ccc>, arg_attrs = [{}, {}, {}, {}, {}, {}, {}, {llvm.nocapture, llvm.noundef, llvm.writeonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.nocapture, llvm.noundef, llvm.readonly}, {llvm.noundef}, {llvm.noundef}], comdat = @__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii, function_type = !llvm.func<void (i64, i64, i64, i64, i64, i64, i32, ptr, ptr, ptr, i32, i32)>, linkage = #llvm.linkage<private>, sym_name = "__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764", sym_visibility = "private", unnamed_addr = 1 : i64, visibility_ = 0 : i64}> ({
^bb0(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr, %arg8: !llvm.ptr, %arg9: !llvm.ptr, %arg10: i32, %arg11: i32):
  %0 = "arith.constant"() <{value = 1 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : index}> : () -> index
  %2 = "arith.constant"() <{value = 22 : i64}> : () -> i64
  %3 = "arith.constant"() <{value = 21 : i64}> : () -> i64
  %4 = "arith.constant"() <{value = 16 : i64}> : () -> i64
  %5 = "arith.constant"() <{value = 20 : i64}> : () -> i64
  %6 = "arith.constant"() <{value = 1 : i64}> : () -> i64
  %7 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %8 = "llvm.mlir.constant"() <{value = 0.000000e+00 : f32}> {polymer.stmt.name = "S0_llvm_mlir_constant"} : () -> f32
  %9 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %10 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %11 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %12 = "arith.index_cast"(%arg11) {polymer.stmt.name = "S4_arith_index_cast"} : (i32) -> index
  %13 = "arith.index_cast"(%arg10) {polymer.stmt.name = "S5_arith_index_cast"} : (i32) -> index
  %14 = "arith.index_cast"(%arg1) {polymer.stmt.name = "S6_arith_index_cast"} : (i64) -> index
  %15 = "arith.index_cast"(%arg0) {polymer.stmt.name = "S7_arith_index_cast"} : (i64) -> index
  %16 = "arith.index_cast"(%14) : (index) -> i64
  %17 = "arith.index_cast"(%16) : (i64) -> index
  %18 = "arith.index_cast"(%15) : (index) -> i64
  %19 = "arith.index_cast"(%18) : (i64) -> index
  "scf.parallel"(%1, %1, %17, %19, %0, %0) <{operandSegmentSizes = array<i32: 2, 2, 2, 0>}> ({
  ^bb0(%arg12: index, %arg13: index):
    "affine.parallel"() <{lowerBoundsGroups = dense<1> : tensor<3xi32>, lowerBoundsMap = affine_map<() -> (0, 0, 0)>, reductions = [], steps = [1, 1, 1], upperBoundsGroups = dense<1> : tensor<3xi32>, upperBoundsMap = affine_map<() -> (16, 16, 1)>}> ({
    ^bb0(%arg14: index, %arg15: index, %arg16: index):
      %20 = "memref.alloca"() <{operandSegmentSizes = array<i32: 0, 0>}> : () -> memref<f32>
      "memref.store"(%8, %20) <{nontemporal = false}> : (f32, memref<f32>) -> ()
      "nvvm.barrier0"() : () -> ()
      %21 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %22 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %23 = "arith.index_cast"(%13) : (index) -> i64
      %24 = "arith.subi"(%23, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %25 = "arith.subi"(%24, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %26 = "arith.addi"(%25, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %27 = "arith.cmpi"(%24, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %28 = "arith.select"(%27, %26, %24) : (i1, i64, i64) -> i64
      %29 = "arith.divsi"(%28, %4) : (i64, i64) -> i64
      %30 = "arith.minsi"(%29, %5) : (i64, i64) -> i64
      %31 = "arith.addi"(%30, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%7, %31, %6) ({
      ^bb0(%arg23: i64):
        %80 = "arith.index_cast"(%arg23) : (i64) -> index
        %81 = "affine.apply"(%80) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %82 = "memref.reinterpret_cast"(%22, %81) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %83 = "arith.index_cast"(%arg23) : (i64) -> index
        %84 = "affine.apply"(%83) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %85 = "memref.reinterpret_cast"(%21, %84) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %86 = "arith.index_cast"(%arg23) : (i64) -> index
        "affine.if"(%arg14, %arg15, %arg16) ({
          "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
          ^bb0(%arg24: index):
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg25: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
              ^bb0(%arg26: index):
                %87 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %88 = "affine.apply"(%arg25, %arg24, %arg12, %86, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %89 = "nvgpu.device_async_copy"(%82, %87, %10, %88) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                %90 = "affine.apply"(%arg25, %arg24) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                %91 = "affine.apply"(%arg25, %arg24, %86, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                %92 = "nvgpu.device_async_copy"(%85, %90, %9, %91) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "affine.yield"() : () -> ()
          }) : () -> ()
          "nvvm.cp.async.commit.group"() : () -> ()
          "affine.yield"() : () -> ()
        }, {
        }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %32 = "memref.get_global"() <{name = @shared_mem_1}> : () -> memref<22x1024xi8, 3>
      %33 = "memref.get_global"() <{name = @shared_mem_0}> : () -> memref<22x1024xi8, 3>
      %34 = "arith.index_cast"(%13) : (index) -> i64
      %35 = "arith.subi"(%34, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %36 = "arith.subi"(%35, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %37 = "arith.addi"(%36, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %38 = "arith.cmpi"(%35, %7) <{predicate = 2 : i64}> : (i64, i64) -> i1
      %39 = "arith.select"(%38, %37, %35) : (i1, i64, i64) -> i64
      %40 = "arith.divsi"(%39, %4) : (i64, i64) -> i64
      %41 = "arith.addi"(%40, %3) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      %42 = "arith.addi"(%41, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
      "scf.for"(%3, %42, %6) ({
      ^bb0(%arg17: i64):
        %45 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %46 = "arith.index_cast"(%45) : (i64) -> index
        %47 = "affine.apply"(%46) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %48 = "memref.reinterpret_cast"(%33, %47) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        %49 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
        %50 = "arith.index_cast"(%49) : (i64) -> index
        %51 = "affine.apply"(%50) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
        %52 = "memref.reinterpret_cast"(%32, %51) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
        "nvvm.cp.async.wait.group"() <{n = 20 : i32}> : () -> ()
        %53 = "memref.load"(%20) <{nontemporal = false}> : (memref<f32>) -> f32
        %54 = "affine.for"(%53) <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 1>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
        ^bb0(%arg21: index, %arg22: f32):
          %74 = "affine.vector_load"(%48, %arg15, %arg21) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %75 = "llvm.bitcast"(%74) {polymer.stmt.name = "S21_llvm_bitcast"} : (vector<4xi8>) -> f32
          %76 = "affine.vector_load"(%52, %arg21, %arg14) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, index) -> vector<4xi8>
          %77 = "llvm.bitcast"(%76) {polymer.stmt.name = "S23_llvm_bitcast"} : (vector<4xi8>) -> f32
          %78 = "llvm.fmul"(%75, %77) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S24_llvm_fmul"} : (f32, f32) -> f32
          %79 = "llvm.fadd"(%arg22, %78) <{fastmathFlags = #llvm.fastmath<contract>}> {polymer.stmt.name = "S25_llvm_fadd"} : (f32, f32) -> f32
          "affine.yield"(%79) {polymer.stmt.name = "S26_affine_yield"} : (f32) -> ()
        }) : (f32) -> f32
        "memref.store"(%54, %20) <{nontemporal = false}> : (f32, memref<f32>) -> ()
        "nvvm.barrier0"() : () -> ()
        %55 = "arith.muli"(%arg17, %4) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %56 = "arith.addi"(%55, %6) <{overflowFlags = #arith.overflow<none>}> : (i64, i64) -> i64
        %57 = "arith.index_cast"(%13) : (index) -> i64
        %58 = "arith.cmpi"(%57, %56) <{predicate = 5 : i64}> : (i64, i64) -> i1
        "scf.if"(%58) ({
          %59 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %60 = "arith.index_cast"(%59) : (i64) -> index
          %61 = "affine.apply"(%60) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %62 = "memref.reinterpret_cast"(%33, %61) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %63 = "arith.remui"(%arg17, %2) : (i64, i64) -> i64
          %64 = "arith.index_cast"(%63) : (i64) -> index
          %65 = "affine.apply"(%64) <{map = affine_map<()[s0] -> (s0 * 1024)>}> : (index) -> index
          %66 = "memref.reinterpret_cast"(%32, %65) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1024>, static_strides = array<i64: 1>}> : (memref<22x1024xi8, 3>, index) -> memref<1024xi8, strided<[1], offset: ?>, 3>
          %67 = "arith.index_cast"(%arg17) : (i64) -> index
          "affine.if"(%arg14, %arg15, %arg16) ({
            "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
            ^bb0(%arg18: index):
              "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (16)>}> ({
              ^bb0(%arg19: index):
                "affine.for"() <{lowerBoundMap = affine_map<() -> (0)>, operandSegmentSizes = array<i32: 0, 0, 0>, step = 1 : index, upperBoundMap = affine_map<() -> (1)>}> ({
                ^bb0(%arg20: index):
                  %68 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %69 = "affine.apply"(%arg19, %arg18, %arg12, %67, %13) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %70 = "nvgpu.device_async_copy"(%62, %68, %10, %69) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  %71 = "affine.apply"(%arg19, %arg18) <{map = affine_map<(d0, d1) -> (d0 * 64 + d1 * 4)>}> : (index, index) -> index
                  %72 = "affine.apply"(%arg19, %arg18, %67, %arg13, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> ((d0 * s0) * 4 + d1 * 4 + d3 * 64 + (d2 * (s0 * 16)) * 4)>}> : (index, index, index, index, index) -> index
                  %73 = "nvgpu.device_async_copy"(%66, %71, %9, %72) <{dstElements = 4 : index, operandSegmentSizes = array<i32: 1, 1, 1, 1, 0>}> : (memref<1024xi8, strided<[1], offset: ?>, 3>, index, memref<?xi8>, index) -> !nvgpu.device.async.token
                  "affine.yield"() : () -> ()
                }) : () -> ()
                "affine.yield"() : () -> ()
              }) : () -> ()
              "affine.yield"() : () -> ()
            }) : () -> ()
            "nvvm.cp.async.commit.group"() : () -> ()
            "affine.yield"() : () -> ()
          }, {
          }) {condition = affine_set<(d0, d1, d2) : (d0 == 0, d1 == 0, d2 == 0)>} : (index, index, index) -> ()
          "scf.yield"() : () -> ()
        }, {
        }) : (i1) -> ()
        "nvvm.barrier0"() : () -> ()
        "scf.yield"() : () -> ()
      }) : (i64, i64, i64) -> ()
      "nvvm.barrier0"() : () -> ()
      %43 = "memref.load"(%20) <{nontemporal = false}> : (memref<f32>) -> f32
      %44 = "llvm.bitcast"(%43) {polymer.stmt.name = "S31_llvm_bitcast"} : (f32) -> vector<4xi8>
      "affine.vector_store"(%44, %11, %arg13, %arg14, %arg15, %arg12, %12) <{map = affine_map<(d0, d1, d2, d3)[s0] -> (d0 * 64 + d1 * 4 + (d2 * s0) * 4 + (d3 * (s0 * 16)) * 4)>}> {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : (vector<4xi8>, memref<?xi8>, index, index, index, index, index) -> ()
      "affine.yield"() {polymer.stmt.name = "S12_affine_yield"} : () -> ()
    }) {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"} : () -> ()
    "scf.reduce"() : () -> ()
  }) {gpu.par.grid} : (index, index, index, index, index, index) -> ()
  "llvm.return"() {polymer.stmt.name = "S35_llvm_return"} : () -> ()
}) {gpu.par.kernel} : () -> ()
gpu-affine-opt: After lower affine:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %c1 = arith.constant 1 : index
  %c0 = arith.constant 0 : index
  %c22_i64 = arith.constant 22 : i64
  %c21_i64 = arith.constant 21 : i64
  %c16_i64 = arith.constant 16 : i64
  %c20_i64 = arith.constant 20 : i64
  %c1_i64 = arith.constant 1 : i64
  %c0_i64 = arith.constant 0 : i64
  %0 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %1 = "memref.ataddr"(%arg9) {polymer.stmt.name = "S1_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %2 = "memref.ataddr"(%arg8) {polymer.stmt.name = "S2_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %3 = "memref.ataddr"(%arg7) {polymer.stmt.name = "S3_memref_ataddr"} : (!llvm.ptr) -> memref<?xi8>
  %4 = arith.index_cast %arg11 {polymer.stmt.name = "S4_arith_index_cast"} : i32 to index
  %5 = arith.index_cast %arg10 {polymer.stmt.name = "S5_arith_index_cast"} : i32 to index
  %6 = arith.index_cast %arg1 {polymer.stmt.name = "S6_arith_index_cast"} : i64 to index
  %7 = arith.index_cast %arg0 {polymer.stmt.name = "S7_arith_index_cast"} : i64 to index
  %8 = arith.index_cast %6 : index to i64
  %9 = arith.index_cast %8 : i64 to index
  %10 = arith.index_cast %7 : index to i64
  %11 = arith.index_cast %10 : i64 to index
  scf.parallel (%arg12, %arg13) = (%c0, %c0) to (%9, %11) step (%c1, %c1) {
    %c0_0 = arith.constant 0 : index
    %c16 = arith.constant 16 : index
    %c0_1 = arith.constant 0 : index
    %c16_2 = arith.constant 16 : index
    %c0_3 = arith.constant 0 : index
    %c1_4 = arith.constant 1 : index
    %c1_5 = arith.constant 1 : index
    %c1_6 = arith.constant 1 : index
    %c1_7 = arith.constant 1 : index
    scf.parallel (%arg14, %arg15, %arg16) = (%c0_0, %c0_1, %c0_3) to (%c16, %c16_2, %c1_4) step (%c1_5, %c1_6, %c1_7) {
      %alloca = memref.alloca() : memref<f32>
      memref.store %0, %alloca[] : memref<f32>
      nvvm.barrier0
      %12 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %13 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %14 = arith.index_cast %5 : index to i64
      %15 = arith.subi %14, %c1_i64 : i64
      %16 = arith.subi %15, %c16_i64 : i64
      %17 = arith.addi %16, %c1_i64 : i64
      %18 = arith.cmpi slt, %15, %c0_i64 : i64
      %19 = arith.select %18, %17, %15 : i64
      %20 = arith.divsi %19, %c16_i64 : i64
      %21 = arith.minsi %20, %c20_i64 : i64
      %22 = arith.addi %21, %c1_i64 : i64
      scf.for %arg17 = %c0_i64 to %22 step %c1_i64  : i64 {
        %46 = arith.index_cast %arg17 : i64 to index
        %c1024 = arith.constant 1024 : index
        %47 = arith.muli %46, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %13 to offset: [%47], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %48 = arith.index_cast %arg17 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %49 = arith.muli %48, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %12 to offset: [%49], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %50 = arith.index_cast %arg17 : i64 to index
        %c0_13 = arith.constant 0 : index
        %51 = arith.cmpi eq, %arg14, %c0_13 : index
        %52 = arith.cmpi eq, %arg15, %c0_13 : index
        %53 = arith.andi %51, %52 : i1
        %54 = arith.cmpi eq, %arg16, %c0_13 : index
        %55 = arith.andi %53, %54 : i1
        scf.if %55 {
          %c0_14 = arith.constant 0 : index
          %c16_15 = arith.constant 16 : index
          %c1_16 = arith.constant 1 : index
          scf.for %arg18 = %c0_14 to %c16_15 step %c1_16 {
            %c0_17 = arith.constant 0 : index
            %c16_18 = arith.constant 16 : index
            %c1_19 = arith.constant 1 : index
            scf.for %arg19 = %c0_17 to %c16_18 step %c1_19 {
              %c0_20 = arith.constant 0 : index
              %c1_21 = arith.constant 1 : index
              %c1_22 = arith.constant 1 : index
              scf.for %arg20 = %c0_20 to %c1_21 step %c1_22 {
                %c64_23 = arith.constant 64 : index
                %56 = arith.muli %arg19, %c64_23 : index
                %c4_24 = arith.constant 4 : index
                %57 = arith.muli %arg18, %c4_24 : index
                %58 = arith.addi %56, %57 : index
                %59 = arith.muli %arg19, %5 : index
                %c4_25 = arith.constant 4 : index
                %60 = arith.muli %59, %c4_25 : index
                %c4_26 = arith.constant 4 : index
                %61 = arith.muli %arg18, %c4_26 : index
                %62 = arith.addi %60, %61 : index
                %c64_27 = arith.constant 64 : index
                %63 = arith.muli %50, %c64_27 : index
                %64 = arith.addi %62, %63 : index
                %c16_28 = arith.constant 16 : index
                %65 = arith.muli %5, %c16_28 : index
                %66 = arith.muli %arg12, %65 : index
                %c4_29 = arith.constant 4 : index
                %67 = arith.muli %66, %c4_29 : index
                %68 = arith.addi %64, %67 : index
                %69 = nvgpu.device_async_copy %2[%68], %reinterpret_cast[%58], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                %c64_30 = arith.constant 64 : index
                %70 = arith.muli %arg19, %c64_30 : index
                %c4_31 = arith.constant 4 : index
                %71 = arith.muli %arg18, %c4_31 : index
                %72 = arith.addi %70, %71 : index
                %73 = arith.muli %arg19, %4 : index
                %c4_32 = arith.constant 4 : index
                %74 = arith.muli %73, %c4_32 : index
                %c4_33 = arith.constant 4 : index
                %75 = arith.muli %arg18, %c4_33 : index
                %76 = arith.addi %74, %75 : index
                %c64_34 = arith.constant 64 : index
                %77 = arith.muli %arg13, %c64_34 : index
                %78 = arith.addi %76, %77 : index
                %c16_35 = arith.constant 16 : index
                %79 = arith.muli %4, %c16_35 : index
                %80 = arith.muli %50, %79 : index
                %c4_36 = arith.constant 4 : index
                %81 = arith.muli %80, %c4_36 : index
                %82 = arith.addi %78, %81 : index
                %83 = nvgpu.device_async_copy %1[%82], %reinterpret_cast_12[%72], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %23 = memref.get_global @shared_mem_1 : memref<22x1024xi8, 3>
      %24 = memref.get_global @shared_mem_0 : memref<22x1024xi8, 3>
      %25 = arith.index_cast %5 : index to i64
      %26 = arith.subi %25, %c1_i64 : i64
      %27 = arith.subi %26, %c16_i64 : i64
      %28 = arith.addi %27, %c1_i64 : i64
      %29 = arith.cmpi slt, %26, %c0_i64 : i64
      %30 = arith.select %29, %28, %26 : i64
      %31 = arith.divsi %30, %c16_i64 : i64
      %32 = arith.addi %31, %c21_i64 : i64
      %33 = arith.addi %32, %c1_i64 : i64
      scf.for %arg17 = %c21_i64 to %33 step %c1_i64  : i64 {
        %46 = arith.remui %arg17, %c22_i64 : i64
        %47 = arith.index_cast %46 : i64 to index
        %c1024 = arith.constant 1024 : index
        %48 = arith.muli %47, %c1024 : index
        %reinterpret_cast = memref.reinterpret_cast %24 to offset: [%48], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        %49 = arith.remui %arg17, %c22_i64 : i64
        %50 = arith.index_cast %49 : i64 to index
        %c1024_11 = arith.constant 1024 : index
        %51 = arith.muli %50, %c1024_11 : index
        %reinterpret_cast_12 = memref.reinterpret_cast %23 to offset: [%51], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
        nvvm.cp.async.wait.group 20
        %52 = memref.load %alloca[] : memref<f32>
        %c0_13 = arith.constant 0 : index
        %c16_14 = arith.constant 16 : index
        %c1_15 = arith.constant 1 : index
        %53 = scf.for %arg18 = %c0_13 to %c16_14 step %c1_15 iter_args(%arg19 = %52) -> (f32) {
          %c64_16 = arith.constant 64 : index
          %58 = arith.muli %arg15, %c64_16 : index
          %c4_17 = arith.constant 4 : index
          %59 = arith.muli %arg18, %c4_17 : index
          %60 = arith.addi %58, %59 : index
          %61 = vector.load %reinterpret_cast[%60] {polymer.access.type = f32, polymer.stmt.name = "S20_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %62 = llvm.bitcast %61 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %c64_18 = arith.constant 64 : index
          %63 = arith.muli %arg18, %c64_18 : index
          %c4_19 = arith.constant 4 : index
          %64 = arith.muli %arg14, %c4_19 : index
          %65 = arith.addi %63, %64 : index
          %66 = vector.load %reinterpret_cast_12[%65] {polymer.access.type = f32, polymer.stmt.name = "S22_affine_vector_load"} : memref<1024xi8, strided<[1], offset: ?>, 3>, vector<4xi8>
          %67 = llvm.bitcast %66 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %68 = llvm.fmul %62, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %69 = llvm.fadd %arg19, %68  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %69 : f32
        }
        memref.store %53, %alloca[] : memref<f32>
        nvvm.barrier0
        %54 = arith.muli %arg17, %c16_i64 : i64
        %55 = arith.addi %54, %c1_i64 : i64
        %56 = arith.index_cast %5 : index to i64
        %57 = arith.cmpi sge, %56, %55 : i64
        scf.if %57 {
          %58 = arith.remui %arg17, %c22_i64 : i64
          %59 = arith.index_cast %58 : i64 to index
          %c1024_16 = arith.constant 1024 : index
          %60 = arith.muli %59, %c1024_16 : index
          %reinterpret_cast_17 = memref.reinterpret_cast %24 to offset: [%60], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          %61 = arith.remui %arg17, %c22_i64 : i64
          %62 = arith.index_cast %61 : i64 to index
          %c1024_18 = arith.constant 1024 : index
          %63 = arith.muli %62, %c1024_18 : index
          %reinterpret_cast_19 = memref.reinterpret_cast %23 to offset: [%63], sizes: [1024], strides: [1] : memref<22x1024xi8, 3> to memref<1024xi8, strided<[1], offset: ?>, 3>
          %64 = arith.index_cast %arg17 : i64 to index
          %c0_20 = arith.constant 0 : index
          %65 = arith.cmpi eq, %arg14, %c0_20 : index
          %66 = arith.cmpi eq, %arg15, %c0_20 : index
          %67 = arith.andi %65, %66 : i1
          %68 = arith.cmpi eq, %arg16, %c0_20 : index
          %69 = arith.andi %67, %68 : i1
          scf.if %69 {
            %c0_21 = arith.constant 0 : index
            %c16_22 = arith.constant 16 : index
            %c1_23 = arith.constant 1 : index
            scf.for %arg18 = %c0_21 to %c16_22 step %c1_23 {
              %c0_24 = arith.constant 0 : index
              %c16_25 = arith.constant 16 : index
              %c1_26 = arith.constant 1 : index
              scf.for %arg19 = %c0_24 to %c16_25 step %c1_26 {
                %c0_27 = arith.constant 0 : index
                %c1_28 = arith.constant 1 : index
                %c1_29 = arith.constant 1 : index
                scf.for %arg20 = %c0_27 to %c1_28 step %c1_29 {
                  %c64_30 = arith.constant 64 : index
                  %70 = arith.muli %arg19, %c64_30 : index
                  %c4_31 = arith.constant 4 : index
                  %71 = arith.muli %arg18, %c4_31 : index
                  %72 = arith.addi %70, %71 : index
                  %73 = arith.muli %arg19, %5 : index
                  %c4_32 = arith.constant 4 : index
                  %74 = arith.muli %73, %c4_32 : index
                  %c4_33 = arith.constant 4 : index
                  %75 = arith.muli %arg18, %c4_33 : index
                  %76 = arith.addi %74, %75 : index
                  %c64_34 = arith.constant 64 : index
                  %77 = arith.muli %64, %c64_34 : index
                  %78 = arith.addi %76, %77 : index
                  %c16_35 = arith.constant 16 : index
                  %79 = arith.muli %5, %c16_35 : index
                  %80 = arith.muli %arg12, %79 : index
                  %c4_36 = arith.constant 4 : index
                  %81 = arith.muli %80, %c4_36 : index
                  %82 = arith.addi %78, %81 : index
                  %83 = nvgpu.device_async_copy %2[%82], %reinterpret_cast_17[%72], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                  %c64_37 = arith.constant 64 : index
                  %84 = arith.muli %arg19, %c64_37 : index
                  %c4_38 = arith.constant 4 : index
                  %85 = arith.muli %arg18, %c4_38 : index
                  %86 = arith.addi %84, %85 : index
                  %87 = arith.muli %arg19, %4 : index
                  %c4_39 = arith.constant 4 : index
                  %88 = arith.muli %87, %c4_39 : index
                  %c4_40 = arith.constant 4 : index
                  %89 = arith.muli %arg18, %c4_40 : index
                  %90 = arith.addi %88, %89 : index
                  %c64_41 = arith.constant 64 : index
                  %91 = arith.muli %arg13, %c64_41 : index
                  %92 = arith.addi %90, %91 : index
                  %c16_42 = arith.constant 16 : index
                  %93 = arith.muli %4, %c16_42 : index
                  %94 = arith.muli %64, %93 : index
                  %c4_43 = arith.constant 4 : index
                  %95 = arith.muli %94, %c4_43 : index
                  %96 = arith.addi %92, %95 : index
                  %97 = nvgpu.device_async_copy %1[%96], %reinterpret_cast_19[%86], 4 : memref<?xi8> to memref<1024xi8, strided<[1], offset: ?>, 3>
                }
              }
            }
            nvvm.cp.async.commit.group
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %34 = memref.load %alloca[] : memref<f32>
      %35 = llvm.bitcast %34 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %c64 = arith.constant 64 : index
      %36 = arith.muli %arg13, %c64 : index
      %c4 = arith.constant 4 : index
      %37 = arith.muli %arg14, %c4 : index
      %38 = arith.addi %36, %37 : index
      %39 = arith.muli %arg15, %4 : index
      %c4_8 = arith.constant 4 : index
      %40 = arith.muli %39, %c4_8 : index
      %41 = arith.addi %38, %40 : index
      %c16_9 = arith.constant 16 : index
      %42 = arith.muli %4, %c16_9 : index
      %43 = arith.muli %arg12, %42 : index
      %c4_10 = arith.constant 4 : index
      %44 = arith.muli %43, %c4_10 : index
      %45 = arith.addi %41, %44 : index
      vector.store %35, %3[%45] {polymer.access.type = f32, polymer.stmt.name = "S32_affine_vector_store"} : memref<?xi8>, vector<4xi8>
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: After lower accesses:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  %1 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %2 = llvm.mlir.constant(1 : i64) : i64
  %3 = builtin.unrealized_conversion_cast %2 : i64 to index
  %4 = llvm.mlir.constant(0 : i64) : i64
  %5 = builtin.unrealized_conversion_cast %4 : i64 to index
  %6 = llvm.mlir.constant(22 : i64) : i64
  %7 = llvm.mlir.constant(21 : i64) : i64
  %8 = llvm.mlir.constant(16 : i64) : i64
  %9 = llvm.mlir.constant(20 : i64) : i64
  %10 = llvm.mlir.constant(1 : i64) : i64
  %11 = llvm.mlir.constant(0 : i64) : i64
  %12 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %13 = llvm.sext %arg11 : i32 to i64
  %14 = llvm.sext %arg10 : i32 to i64
  scf.parallel (%arg12, %arg13) = (%5, %5) to (%1, %0) step (%3, %3) {
    %15 = builtin.unrealized_conversion_cast %arg13 : index to i64
    %16 = builtin.unrealized_conversion_cast %arg12 : index to i64
    %17 = llvm.mlir.constant(0 : i64) : i64
    %18 = builtin.unrealized_conversion_cast %17 : i64 to index
    %19 = llvm.mlir.constant(16 : i64) : i64
    %20 = builtin.unrealized_conversion_cast %19 : i64 to index
    %21 = llvm.mlir.constant(0 : i64) : i64
    %22 = builtin.unrealized_conversion_cast %21 : i64 to index
    %23 = llvm.mlir.constant(16 : i64) : i64
    %24 = builtin.unrealized_conversion_cast %23 : i64 to index
    %25 = llvm.mlir.constant(0 : i64) : i64
    %26 = builtin.unrealized_conversion_cast %25 : i64 to index
    %27 = llvm.mlir.constant(1 : i64) : i64
    %28 = builtin.unrealized_conversion_cast %27 : i64 to index
    %29 = llvm.mlir.constant(1 : i64) : i64
    %30 = builtin.unrealized_conversion_cast %29 : i64 to index
    %31 = llvm.mlir.constant(1 : i64) : i64
    %32 = builtin.unrealized_conversion_cast %31 : i64 to index
    %33 = llvm.mlir.constant(1 : i64) : i64
    %34 = builtin.unrealized_conversion_cast %33 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%18, %22, %26) to (%20, %24, %28) step (%30, %32, %34) {
      %35 = builtin.unrealized_conversion_cast %arg16 : index to i64
      %36 = builtin.unrealized_conversion_cast %arg15 : index to i64
      %37 = builtin.unrealized_conversion_cast %arg14 : index to i64
      %38 = llvm.mlir.constant(1 : i64) : i64
      %39 = llvm.alloca %38 x f32 : (i64) -> !llvm.ptr
      llvm.store %12, %39 : f32, !llvm.ptr
      nvvm.barrier0
      %40 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %41 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %42 = llvm.sub %14, %10 : i64
      %43 = llvm.sub %42, %8 : i64
      %44 = llvm.add %43, %10 : i64
      %45 = llvm.icmp "slt" %42, %11 : i64
      %46 = llvm.select %45, %44, %42 : i1, i64
      %47 = llvm.sdiv %46, %8  : i64
      %48 = llvm.intr.smin(%47, %9)  : (i64, i64) -> i64
      %49 = llvm.add %48, %10 : i64
      scf.for %arg17 = %11 to %49 step %10  : i64 {
        %79 = llvm.mlir.constant(1024 : i64) : i64
        %80 = llvm.mul %arg17, %79 : i64
        %81 = llvm.getelementptr %41[%80] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %82 = llvm.mlir.constant(1024 : i64) : i64
        %83 = llvm.mul %arg17, %82 : i64
        %84 = llvm.getelementptr %40[%83] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %85 = llvm.mlir.constant(0 : i64) : i64
        %86 = llvm.icmp "eq" %37, %85 : i64
        %87 = llvm.icmp "eq" %36, %85 : i64
        %88 = llvm.and %86, %87  : i1
        %89 = llvm.icmp "eq" %35, %85 : i64
        %90 = llvm.and %88, %89  : i1
        scf.if %90 {
          %91 = llvm.mlir.constant(0 : i64) : i64
          %92 = builtin.unrealized_conversion_cast %91 : i64 to index
          %93 = llvm.mlir.constant(16 : i64) : i64
          %94 = builtin.unrealized_conversion_cast %93 : i64 to index
          %95 = llvm.mlir.constant(1 : i64) : i64
          %96 = builtin.unrealized_conversion_cast %95 : i64 to index
          scf.for %arg18 = %92 to %94 step %96 {
            %97 = builtin.unrealized_conversion_cast %arg18 : index to i64
            %98 = llvm.mlir.constant(0 : i64) : i64
            %99 = builtin.unrealized_conversion_cast %98 : i64 to index
            %100 = llvm.mlir.constant(16 : i64) : i64
            %101 = builtin.unrealized_conversion_cast %100 : i64 to index
            %102 = llvm.mlir.constant(1 : i64) : i64
            %103 = builtin.unrealized_conversion_cast %102 : i64 to index
            scf.for %arg19 = %99 to %101 step %103 {
              %104 = builtin.unrealized_conversion_cast %arg19 : index to i64
              %105 = llvm.mlir.constant(0 : i64) : i64
              %106 = builtin.unrealized_conversion_cast %105 : i64 to index
              %107 = llvm.mlir.constant(1 : i64) : i64
              %108 = builtin.unrealized_conversion_cast %107 : i64 to index
              %109 = llvm.mlir.constant(1 : i64) : i64
              %110 = builtin.unrealized_conversion_cast %109 : i64 to index
              scf.for %arg20 = %106 to %108 step %110 {
                %111 = llvm.mlir.constant(64 : i64) : i64
                %112 = llvm.mul %104, %111 : i64
                %113 = llvm.mlir.constant(4 : i64) : i64
                %114 = llvm.mul %97, %113 : i64
                %115 = llvm.add %112, %114 : i64
                %116 = llvm.mul %104, %14 : i64
                %117 = llvm.mlir.constant(4 : i64) : i64
                %118 = llvm.mul %116, %117 : i64
                %119 = llvm.mlir.constant(4 : i64) : i64
                %120 = llvm.mul %97, %119 : i64
                %121 = llvm.add %118, %120 : i64
                %122 = llvm.mlir.constant(64 : i64) : i64
                %123 = llvm.mul %arg17, %122 : i64
                %124 = llvm.add %121, %123 : i64
                %125 = llvm.mlir.constant(16 : i64) : i64
                %126 = llvm.mul %14, %125 : i64
                %127 = llvm.mul %16, %126 : i64
                %128 = llvm.mlir.constant(4 : i64) : i64
                %129 = llvm.mul %127, %128 : i64
                %130 = llvm.add %124, %129 : i64
                %131 = llvm.getelementptr %81[%115] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %132 = llvm.getelementptr %arg8[%130] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %133 = llvm.addrspacecast %132 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %131, %133, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %134 = llvm.mlir.constant(0 : i32) : i32
                %135 = llvm.mlir.constant(64 : i64) : i64
                %136 = llvm.mul %104, %135 : i64
                %137 = llvm.mlir.constant(4 : i64) : i64
                %138 = llvm.mul %97, %137 : i64
                %139 = llvm.add %136, %138 : i64
                %140 = llvm.mul %104, %13 : i64
                %141 = llvm.mlir.constant(4 : i64) : i64
                %142 = llvm.mul %140, %141 : i64
                %143 = llvm.mlir.constant(4 : i64) : i64
                %144 = llvm.mul %97, %143 : i64
                %145 = llvm.add %142, %144 : i64
                %146 = llvm.mlir.constant(64 : i64) : i64
                %147 = llvm.mul %15, %146 : i64
                %148 = llvm.add %145, %147 : i64
                %149 = llvm.mlir.constant(16 : i64) : i64
                %150 = llvm.mul %13, %149 : i64
                %151 = llvm.mul %arg17, %150 : i64
                %152 = llvm.mlir.constant(4 : i64) : i64
                %153 = llvm.mul %151, %152 : i64
                %154 = llvm.add %148, %153 : i64
                %155 = llvm.getelementptr %84[%139] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %156 = llvm.getelementptr %arg9[%154] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %157 = llvm.addrspacecast %156 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %155, %157, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %158 = llvm.mlir.constant(0 : i32) : i32
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %50 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %51 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %52 = llvm.sub %14, %10 : i64
      %53 = llvm.sub %52, %8 : i64
      %54 = llvm.add %53, %10 : i64
      %55 = llvm.icmp "slt" %52, %11 : i64
      %56 = llvm.select %55, %54, %52 : i1, i64
      %57 = llvm.sdiv %56, %8  : i64
      %58 = llvm.add %57, %7 : i64
      %59 = llvm.add %58, %10 : i64
      scf.for %arg17 = %7 to %59 step %10  : i64 {
        %79 = llvm.urem %arg17, %6  : i64
        %80 = llvm.mlir.constant(1024 : i64) : i64
        %81 = llvm.mul %79, %80 : i64
        %82 = llvm.getelementptr %51[%81] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %83 = llvm.urem %arg17, %6  : i64
        %84 = llvm.mlir.constant(1024 : i64) : i64
        %85 = llvm.mul %83, %84 : i64
        %86 = llvm.getelementptr %50[%85] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        nvvm.cp.async.wait.group 20
        %87 = llvm.load %39 : !llvm.ptr -> f32
        %88 = llvm.mlir.constant(0 : i64) : i64
        %89 = builtin.unrealized_conversion_cast %88 : i64 to index
        %90 = llvm.mlir.constant(16 : i64) : i64
        %91 = builtin.unrealized_conversion_cast %90 : i64 to index
        %92 = llvm.mlir.constant(1 : i64) : i64
        %93 = builtin.unrealized_conversion_cast %92 : i64 to index
        %94 = scf.for %arg18 = %89 to %91 step %93 iter_args(%arg19 = %87) -> (f32) {
          %98 = builtin.unrealized_conversion_cast %arg18 : index to i64
          %99 = llvm.mlir.constant(64 : i64) : i64
          %100 = llvm.mul %36, %99 : i64
          %101 = llvm.mlir.constant(4 : i64) : i64
          %102 = llvm.mul %98, %101 : i64
          %103 = llvm.add %100, %102 : i64
          %104 = llvm.getelementptr %82[%103] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %105 = llvm.load %104 : !llvm.ptr<3> -> f32
          %106 = llvm.bitcast %105 : f32 to vector<4xi8>
          %107 = llvm.bitcast %106 {polymer.stmt.name = "S21_llvm_bitcast"} : vector<4xi8> to f32
          %108 = llvm.mlir.constant(64 : i64) : i64
          %109 = llvm.mul %98, %108 : i64
          %110 = llvm.mlir.constant(4 : i64) : i64
          %111 = llvm.mul %37, %110 : i64
          %112 = llvm.add %109, %111 : i64
          %113 = llvm.getelementptr %86[%112] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %114 = llvm.load %113 : !llvm.ptr<3> -> f32
          %115 = llvm.bitcast %114 : f32 to vector<4xi8>
          %116 = llvm.bitcast %115 {polymer.stmt.name = "S23_llvm_bitcast"} : vector<4xi8> to f32
          %117 = llvm.fmul %107, %116  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %118 = llvm.fadd %arg19, %117  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %118 : f32
        }
        llvm.store %94, %39 : f32, !llvm.ptr
        nvvm.barrier0
        %95 = llvm.mul %arg17, %8 : i64
        %96 = llvm.add %95, %10 : i64
        %97 = llvm.icmp "sge" %14, %96 : i64
        scf.if %97 {
          %98 = llvm.urem %arg17, %6  : i64
          %99 = llvm.mlir.constant(1024 : i64) : i64
          %100 = llvm.mul %98, %99 : i64
          %101 = llvm.getelementptr %51[%100] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %102 = llvm.urem %arg17, %6  : i64
          %103 = llvm.mlir.constant(1024 : i64) : i64
          %104 = llvm.mul %102, %103 : i64
          %105 = llvm.getelementptr %50[%104] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %106 = llvm.mlir.constant(0 : i64) : i64
          %107 = llvm.icmp "eq" %37, %106 : i64
          %108 = llvm.icmp "eq" %36, %106 : i64
          %109 = llvm.and %107, %108  : i1
          %110 = llvm.icmp "eq" %35, %106 : i64
          %111 = llvm.and %109, %110  : i1
          scf.if %111 {
            %112 = llvm.mlir.constant(0 : i64) : i64
            %113 = builtin.unrealized_conversion_cast %112 : i64 to index
            %114 = llvm.mlir.constant(16 : i64) : i64
            %115 = builtin.unrealized_conversion_cast %114 : i64 to index
            %116 = llvm.mlir.constant(1 : i64) : i64
            %117 = builtin.unrealized_conversion_cast %116 : i64 to index
            scf.for %arg18 = %113 to %115 step %117 {
              %118 = builtin.unrealized_conversion_cast %arg18 : index to i64
              %119 = llvm.mlir.constant(0 : i64) : i64
              %120 = builtin.unrealized_conversion_cast %119 : i64 to index
              %121 = llvm.mlir.constant(16 : i64) : i64
              %122 = builtin.unrealized_conversion_cast %121 : i64 to index
              %123 = llvm.mlir.constant(1 : i64) : i64
              %124 = builtin.unrealized_conversion_cast %123 : i64 to index
              scf.for %arg19 = %120 to %122 step %124 {
                %125 = builtin.unrealized_conversion_cast %arg19 : index to i64
                %126 = llvm.mlir.constant(0 : i64) : i64
                %127 = builtin.unrealized_conversion_cast %126 : i64 to index
                %128 = llvm.mlir.constant(1 : i64) : i64
                %129 = builtin.unrealized_conversion_cast %128 : i64 to index
                %130 = llvm.mlir.constant(1 : i64) : i64
                %131 = builtin.unrealized_conversion_cast %130 : i64 to index
                scf.for %arg20 = %127 to %129 step %131 {
                  %132 = llvm.mlir.constant(64 : i64) : i64
                  %133 = llvm.mul %125, %132 : i64
                  %134 = llvm.mlir.constant(4 : i64) : i64
                  %135 = llvm.mul %118, %134 : i64
                  %136 = llvm.add %133, %135 : i64
                  %137 = llvm.mul %125, %14 : i64
                  %138 = llvm.mlir.constant(4 : i64) : i64
                  %139 = llvm.mul %137, %138 : i64
                  %140 = llvm.mlir.constant(4 : i64) : i64
                  %141 = llvm.mul %118, %140 : i64
                  %142 = llvm.add %139, %141 : i64
                  %143 = llvm.mlir.constant(64 : i64) : i64
                  %144 = llvm.mul %arg17, %143 : i64
                  %145 = llvm.add %142, %144 : i64
                  %146 = llvm.mlir.constant(16 : i64) : i64
                  %147 = llvm.mul %14, %146 : i64
                  %148 = llvm.mul %16, %147 : i64
                  %149 = llvm.mlir.constant(4 : i64) : i64
                  %150 = llvm.mul %148, %149 : i64
                  %151 = llvm.add %145, %150 : i64
                  %152 = llvm.getelementptr %101[%136] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %153 = llvm.getelementptr %arg8[%151] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %154 = llvm.addrspacecast %153 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %152, %154, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  %155 = llvm.mlir.constant(0 : i32) : i32
                  %156 = llvm.mlir.constant(64 : i64) : i64
                  %157 = llvm.mul %125, %156 : i64
                  %158 = llvm.mlir.constant(4 : i64) : i64
                  %159 = llvm.mul %118, %158 : i64
                  %160 = llvm.add %157, %159 : i64
                  %161 = llvm.mul %125, %13 : i64
                  %162 = llvm.mlir.constant(4 : i64) : i64
                  %163 = llvm.mul %161, %162 : i64
                  %164 = llvm.mlir.constant(4 : i64) : i64
                  %165 = llvm.mul %118, %164 : i64
                  %166 = llvm.add %163, %165 : i64
                  %167 = llvm.mlir.constant(64 : i64) : i64
                  %168 = llvm.mul %15, %167 : i64
                  %169 = llvm.add %166, %168 : i64
                  %170 = llvm.mlir.constant(16 : i64) : i64
                  %171 = llvm.mul %13, %170 : i64
                  %172 = llvm.mul %arg17, %171 : i64
                  %173 = llvm.mlir.constant(4 : i64) : i64
                  %174 = llvm.mul %172, %173 : i64
                  %175 = llvm.add %169, %174 : i64
                  %176 = llvm.getelementptr %105[%160] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %177 = llvm.getelementptr %arg9[%175] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %178 = llvm.addrspacecast %177 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %176, %178, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  %179 = llvm.mlir.constant(0 : i32) : i32
                }
              }
            }
            nvvm.cp.async.commit.group
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %60 = llvm.load %39 : !llvm.ptr -> f32
      %61 = llvm.bitcast %60 {polymer.stmt.name = "S31_llvm_bitcast"} : f32 to vector<4xi8>
      %62 = llvm.mlir.constant(64 : i64) : i64
      %63 = llvm.mul %15, %62 : i64
      %64 = llvm.mlir.constant(4 : i64) : i64
      %65 = llvm.mul %37, %64 : i64
      %66 = llvm.add %63, %65 : i64
      %67 = llvm.mul %36, %13 : i64
      %68 = llvm.mlir.constant(4 : i64) : i64
      %69 = llvm.mul %67, %68 : i64
      %70 = llvm.add %66, %69 : i64
      %71 = llvm.mlir.constant(16 : i64) : i64
      %72 = llvm.mul %13, %71 : i64
      %73 = llvm.mul %16, %72 : i64
      %74 = llvm.mlir.constant(4 : i64) : i64
      %75 = llvm.mul %73, %74 : i64
      %76 = llvm.add %70, %75 : i64
      %77 = llvm.getelementptr %arg7[%76] : (!llvm.ptr, i64) -> !llvm.ptr, i8
      %78 = llvm.bitcast %61 : vector<4xi8> to f32
      llvm.store %78, %77 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: Canonicalized:
llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
  %0 = llvm.mlir.constant(4 : i64) : i64
  %1 = llvm.mlir.constant(64 : i64) : i64
  %2 = llvm.mlir.constant(1024 : i64) : i64
  %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
  %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
  %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
  %6 = llvm.mlir.constant(20 : i64) : i64
  %7 = llvm.mlir.constant(16 : i64) : i64
  %8 = llvm.mlir.constant(21 : i64) : i64
  %9 = llvm.mlir.constant(22 : i64) : i64
  %10 = llvm.mlir.constant(0 : i64) : i64
  %11 = llvm.mlir.constant(1 : i64) : i64
  %12 = builtin.unrealized_conversion_cast %arg0 : i64 to index
  %13 = builtin.unrealized_conversion_cast %arg1 : i64 to index
  %14 = builtin.unrealized_conversion_cast %11 : i64 to index
  %15 = builtin.unrealized_conversion_cast %10 : i64 to index
  %16 = llvm.sext %arg11 : i32 to i64
  %17 = llvm.sext %arg10 : i32 to i64
  scf.parallel (%arg12, %arg13) = (%15, %15) to (%13, %12) step (%14, %14) {
    %18 = builtin.unrealized_conversion_cast %arg13 : index to i64
    %19 = builtin.unrealized_conversion_cast %arg12 : index to i64
    %20 = builtin.unrealized_conversion_cast %7 : i64 to index
    scf.parallel (%arg14, %arg15, %arg16) = (%15, %15, %15) to (%20, %20, %14) step (%14, %14, %14) {
      %21 = builtin.unrealized_conversion_cast %arg16 : index to i64
      %22 = builtin.unrealized_conversion_cast %arg15 : index to i64
      %23 = builtin.unrealized_conversion_cast %arg14 : index to i64
      %24 = llvm.alloca %11 x f32 : (i64) -> !llvm.ptr
      llvm.store %5, %24 : f32, !llvm.ptr
      nvvm.barrier0
      %25 = llvm.sub %17, %11 : i64
      %26 = llvm.sub %25, %7 : i64
      %27 = llvm.add %26, %11 : i64
      %28 = llvm.icmp "slt" %25, %10 : i64
      %29 = llvm.select %28, %27, %25 : i1, i64
      %30 = llvm.sdiv %29, %7  : i64
      %31 = llvm.intr.smin(%30, %6)  : (i64, i64) -> i64
      %32 = llvm.add %31, %11 : i64
      scf.for %arg17 = %10 to %32 step %11  : i64 {
        %47 = llvm.mul %arg17, %2 : i64
        %48 = llvm.getelementptr %3[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %49 = llvm.getelementptr %4[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %50 = llvm.icmp "eq" %23, %10 : i64
        %51 = llvm.icmp "eq" %22, %10 : i64
        %52 = llvm.and %50, %51  : i1
        %53 = llvm.icmp "eq" %21, %10 : i64
        %54 = llvm.and %52, %53  : i1
        scf.if %54 {
          scf.for %arg18 = %15 to %20 step %14 {
            %55 = builtin.unrealized_conversion_cast %arg18 : index to i64
            scf.for %arg19 = %15 to %20 step %14 {
              %56 = builtin.unrealized_conversion_cast %arg19 : index to i64
              scf.for %arg20 = %15 to %14 step %14 {
                %57 = llvm.mul %56, %1 : i64
                %58 = llvm.mul %55, %0 : i64
                %59 = llvm.add %57, %58 : i64
                %60 = llvm.mul %56, %17 : i64
                %61 = llvm.mul %60, %0 : i64
                %62 = llvm.add %61, %58 : i64
                %63 = llvm.mul %arg17, %1 : i64
                %64 = llvm.add %62, %63 : i64
                %65 = llvm.mul %17, %7 : i64
                %66 = llvm.mul %19, %65 : i64
                %67 = llvm.mul %66, %0 : i64
                %68 = llvm.add %64, %67 : i64
                %69 = llvm.getelementptr %48[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %70 = llvm.getelementptr %arg8[%68] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %71 = llvm.addrspacecast %70 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %69, %71, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                %72 = llvm.mul %56, %16 : i64
                %73 = llvm.mul %72, %0 : i64
                %74 = llvm.add %73, %58 : i64
                %75 = llvm.mul %18, %1 : i64
                %76 = llvm.add %74, %75 : i64
                %77 = llvm.mul %16, %7 : i64
                %78 = llvm.mul %arg17, %77 : i64
                %79 = llvm.mul %78, %0 : i64
                %80 = llvm.add %76, %79 : i64
                %81 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                %82 = llvm.getelementptr %arg9[%80] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                %83 = llvm.addrspacecast %82 : !llvm.ptr to !llvm.ptr<1>
                nvvm.cp.async.shared.global %81, %83, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
              }
            }
          }
          nvvm.cp.async.commit.group
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %33 = llvm.add %30, %8 : i64
      %34 = llvm.add %33, %11 : i64
      scf.for %arg17 = %8 to %34 step %11  : i64 {
        %47 = llvm.urem %arg17, %9  : i64
        %48 = llvm.mul %47, %2 : i64
        %49 = llvm.getelementptr %3[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        %50 = llvm.getelementptr %4[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
        nvvm.cp.async.wait.group 20
        %51 = llvm.load %24 : !llvm.ptr -> f32
        %52 = scf.for %arg18 = %15 to %20 step %14 iter_args(%arg19 = %51) -> (f32) {
          %56 = builtin.unrealized_conversion_cast %arg18 : index to i64
          %57 = llvm.mul %22, %1 : i64
          %58 = llvm.mul %56, %0 : i64
          %59 = llvm.add %57, %58 : i64
          %60 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %61 = llvm.load %60 : !llvm.ptr<3> -> f32
          %62 = llvm.mul %56, %1 : i64
          %63 = llvm.mul %23, %0 : i64
          %64 = llvm.add %62, %63 : i64
          %65 = llvm.getelementptr %50[%64] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %66 = llvm.load %65 : !llvm.ptr<3> -> f32
          %67 = llvm.fmul %61, %66  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
          %68 = llvm.fadd %arg19, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
          scf.yield %68 : f32
        }
        llvm.store %52, %24 : f32, !llvm.ptr
        nvvm.barrier0
        %53 = llvm.mul %arg17, %7 : i64
        %54 = llvm.add %53, %11 : i64
        %55 = llvm.icmp "sge" %17, %54 : i64
        scf.if %55 {
          %56 = llvm.icmp "eq" %23, %10 : i64
          %57 = llvm.icmp "eq" %22, %10 : i64
          %58 = llvm.and %56, %57  : i1
          %59 = llvm.icmp "eq" %21, %10 : i64
          %60 = llvm.and %58, %59  : i1
          scf.if %60 {
            scf.for %arg18 = %15 to %20 step %14 {
              %61 = builtin.unrealized_conversion_cast %arg18 : index to i64
              scf.for %arg19 = %15 to %20 step %14 {
                %62 = builtin.unrealized_conversion_cast %arg19 : index to i64
                scf.for %arg20 = %15 to %14 step %14 {
                  %63 = llvm.mul %62, %1 : i64
                  %64 = llvm.mul %61, %0 : i64
                  %65 = llvm.add %63, %64 : i64
                  %66 = llvm.mul %62, %17 : i64
                  %67 = llvm.mul %66, %0 : i64
                  %68 = llvm.add %67, %64 : i64
                  %69 = llvm.mul %arg17, %1 : i64
                  %70 = llvm.add %68, %69 : i64
                  %71 = llvm.mul %17, %7 : i64
                  %72 = llvm.mul %19, %71 : i64
                  %73 = llvm.mul %72, %0 : i64
                  %74 = llvm.add %70, %73 : i64
                  %75 = llvm.getelementptr %49[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %76 = llvm.getelementptr %arg8[%74] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %77 = llvm.addrspacecast %76 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %75, %77, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  %78 = llvm.mul %62, %16 : i64
                  %79 = llvm.mul %78, %0 : i64
                  %80 = llvm.add %79, %64 : i64
                  %81 = llvm.mul %18, %1 : i64
                  %82 = llvm.add %80, %81 : i64
                  %83 = llvm.mul %16, %7 : i64
                  %84 = llvm.mul %arg17, %83 : i64
                  %85 = llvm.mul %84, %0 : i64
                  %86 = llvm.add %82, %85 : i64
                  %87 = llvm.getelementptr %50[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %88 = llvm.getelementptr %arg9[%86] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %89 = llvm.addrspacecast %88 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %87, %89, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                }
              }
            }
            nvvm.cp.async.commit.group
          }
        }
        nvvm.barrier0
      }
      nvvm.barrier0
      %35 = llvm.load %24 : !llvm.ptr -> f32
      %36 = llvm.mul %18, %1 : i64
      %37 = llvm.mul %23, %0 : i64
      %38 = llvm.add %36, %37 : i64
      %39 = llvm.mul %22, %16 : i64
      %40 = llvm.mul %39, %0 : i64
      %41 = llvm.add %38, %40 : i64
      %42 = llvm.mul %16, %7 : i64
      %43 = llvm.mul %19, %42 : i64
      %44 = llvm.mul %43, %0 : i64
      %45 = llvm.add %41, %44 : i64
      %46 = llvm.getelementptr %arg7[%45] : (!llvm.ptr, i64) -> !llvm.ptr, i8
      llvm.store %35, %46 : f32, !llvm.ptr
      scf.reduce 
    } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
    scf.reduce 
  } {gpu.par.grid}
  llvm.return {polymer.stmt.name = "S35_llvm_return"}
}
gpu-affine-opt: After gpu module lower accesses:gpu.module @__mlir_gpu_module [#nvvm.target<chip = "sm_80">] {
  llvm.mlir.global external @shared_mem_1() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
    %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
    llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
  }
  llvm.mlir.global external @shared_mem_0() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
    %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
    llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
  }
  llvm.comdat @__llvm_global_comdat {
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2As any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2Bs any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2As any
    llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2Bs any
    llvm.comdat_selector @_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii any
    llvm.comdat_selector @_Z13MatrixMulCUDAILi32EEvPfS0_S0_ii any
  }
  llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
    %0 = llvm.mlir.constant(4 : i64) : i64
    %1 = llvm.mlir.constant(64 : i64) : i64
    %2 = llvm.mlir.constant(1024 : i64) : i64
    %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
    %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
    %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
    %6 = llvm.mlir.constant(20 : i64) : i64
    %7 = llvm.mlir.constant(16 : i64) : i64
    %8 = llvm.mlir.constant(21 : i64) : i64
    %9 = llvm.mlir.constant(22 : i64) : i64
    %10 = llvm.mlir.constant(0 : i64) : i64
    %11 = llvm.mlir.constant(1 : i64) : i64
    %12 = builtin.unrealized_conversion_cast %arg0 : i64 to index
    %13 = builtin.unrealized_conversion_cast %arg1 : i64 to index
    %14 = builtin.unrealized_conversion_cast %11 : i64 to index
    %15 = builtin.unrealized_conversion_cast %10 : i64 to index
    %16 = llvm.sext %arg11 : i32 to i64
    %17 = llvm.sext %arg10 : i32 to i64
    scf.parallel (%arg12, %arg13) = (%15, %15) to (%13, %12) step (%14, %14) {
      %18 = builtin.unrealized_conversion_cast %arg13 : index to i64
      %19 = builtin.unrealized_conversion_cast %arg12 : index to i64
      %20 = builtin.unrealized_conversion_cast %7 : i64 to index
      scf.parallel (%arg14, %arg15, %arg16) = (%15, %15, %15) to (%20, %20, %14) step (%14, %14, %14) {
        %21 = builtin.unrealized_conversion_cast %arg16 : index to i64
        %22 = builtin.unrealized_conversion_cast %arg15 : index to i64
        %23 = builtin.unrealized_conversion_cast %arg14 : index to i64
        %24 = llvm.alloca %11 x f32 : (i64) -> !llvm.ptr
        llvm.store %5, %24 : f32, !llvm.ptr
        nvvm.barrier0
        %25 = llvm.sub %17, %11 : i64
        %26 = llvm.sub %25, %7 : i64
        %27 = llvm.add %26, %11 : i64
        %28 = llvm.icmp "slt" %25, %10 : i64
        %29 = llvm.select %28, %27, %25 : i1, i64
        %30 = llvm.sdiv %29, %7  : i64
        %31 = llvm.intr.smin(%30, %6)  : (i64, i64) -> i64
        %32 = llvm.add %31, %11 : i64
        scf.for %arg17 = %10 to %32 step %11  : i64 {
          %47 = llvm.mul %arg17, %2 : i64
          %48 = llvm.getelementptr %3[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %49 = llvm.getelementptr %4[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %50 = llvm.icmp "eq" %23, %10 : i64
          %51 = llvm.icmp "eq" %22, %10 : i64
          %52 = llvm.and %50, %51  : i1
          %53 = llvm.icmp "eq" %21, %10 : i64
          %54 = llvm.and %52, %53  : i1
          scf.if %54 {
            scf.for %arg18 = %15 to %20 step %14 {
              %55 = builtin.unrealized_conversion_cast %arg18 : index to i64
              scf.for %arg19 = %15 to %20 step %14 {
                %56 = builtin.unrealized_conversion_cast %arg19 : index to i64
                scf.for %arg20 = %15 to %14 step %14 {
                  %57 = llvm.mul %56, %1 : i64
                  %58 = llvm.mul %55, %0 : i64
                  %59 = llvm.add %57, %58 : i64
                  %60 = llvm.mul %56, %17 : i64
                  %61 = llvm.mul %60, %0 : i64
                  %62 = llvm.add %61, %58 : i64
                  %63 = llvm.mul %arg17, %1 : i64
                  %64 = llvm.add %62, %63 : i64
                  %65 = llvm.mul %17, %7 : i64
                  %66 = llvm.mul %19, %65 : i64
                  %67 = llvm.mul %66, %0 : i64
                  %68 = llvm.add %64, %67 : i64
                  %69 = llvm.getelementptr %48[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %70 = llvm.getelementptr %arg8[%68] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %71 = llvm.addrspacecast %70 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %69, %71, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  %72 = llvm.mul %56, %16 : i64
                  %73 = llvm.mul %72, %0 : i64
                  %74 = llvm.add %73, %58 : i64
                  %75 = llvm.mul %18, %1 : i64
                  %76 = llvm.add %74, %75 : i64
                  %77 = llvm.mul %16, %7 : i64
                  %78 = llvm.mul %arg17, %77 : i64
                  %79 = llvm.mul %78, %0 : i64
                  %80 = llvm.add %76, %79 : i64
                  %81 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                  %82 = llvm.getelementptr %arg9[%80] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                  %83 = llvm.addrspacecast %82 : !llvm.ptr to !llvm.ptr<1>
                  nvvm.cp.async.shared.global %81, %83, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                }
              }
            }
            nvvm.cp.async.commit.group
          }
          nvvm.barrier0
        }
        nvvm.barrier0
        %33 = llvm.add %30, %8 : i64
        %34 = llvm.add %33, %11 : i64
        scf.for %arg17 = %8 to %34 step %11  : i64 {
          %47 = llvm.urem %arg17, %9  : i64
          %48 = llvm.mul %47, %2 : i64
          %49 = llvm.getelementptr %3[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          %50 = llvm.getelementptr %4[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
          nvvm.cp.async.wait.group 20
          %51 = llvm.load %24 : !llvm.ptr -> f32
          %52 = scf.for %arg18 = %15 to %20 step %14 iter_args(%arg19 = %51) -> (f32) {
            %56 = builtin.unrealized_conversion_cast %arg18 : index to i64
            %57 = llvm.mul %22, %1 : i64
            %58 = llvm.mul %56, %0 : i64
            %59 = llvm.add %57, %58 : i64
            %60 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %61 = llvm.load %60 : !llvm.ptr<3> -> f32
            %62 = llvm.mul %56, %1 : i64
            %63 = llvm.mul %23, %0 : i64
            %64 = llvm.add %62, %63 : i64
            %65 = llvm.getelementptr %50[%64] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %66 = llvm.load %65 : !llvm.ptr<3> -> f32
            %67 = llvm.fmul %61, %66  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
            %68 = llvm.fadd %arg19, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
            scf.yield %68 : f32
          }
          llvm.store %52, %24 : f32, !llvm.ptr
          nvvm.barrier0
          %53 = llvm.mul %arg17, %7 : i64
          %54 = llvm.add %53, %11 : i64
          %55 = llvm.icmp "sge" %17, %54 : i64
          scf.if %55 {
            %56 = llvm.icmp "eq" %23, %10 : i64
            %57 = llvm.icmp "eq" %22, %10 : i64
            %58 = llvm.and %56, %57  : i1
            %59 = llvm.icmp "eq" %21, %10 : i64
            %60 = llvm.and %58, %59  : i1
            scf.if %60 {
              scf.for %arg18 = %15 to %20 step %14 {
                %61 = builtin.unrealized_conversion_cast %arg18 : index to i64
                scf.for %arg19 = %15 to %20 step %14 {
                  %62 = builtin.unrealized_conversion_cast %arg19 : index to i64
                  scf.for %arg20 = %15 to %14 step %14 {
                    %63 = llvm.mul %62, %1 : i64
                    %64 = llvm.mul %61, %0 : i64
                    %65 = llvm.add %63, %64 : i64
                    %66 = llvm.mul %62, %17 : i64
                    %67 = llvm.mul %66, %0 : i64
                    %68 = llvm.add %67, %64 : i64
                    %69 = llvm.mul %arg17, %1 : i64
                    %70 = llvm.add %68, %69 : i64
                    %71 = llvm.mul %17, %7 : i64
                    %72 = llvm.mul %19, %71 : i64
                    %73 = llvm.mul %72, %0 : i64
                    %74 = llvm.add %70, %73 : i64
                    %75 = llvm.getelementptr %49[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %76 = llvm.getelementptr %arg8[%74] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %77 = llvm.addrspacecast %76 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %75, %77, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %78 = llvm.mul %62, %16 : i64
                    %79 = llvm.mul %78, %0 : i64
                    %80 = llvm.add %79, %64 : i64
                    %81 = llvm.mul %18, %1 : i64
                    %82 = llvm.add %80, %81 : i64
                    %83 = llvm.mul %16, %7 : i64
                    %84 = llvm.mul %arg17, %83 : i64
                    %85 = llvm.mul %84, %0 : i64
                    %86 = llvm.add %82, %85 : i64
                    %87 = llvm.getelementptr %50[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %88 = llvm.getelementptr %arg9[%86] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %89 = llvm.addrspacecast %88 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %87, %89, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
          }
          nvvm.barrier0
        }
        nvvm.barrier0
        %35 = llvm.load %24 : !llvm.ptr -> f32
        %36 = llvm.mul %18, %1 : i64
        %37 = llvm.mul %23, %0 : i64
        %38 = llvm.add %36, %37 : i64
        %39 = llvm.mul %22, %16 : i64
        %40 = llvm.mul %39, %0 : i64
        %41 = llvm.add %38, %40 : i64
        %42 = llvm.mul %16, %7 : i64
        %43 = llvm.mul %19, %42 : i64
        %44 = llvm.mul %43, %0 : i64
        %45 = llvm.add %41, %44 : i64
        %46 = llvm.getelementptr %arg7[%45] : (!llvm.ptr, i64) -> !llvm.ptr, i8
        llvm.store %35, %46 : f32, !llvm.ptr
        scf.reduce 
      } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
      scf.reduce 
    } {gpu.par.grid}
    llvm.return {polymer.stmt.name = "S35_llvm_return"}
  }
}
module attributes {dlti.dl_spec = #dlti.dl_spec<f80 = dense<128> : vector<2xi64>, i128 = dense<128> : vector<2xi64>, !llvm.ptr<272> = dense<64> : vector<4xi64>, i64 = dense<64> : vector<2xi64>, !llvm.ptr<270> = dense<32> : vector<4xi64>, f128 = dense<128> : vector<2xi64>, !llvm.ptr<271> = dense<32> : vector<4xi64>, f16 = dense<16> : vector<2xi64>, f64 = dense<64> : vector<2xi64>, i16 = dense<16> : vector<2xi64>, i32 = dense<32> : vector<2xi64>, i1 = dense<8> : vector<2xi64>, i8 = dense<8> : vector<2xi64>, !llvm.ptr = dense<64> : vector<4xi64>, "dlti.stack_alignment" = 128 : i64, "dlti.endianness" = "little">, gpu.container_module} {
  gpu.module @__mlir_gpu_module [#nvvm.target<chip = "sm_80">] {
    llvm.mlir.global external @shared_mem_1() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.mlir.global external @shared_mem_0() {addr_space = 3 : i32} : !llvm.array<22 x array<1024 x i8>> {
      %0 = llvm.mlir.undef : !llvm.array<22 x array<1024 x i8>>
      llvm.return %0 : !llvm.array<22 x array<1024 x i8>>
    }
    llvm.comdat @__llvm_global_comdat {
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi16EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2As any
      llvm.comdat_selector @_ZZ13MatrixMulCUDAILi32EEvPfS0_S0_iiE2Bs any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii any
      llvm.comdat_selector @_Z13MatrixMulCUDAILi32EEvPfS0_S0_ii any
    }
    llvm.func private local_unnamed_addr @__mlir.par.kernel._Z13MatrixMulCUDAILi16EEvPfS0_S0_ii_32764(%arg0: i64, %arg1: i64, %arg2: i64, %arg3: i64, %arg4: i64, %arg5: i64, %arg6: i32, %arg7: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.writeonly}, %arg8: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg9: !llvm.ptr {llvm.nocapture, llvm.noundef, llvm.readonly}, %arg10: i32 {llvm.noundef}, %arg11: i32 {llvm.noundef}) comdat(@__llvm_global_comdat::@_Z13MatrixMulCUDAILi16EEvPfS0_S0_ii) attributes {gpu.par.kernel, sym_visibility = "private"} {
      %0 = llvm.mlir.constant(4 : i64) : i64
      %1 = llvm.mlir.constant(64 : i64) : i64
      %2 = llvm.mlir.constant(1024 : i64) : i64
      %3 = llvm.mlir.addressof @shared_mem_0 : !llvm.ptr<3>
      %4 = llvm.mlir.addressof @shared_mem_1 : !llvm.ptr<3>
      %5 = llvm.mlir.constant(0.000000e+00 : f32) {polymer.stmt.name = "S0_llvm_mlir_constant"} : f32
      %6 = llvm.mlir.constant(20 : i64) : i64
      %7 = llvm.mlir.constant(16 : i64) : i64
      %8 = llvm.mlir.constant(21 : i64) : i64
      %9 = llvm.mlir.constant(22 : i64) : i64
      %10 = llvm.mlir.constant(0 : i64) : i64
      %11 = llvm.mlir.constant(1 : i64) : i64
      %12 = builtin.unrealized_conversion_cast %arg0 : i64 to index
      %13 = builtin.unrealized_conversion_cast %arg1 : i64 to index
      %14 = builtin.unrealized_conversion_cast %11 : i64 to index
      %15 = builtin.unrealized_conversion_cast %10 : i64 to index
      %16 = llvm.sext %arg11 : i32 to i64
      %17 = llvm.sext %arg10 : i32 to i64
      scf.parallel (%arg12, %arg13) = (%15, %15) to (%13, %12) step (%14, %14) {
        %18 = builtin.unrealized_conversion_cast %arg13 : index to i64
        %19 = builtin.unrealized_conversion_cast %arg12 : index to i64
        %20 = builtin.unrealized_conversion_cast %7 : i64 to index
        scf.parallel (%arg14, %arg15, %arg16) = (%15, %15, %15) to (%20, %20, %14) step (%14, %14, %14) {
          %21 = builtin.unrealized_conversion_cast %arg16 : index to i64
          %22 = builtin.unrealized_conversion_cast %arg15 : index to i64
          %23 = builtin.unrealized_conversion_cast %arg14 : index to i64
          %24 = llvm.alloca %11 x f32 : (i64) -> !llvm.ptr
          llvm.store %5, %24 : f32, !llvm.ptr
          nvvm.barrier0
          %25 = llvm.sub %17, %11 : i64
          %26 = llvm.sub %25, %7 : i64
          %27 = llvm.add %26, %11 : i64
          %28 = llvm.icmp "slt" %25, %10 : i64
          %29 = llvm.select %28, %27, %25 : i1, i64
          %30 = llvm.sdiv %29, %7  : i64
          %31 = llvm.intr.smin(%30, %6)  : (i64, i64) -> i64
          %32 = llvm.add %31, %11 : i64
          scf.for %arg17 = %10 to %32 step %11  : i64 {
            %47 = llvm.mul %arg17, %2 : i64
            %48 = llvm.getelementptr %3[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %49 = llvm.getelementptr %4[%47] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %50 = llvm.icmp "eq" %23, %10 : i64
            %51 = llvm.icmp "eq" %22, %10 : i64
            %52 = llvm.and %50, %51  : i1
            %53 = llvm.icmp "eq" %21, %10 : i64
            %54 = llvm.and %52, %53  : i1
            scf.if %54 {
              scf.for %arg18 = %15 to %20 step %14 {
                %55 = builtin.unrealized_conversion_cast %arg18 : index to i64
                scf.for %arg19 = %15 to %20 step %14 {
                  %56 = builtin.unrealized_conversion_cast %arg19 : index to i64
                  scf.for %arg20 = %15 to %14 step %14 {
                    %57 = llvm.mul %56, %1 : i64
                    %58 = llvm.mul %55, %0 : i64
                    %59 = llvm.add %57, %58 : i64
                    %60 = llvm.mul %56, %17 : i64
                    %61 = llvm.mul %60, %0 : i64
                    %62 = llvm.add %61, %58 : i64
                    %63 = llvm.mul %arg17, %1 : i64
                    %64 = llvm.add %62, %63 : i64
                    %65 = llvm.mul %17, %7 : i64
                    %66 = llvm.mul %19, %65 : i64
                    %67 = llvm.mul %66, %0 : i64
                    %68 = llvm.add %64, %67 : i64
                    %69 = llvm.getelementptr %48[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %70 = llvm.getelementptr %arg8[%68] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %71 = llvm.addrspacecast %70 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %69, %71, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    %72 = llvm.mul %56, %16 : i64
                    %73 = llvm.mul %72, %0 : i64
                    %74 = llvm.add %73, %58 : i64
                    %75 = llvm.mul %18, %1 : i64
                    %76 = llvm.add %74, %75 : i64
                    %77 = llvm.mul %16, %7 : i64
                    %78 = llvm.mul %arg17, %77 : i64
                    %79 = llvm.mul %78, %0 : i64
                    %80 = llvm.add %76, %79 : i64
                    %81 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                    %82 = llvm.getelementptr %arg9[%80] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                    %83 = llvm.addrspacecast %82 : !llvm.ptr to !llvm.ptr<1>
                    nvvm.cp.async.shared.global %81, %83, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                  }
                }
              }
              nvvm.cp.async.commit.group
            }
            nvvm.barrier0
          }
          nvvm.barrier0
          %33 = llvm.add %30, %8 : i64
          %34 = llvm.add %33, %11 : i64
          scf.for %arg17 = %8 to %34 step %11  : i64 {
            %47 = llvm.urem %arg17, %9  : i64
            %48 = llvm.mul %47, %2 : i64
            %49 = llvm.getelementptr %3[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            %50 = llvm.getelementptr %4[%48] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
            nvvm.cp.async.wait.group 20
            %51 = llvm.load %24 : !llvm.ptr -> f32
            %52 = scf.for %arg18 = %15 to %20 step %14 iter_args(%arg19 = %51) -> (f32) {
              %56 = builtin.unrealized_conversion_cast %arg18 : index to i64
              %57 = llvm.mul %22, %1 : i64
              %58 = llvm.mul %56, %0 : i64
              %59 = llvm.add %57, %58 : i64
              %60 = llvm.getelementptr %49[%59] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %61 = llvm.load %60 : !llvm.ptr<3> -> f32
              %62 = llvm.mul %56, %1 : i64
              %63 = llvm.mul %23, %0 : i64
              %64 = llvm.add %62, %63 : i64
              %65 = llvm.getelementptr %50[%64] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
              %66 = llvm.load %65 : !llvm.ptr<3> -> f32
              %67 = llvm.fmul %61, %66  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S24_llvm_fmul"} : f32
              %68 = llvm.fadd %arg19, %67  {fastmathFlags = #llvm.fastmath<contract>, polymer.stmt.name = "S25_llvm_fadd"} : f32
              scf.yield %68 : f32
            }
            llvm.store %52, %24 : f32, !llvm.ptr
            nvvm.barrier0
            %53 = llvm.mul %arg17, %7 : i64
            %54 = llvm.add %53, %11 : i64
            %55 = llvm.icmp "sge" %17, %54 : i64
            scf.if %55 {
              %56 = llvm.icmp "eq" %23, %10 : i64
              %57 = llvm.icmp "eq" %22, %10 : i64
              %58 = llvm.and %56, %57  : i1
              %59 = llvm.icmp "eq" %21, %10 : i64
              %60 = llvm.and %58, %59  : i1
              scf.if %60 {
                scf.for %arg18 = %15 to %20 step %14 {
                  %61 = builtin.unrealized_conversion_cast %arg18 : index to i64
                  scf.for %arg19 = %15 to %20 step %14 {
                    %62 = builtin.unrealized_conversion_cast %arg19 : index to i64
                    scf.for %arg20 = %15 to %14 step %14 {
                      %63 = llvm.mul %62, %1 : i64
                      %64 = llvm.mul %61, %0 : i64
                      %65 = llvm.add %63, %64 : i64
                      %66 = llvm.mul %62, %17 : i64
                      %67 = llvm.mul %66, %0 : i64
                      %68 = llvm.add %67, %64 : i64
                      %69 = llvm.mul %arg17, %1 : i64
                      %70 = llvm.add %68, %69 : i64
                      %71 = llvm.mul %17, %7 : i64
                      %72 = llvm.mul %19, %71 : i64
                      %73 = llvm.mul %72, %0 : i64
                      %74 = llvm.add %70, %73 : i64
                      %75 = llvm.getelementptr %49[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                      %76 = llvm.getelementptr %arg8[%74] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                      %77 = llvm.addrspacecast %76 : !llvm.ptr to !llvm.ptr<1>
                      nvvm.cp.async.shared.global %75, %77, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                      %78 = llvm.mul %62, %16 : i64
                      %79 = llvm.mul %78, %0 : i64
                      %80 = llvm.add %79, %64 : i64
                      %81 = llvm.mul %18, %1 : i64
                      %82 = llvm.add %80, %81 : i64
                      %83 = llvm.mul %16, %7 : i64
                      %84 = llvm.mul %arg17, %83 : i64
                      %85 = llvm.mul %84, %0 : i64
                      %86 = llvm.add %82, %85 : i64
                      %87 = llvm.getelementptr %50[%65] : (!llvm.ptr<3>, i64) -> !llvm.ptr<3>, i8
                      %88 = llvm.getelementptr %arg9[%86] : (!llvm.ptr, i64) -> !llvm.ptr, i8
                      %89 = llvm.addrspacecast %88 : !llvm.ptr to !llvm.ptr<1>
                      nvvm.cp.async.shared.global %87, %89, 4, cache =  ca : !llvm.ptr<3>, !llvm.ptr<1>
                    }
                  }
                }
                nvvm.cp.async.commit.group
              }
            }
            nvvm.barrier0
          }
          nvvm.barrier0
          %35 = llvm.load %24 : !llvm.ptr -> f32
          %36 = llvm.mul %18, %1 : i64
          %37 = llvm.mul %23, %0 : i64
          %38 = llvm.add %36, %37 : i64
          %39 = llvm.mul %22, %16 : i64
          %40 = llvm.mul %39, %0 : i64
          %41 = llvm.add %38, %40 : i64
          %42 = llvm.mul %16, %7 : i64
          %43 = llvm.mul %19, %42 : i64
          %44 = llvm.mul %43, %0 : i64
          %45 = llvm.add %41, %44 : i64
          %46 = llvm.getelementptr %arg7[%45] : (!llvm.ptr, i64) -> !llvm.ptr, i8
          llvm.store %35, %46 : f32, !llvm.ptr
          scf.reduce 
        } {gpu.par.block, polymer.stmt.name = "RS0_affine_parallel"}
        scf.reduce 
      } {gpu.par.grid}
      llvm.return {polymer.stmt.name = "S35_llvm_return"}
    }
  }
}

Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
Eliminated dead instances: [P0, P1, P2, P3] -> { S4_arith_index_cast[]; S1_memref_ataddr[]; S9_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S8_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S5_arith_index_cast[]; S3_memref_ataddr[]; S6_arith_index_cast[]; S2_memref_ataddr[]; S29_affine_yield[i0, i1, 0, i3] : 0 <= i0 < P1 and 0 <= i1 < P0 and i3 >= 0 and 16i3 < P2; S7_arith_index_cast[]; S34_affine_yield[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0; S0_llvm_mlir_constant[]; S10_memref_alloca[i0, i1, 0] : 0 <= i0 < P1 and 0 <= i1 < P0 }
